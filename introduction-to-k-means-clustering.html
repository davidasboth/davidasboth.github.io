<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags always come first -->
	<meta charset="utf-8">

	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src="https://www.googletagmanager.com/gtag/js?id=UA-68436188-1"></script>
	<script>
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){dataLayer.push(arguments);}
	  gtag('js', new Date());

	  gtag('config', 'UA-68436188-1');
	</script>


	<!-- Start cookieyes banner -->
    <script id="cookieyes" type="text/javascript" src="https://cdn-cookieyes.com/client_data/2808e7a66d4c9b126f5d2ac9/script.js"></script>
    <!-- End cookieyes banner --> 

	<!-- Favicon stuff -->
	<link rel="apple-touch-icon" sizes="180x180" href="/theme/images/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/theme/images/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/theme/images/favicon-16x16.png">
	<link rel="manifest" href="/theme/images/site.webmanifest">

	<meta http-equiv="x-ua-compatible" content="ie=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>David Asboth - Data Solutions &amp; Consultancy</title>

	<link rel="canonical" href="/introduction-to-k-means-clustering.html">
	
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&display=swap" rel="stylesheet"> 
	
	<link rel="stylesheet" href="/theme/css/style.css">
	<link rel="stylesheet" href="/theme/css/pygments.css">


</head>

<body>
	<header id="stickyHeader">
		<div class="container">
			<a href="/"><img src="/theme/images/da-logo.svg" alt="David Asboth logo" id="header-logo"/></a>
			<nav>
				<ul>
					<li>
						<a href="/#solutions">Solutions</a>
					</li>
					<li>
							<a href="/#about">About</a>
					</li>
					<li>
							<a href="/articles">Articles</a>
					</li>
					<li>
					  <a href="#contact" class="button button-filled">Contact</a>
					</li>
				</ul>
			</nav>
		</div>
	</header>


<main id="articles-tutorials">
    <section id="splash">
        <div class="container">
            <h1 class="text-centered">Introduction to K-means Clustering</h1>
            <div class="text-centered padding-medium"><a class="tag" href="category/machine-learning.html">machine learning</a></div>
            <p class="text-centered"><p>An introduction to the popular k-means clustering algorithm with intuition and Python code.</p></p>
         </div>
    </section>

    <section id="article-content">
        <div class="container">
            
            <p>This is the first of a two-part post on K-means clustering.</p>
<h2>Introduction</h2>
<h3>Unsupervised Learning</h3>
<p>I've talked about unsupervised learning before when dealing with
<a href="/self-organising-maps-an-introduction">Self-Organising Maps</a>,
but just to recap. Unsupervised learning is when you have a dataset of
features with no pre-defined outcomes. You give it to an algorithm to
learn patterns in the data without knowing in advance what associations
you want it to learn.</p>
<p>So you're not trying to teach it to tell the difference between images
of cats and dogs; instead, you're trying to make it learn something
about the structure of the images, so it can find similar images without
explicitly knowing about cats and dogs.</p>
<p>K-means is a type of unsupervised learning method, specifically a type
of clustering.</p>
<h3>Clustering</h3>
<p>Clustering deals with finding groups of similar data points.</p>
<p>There are two criteria that make a "good" set of clusters:</p>
<ul>
<li><strong>Intra-cluster similarity</strong>. That is, all the data points within a
    cluster should be similar to each other (we'll deal with what
    'similar' means a bit later).</li>
<li><strong>Inter-cluster dissimilarity.</strong> That is, data points in one cluster
    should be sufficiently different from data points in another
    cluster.</li>
</ul>
<p>This is also what we're trying to achieve with k-means. It is only one
of the <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html">many types</a>
of clustering algorithm, but I've chosen it as it's popular as well as
being easy to understand and implement.</p>
<h3>Uses of Clustering</h3>
<p>What are some uses of clustering?</p>
<p>Finding similarities in your data that
you couldn't do by inspection has a lot of uses. The classic example is
"segmenting your customer base", that is identifying customers with
similar buying behaviours for better targeted advertising. Another form
of clustering, hierarchical clustering, is <a href="http://astronomy.swin.edu.au/cosmos/h/hierarchical+clustering">used in astronomy</a>.
You can even use it <a href="/analysing-london-house-prices/">to find similar boroughs in London based on house-buying behaviour</a>.</p>
<h3>Potential Problems</h3>
<h4>Choosing K</h4>
<p>Before you start training your data to learn the clusters, you need to
choose a value for <span class="math">\(k\)</span>. That is, you have to decide beforehand how
many groups there are going to be. This sounds like it defeats the
purpose of being unsupervised, and it is indeed something that you have
to set manually.</p>
<p>Due to the random nature of the initialisation of the algorithm, and the
uncertainty in the correct value for <span class="math">\(k\)</span>, you might find re-running
the algorithm multiple times gives you different results.</p>
<h4>Subjective Evaluation</h4>
<p>As with unsupervised algorithms in general, evaluating the outcome is
partly subjective. There are objective measures with which we can
compare different runs, but the final evaluation will be based on which
one <em>feels</em> best.</p>
<p>Sometimes you can look at the characteristics of the clusters to see
which one makes most sense. For example, if segmenting your customers
into clusters results in two clusters that both contain mostly customers
over 60, you might choose another run that better separates your
customers based on age. Of course, the clusters will be created based on
your data, so if there really are two distinct groups of over-60
customers then no amount of runs will change that!</p>
<h2>The K-means Algorithm (with words)</h2>
<p>Clusters have two properties: a <strong>centroid</strong> and a set of your data
points that are assigned to the cluster.</p>
<p>The centroid is simply a point which is the <strong>mean</strong> of the data points
that belong to it (hence, "k-means").</p>
<p>Mathematically, the centroids are a point in n-dimensional space, where
n is the number of features your data has.</p>
<p>The basic idea behind the k-means clustering algorithm is simple:</p>
<ol>
<li>Start with a chosen value of <span class="math">\(k\)</span>.</li>
<li>Choose <span class="math">\(k\)</span> of your data points at random to be your starting
    centroids.</li>
<li>For each data point, assign it to a cluster based on which of the
    <span class="math">\(k\)</span> centroids it is <em>closest</em> to. Closest can mean any distance
    measure. The Euclidean distance is often used.</li>
<li>Now you have <span class="math">\(k\)</span> groups of data points assigned to a cluster.
    Re-calculate the position of each cluster centroid by taking the
    <em>mean</em> of the new points that are now associated with that cluster.</li>
<li>Repeat steps 3 and 4 until convergence. You are typically done when
    no points have changed clusters since the last iteration.</li>
</ol>
<p>It's better to see this happen visually - <a href="https://www.naftaliharris.com/blog/visualizing-k-means-clustering">here's a good interactive example</a>.
 </p>
<h2>The K-means Algorithm (with code)</h2>
<p>Let's go through the steps again with code. Let's use <a href="http://archive.ics.uci.edu/ml/datasets/Iris">the iris dataset</a>.</p>
<h3>Steps 1 &amp; 2 - Initialisation</h3>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">)</span> <span class="c1"># I&#39;ll supply this alongside the Jupyter notebook</span>
<span class="c1"># we don&#39;t need the target variable</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># random initialisation of centroids = pick K data points at random as centroids</span>
<span class="n">init_centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">init_centroids</span><span class="p">:</span>
    <span class="c1"># get the data point at index i</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
    <span class="c1"># append it to the centroids list</span>
    <span class="n">centroids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div>

<h3>Steps 3 &amp; 4 - Learning</h3>
<div class="codehilite"><pre><span></span><code><span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">assign_to_cluster</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="c1"># calculate distance (without sqrt) to each centroid</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">centroids</span><span class="p">:</span>
        <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">(((</span><span class="n">point</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="c1"># find index of closest cluster</span>
    <span class="n">closest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
    <span class="c1"># assign point to that cluster</span>
    <span class="n">clusters</span><span class="p">[</span><span class="n">closest</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">closest</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="c1"># first, reset the clusters</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">clusters</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
    <span class="c1"># assign each data point to nearest cluster</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
        <span class="n">assign_to_cluster</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    
    <span class="c1"># now, recalculate the centroids</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<p>First we've defined a function that calculates the distance between a
data point and each of the cluster centroids. We can use the
implementation trick where we don't need the <em>actual</em> Euclidean distance
(to avoid the expensive square root operation). We then assign the data
point to that cluster.</p>
<p>Then we iteratively assign points to the clusters and re-position the
cluster centroids. With the new centroid positions, we assign points
again, then re-calculate the centroids again and so on.</p>
<p>In this example I've forced it to run just 20 times, so there might
still be room for improvement, but typically you'd run this until no
points have changed cluster since the last iteration.</p>
<p>Once we've done that we can plot two of the data's dimensions and colour
each point by its assigned cluster (and mark the cluster centroids).</p>
<p>We've gone from this plot of raw data:</p>
<p><img alt="Iris petal plot (no clusters)" src="/images/introduction-to-k-means-clustering/kmeans_iris_1.png" /></p>
<p>Plot of raw data before clustering</p>
<p>To this plot where we've clustered our points into 3 groups:</p>
<p><img alt="Iris data with 3 clusters" src="/images/introduction-to-k-means-clustering/kmeans_iris_2.png" /></p>
<p>3 clusters after just 20 iterations</p>
<p>There is still some overlap between the black and blue clusters, but
just 20 iterations have quite effectively grouped our data into 3
clusters.</p>
<p>That's all there is to it!</p>
<p>Here is the <a href="https://github.com/davidasboth/blog-notebooks/blob/master/k-means/K-Means.ipynb">associated Jupyter notebook</a>.</p>
<p>In Part 2, I'll talk about another practical application of the k-means
algorithm (using scikit-learn this time) as well as some implementation
details such as how to pick the value of <span class="math">\(k\)</span>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script>

        </div>
    </section>

<section id="about">
    <div class="container flex-container">
      <div class="col-40 image-full">
            <img src="/theme/images/David-Asboth-profile-image-square.jpg">
        </div>
      <div class="col-60">
        <h2>About David</h2>
        <p>I'm a freelance data scientist, consultant, and educator with an MSc. in Data Science and a background in software and web development. My previous roles have been a range of data science, software development, team management and software architecting jobs.</p>
        <p class="text-pre-heading">Things I also do:</p>
        <ul>
          <li>
            I co-host the <a href="https://halfstackdatascience.com">Half Stack Data Science podcast</a> where we talk about the realities of data science in the business world
            </li>
          <li>
            I've written various <a href="/articles">articles and tutorials</a> about data science
          </li>
          <li>
            I've given a selection of talks at large conferences and universities, all on similar topics of "real world data science"
            </li>
          <li>
            I occasionally stream some data science over on <a href="https://www.twitch.tv/jazzsloth">Twitch</a>, where I take a vague project idea, a dataset, and try to come up with an answer in about an hour, explaining the code and thought process as I go.
            </li>
          </ul>
        </div>
      </div>
    </section>
<section id="contact">
<div class="container flex-container">
  <div class="col-40 image-full">
	<img src="/theme/images/DataScienceFestival-02-contact.jpg">
	</div>
  <div class="col-60">
	<h2>Contact me</h2>
	<p>Please give me a brief idea of what you're looking for help with, and I'll get back to you very soon. If you're having trouble with the form below, you can also email me at <a href="mailto:hello@davidasboth.com" target="_blank">hello@davidasboth.com</a></p>
	<form action="https://api.formcake.com/api/form/184308c0-d548-4a26-ae33-db870931e16d/submission" method="POST">
	  <input type="text" name="name" placeholder="Name *" required>
	  <input type="email" name="email" placeholder="Email address *" required>
	  <select name="service-required" required>
		<option value="" disabled selected hidden>What are you looking for help with?</option>
		<option value="consulting-largeorg" >Consulting for a large organisation</option>
		<option value="consulting-smallbiz" >Consulting for a small business/startup</option>
		<option value="education-staff" >Education for staff members</option>
		<option value="mentoring-1to1" >Mentoring 1:1s for students/career newbies</option>
		 <option value="other" >Other</option>
		</select>
	  <textarea placeholder="Message *" name="message" required></textarea>
	  <input type="text" name="spiderweb" style="display: none">
		  <input type="submit" class="button button-filled" value="Get in touch!">
		  <a class="icon" href="https://twitter.com/davidasboth"><img src="/theme/images/icon-twitter.svg" alt="Twitter"/></a>
		  <a class="icon" href="https://www.linkedin.com/in/david-asboth-9256772b"><img src="/theme/images/icon-linkedin.svg" alt="LinkedIn"/></a>
	  </form>
	</div>
  </div>
</section></main>


	<footer>
		<div class="container flex-container">
			<p>© 2022 David Asboth. Site designed with love by <a href="http://design.barbasboth.com/">Barbara Asboth</a>. All Rights Reserved.</p>
			<p>hello@davidasboth.com</p>
		</div>
	</footer>
	<script>
	// Add the sticky class to the header when you reach its scroll position. Remove "sticky" when you leave the scroll position
		
	var header = document.getElementById("stickyHeader");

	// Get the offset position of the navbar
	var sticky = header.offsetTop + '180';
	function toggleStickyHeader() {
	  if (window.pageYOffset > sticky) {
		header.classList.add("sticky");
	  } else {
		header.classList.remove("sticky");
	  }
	}
	window.onscroll = function() {toggleStickyHeader()};
	</script>
</body>

</html>