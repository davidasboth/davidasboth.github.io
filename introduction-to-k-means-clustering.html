<!DOCTYPE html>
<html lang="en">

<head>
	<!-- Required meta tags always come first -->
	<meta charset="utf-8">

	<!-- Favicon stuff -->
	<link rel="apple-touch-icon" sizes="180x180" href="/theme/images/apple-touch-icon.png">
	<link rel="icon" type="image/png" sizes="32x32" href="/theme/images/favicon-32x32.png">
	<link rel="icon" type="image/png" sizes="16x16" href="/theme/images/favicon-16x16.png">
	<link rel="manifest" href="/theme/images/site.webmanifest">

	<meta http-equiv="x-ua-compatible" content="ie=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1">

	<title>David Asboth - Data Solutions &amp; Consultancy - Introduction to K-means Clustering</title>

	<link rel="canonical" href="/introduction-to-k-means-clustering.html">
	
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Atkinson+Hyperlegible:wght@400;700&display=swap" rel="stylesheet"> 
	
	<link rel="stylesheet" href="/theme/css/style.css">

  



    <meta name="tags" content="featured" />
    <meta name="tags" content="python" />

</head>

<body>
	<header>
		<div class="container">
			<a href="/"><img src="/theme/images/da-logo.svg" alt="David Asboth logo" id="header-logo"/></a>
			<nav>
				<ul>
					<li>
						<a href="/#solutions">Solutions</a>
					</li>
				<li>
						<a href="/#about">About</a>
					</li>
				<li>
						<a href="/articles">Articles</a>
					</li>
				<li>
				  <a href="#contact" class="button button-filled">Contact</a>
				  </li></ul>
			</nav>
		</div>
	</header>

<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="/introduction-to-k-means-clustering.html" rel="bookmark"
         title="Permalink to Introduction to K-means Clustering">Introduction to K-means Clustering</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2016-11-20T15:18:00+00:00">
      Sun 20 November 2016
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="/author/david.html">david</a>
    </address>
    <div class="category">
        Category: <a href="/category/machine-learning.html">machine learning</a>
    </div>
    <div class="tags">
        Tags:
            <a href="/tag/featured.html">featured</a>
            <a href="/tag/python.html">python</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p>This is the first of a two-part post on K-means clustering.</p>
<h1>Introduction</h1>
<h2>Unsupervised Learning</h2>
<p>I've talked about unsupervised learning before when dealing with
<a href="/blog/self-organising-maps-an-introduction/">Self-Organising Maps</a>,
but just to recap. Unsupervised learning is when you have a dataset of
features with no pre-defined outcomes. You give it to an algorithm to
learn patterns in the data without knowing in advance what associations
you want it to learn.</p>
<p>So you're not trying to teach it to tell the difference between images
of cats and dogs; instead, you're trying to make it learn something
about the structure of the images, so it can find similar images without
explicitly knowing about cats and dogs.</p>
<p>K-means is a type of unsupervised learning method, specifically a type
of clustering.</p>
<h2>Clustering</h2>
<p>Clustering deals with finding groups of similar data points.</p>
<p>There are two criteria that make a "good" set of clusters:</p>
<ul>
<li><strong>Intra-cluster similarity</strong>. That is, all the data points within a
    cluster should be similar to each other (we'll deal with what
    'similar' means a bit later).</li>
<li><strong>Inter-cluster dissimilarity.</strong> That is, data points in one cluster
    should be sufficiently different from data points in another
    cluster.</li>
</ul>
<p>This is also what we're trying to achieve with k-means. It is only one
of the <a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html">many types</a>
of clustering algorithm, but I've chosen it as it's popular as well as
being easy to understand and implement.</p>
<h2>Uses of Clustering</h2>
<p>What are some uses of clustering? Finding similarities in your data that
you couldn't do by inspection has a lot of uses. The classic example is
"segmenting your customer base", that is identifying customers with
similar buying behaviours for better targeted advertising. Another form
of clustering, hierarchical clustering, is <a href="http://astronomy.swin.edu.au/cosmos/h/hierarchical+clustering">used in astronomy</a>.
You can even use it <a href="/blog/analysing-london-house-prices/">to find similar boroughs in London based on house-buying behaviour</a>.</p>
<h2>Potential Problems</h2>
<h3>Choosing K</h3>
<p>Before you start training your data to learn the clusters, you need to
choose a value for <span class="math">\(k\)</span>. That is, you have to decide beforehand how
many groups there are going to be. This sounds like it defeats the
purpose of being unsupervised, and it is indeed something that you have
to set manually.</p>
<p>Due to the random nature of the initialisation of the algorithm, and the
uncertainty in the correct value for <span class="math">\(k\)</span>, it is advisable to re-run
the algorithm multiple times and decide on which configuration to use.</p>
<h3>Subjective Evaluation</h3>
<p>As with unsupervised algorithms in general, evaluating the outcome is
partly subjective. There are objective measures with which we can
compare different runs, but the final evaluation will be based on which
one <em>feels</em> best.</p>
<p>Sometimes you can look at the characteristics of the clusters to see
which one makes most sense. For example, if segmenting your customers
into clusters results in two clusters that both contain mostly customers
over 60, you might choose another run that better separates your
customers based on age. Of course, the clusters will be created based on
your data, so if there really are two distinct groups of over-60
customers then no amount of runs will change that!</p>
<h1>The K-means Algorithm (with words)</h1>
<p>Clusters have two properties: a <strong>centroid</strong> and a set of your data
points that are assigned to the cluster.</p>
<p>The centroid is simply a point which is the <strong>mean</strong> of the data points
that belong to it (hence, "k-means").</p>
<p>Mathematically, the centroids are a point in n-dimensional space, where
n is the number of features your data has.</p>
<p>The basic idea behind the k-means clustering algorithm is simple:</p>
<ol>
<li>Start with a chosen value of <span class="math">\(k\)</span>.</li>
<li>Choose <span class="math">\(k\)</span> of your data points at random to be your starting
    centroids.</li>
<li>For each data point, assign it to a cluster based on which of the
    <span class="math">\(k\)</span> centroids it is <em>closest</em> to. Closest can mean any distance
    measure. The Euclidean distance is often used.</li>
<li>Now you have <span class="math">\(k\)</span> groups of data points assigned to a cluster.
    Re-calculate the position of each cluster centroid by taking the
    <em>mean</em> of the new points that are now associated with that cluster.</li>
<li>Repeat steps 3 and 4 until convergence. You are typically done when
    no points have changed clusters since the last iteration.</li>
</ol>
<p>It's better to see this happen visually - <a href="http://www.onmyphd.com/?p=k-means.clustering#h3_goodexample">here's a good example</a>.
 </p>
<h1>The K-means Algorithm (with code)</h1>
<p>Let's go through the steps again with code. Let's use <a href="http://archive.ics.uci.edu/ml/datasets/Iris">the iris dataset</a>.</p>
<h2>Steps 1 &amp; 2 - Initialisation</h2>
<div class="highlight"><pre><span></span><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">)</span> <span class="c1"># I&#39;ll supply this alongside the Jupyter notebook</span>
<span class="c1"># we don&#39;t need the target variable</span>
<span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;class&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="n">k</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">centroids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># random initialisation of centroids = pick K data points at random as centroids</span>
<span class="n">init_centroids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)),</span> <span class="n">size</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">init_centroids</span><span class="p">:</span>
    <span class="c1"># get the data point at index i</span>
    <span class="n">pt</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">,:]</span>
    <span class="c1"># append it to the centroids list</span>
    <span class="n">centroids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pt</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
</code></pre></div>

<h2>Steps 3 &amp; 4 - Learning</h2>
<div class="highlight"><pre><span></span><code><span class="n">n_iterations</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">def</span> <span class="nf">assign_to_cluster</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
    <span class="n">point</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
    <span class="c1"># calculate distance (without sqrt) to each centroid</span>
    <span class="n">distances</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">centroids</span><span class="p">:</span>
        <span class="n">distances</span><span class="o">.</span><span class="n">append</span><span class="p">(((</span><span class="n">point</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">())</span>
    <span class="c1"># find index of closest cluster</span>
    <span class="n">closest</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmin</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
    <span class="c1"># assign point to that cluster</span>
    <span class="n">clusters</span><span class="p">[</span><span class="n">closest</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">point</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">idx</span><span class="p">,</span><span class="s1">&#39;cluster&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">closest</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iterations</span><span class="p">):</span>
    <span class="c1"># first, reset the clusters</span>
    <span class="n">clusters</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">clusters</span><span class="o">.</span><span class="n">append</span><span class="p">([])</span>
    <span class="c1"># assign each data point to nearest cluster</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="p">)):</span>
        <span class="n">assign_to_cluster</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    
    <span class="c1"># now, recalculate the centroids</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">k</span><span class="p">):</span>
        <span class="n">centroids</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">clusters</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div>

<p>First we've defined a function that calculates the distance between a
data point and each of the cluster centroids. We can use the
implementation trick where we don't need the <em>actual</em> Euclidean distance
(to avoid the expensive square root operation). We then assign the data
point to that cluster.</p>
<p>Then we iteratively assign points to the clusters and re-position the
cluster centroids. With the new centroid positions, we assign points
again, then re-calculate the centroids again and so on.</p>
<p>In this example I've forced it to run just 20 times, so there might
still be room for improvement, but typically you'd run this until no
points have changed cluster since the last iteration.</p>
<p>Once we've done that we can plot two of the data's dimensions and colour
each point by its assigned cluster (and mark the cluster centroids).</p>
<p>We've gone from this plot of raw data:</p>
<p><img alt="Iris petal plot (no clusters)" src="/images/introduction-to-k-means-clustering/kmeans_iris_1.png"></p>
<p>Plot of raw data before clustering</p>
<p>To this plot where we've clustered our points into 3 groups:</p>
<p><img alt="Iris data with 3 clusters" src="/images/introduction-to-k-means-clustering/kmeans_iris_2.png"></p>
<p>3 clusters after just 20 iterations</p>
<p>There is still some overlap between the black and blue clusters, but
just 20 iterations have quite effectively grouped our data into 3
clusters.</p>
<p>That's all there is to it!</p>
<p>Here is the <a href="https://github.com/davidasboth/blog-notebooks/blob/master/k-means/K-Means.ipynb">associated Jupyter notebook</a>.</p>
<p>In Part 2, I'll talk about another practical application of the k-means
algorithm (using scikit-learn this time) as well as some implementation
details such as how to pick the value of <span class="math">\(k\)</span>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script><script src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
  </div><!-- /.entry-content -->
</section>

	<footer>
		<div class="container flex-container">
			<p>© 2022 David Asboth.  All Rights Reserved.</p>
			<p>hello@davidasboth.com</p>
		</div>
	</footer>
	
	<script>
	  /*(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-68436188-1', 'auto');
	  ga('send', 'pageview');*/

	</script>
</body>

</html>