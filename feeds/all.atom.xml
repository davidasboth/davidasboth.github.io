<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>David Asboth | Data Science</title><link href="/" rel="alternate"></link><link href="/feeds/all.atom.xml" rel="self"></link><id>/</id><updated>2017-04-17T13:48:00+01:00</updated><entry><title>Machine Learning Haikus</title><link href="/blog/machine-learning-haikus" rel="alternate"></link><published>2017-04-17T13:48:00+01:00</published><updated>2017-04-17T13:48:00+01:00</updated><author><name>david</name></author><id>tag:None,2017-04-17:/blog/machine-learning-haikus</id><summary type="html">&lt;p&gt;You know how there are lots of blog posts out there about machine
learning algorithms "in plain English"? They're popular because many
people want to learn about machine learning, but don't want to be
bombarded with heavy maths the moment they start. I agree, I think
machine learning should be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;You know how there are lots of blog posts out there about machine
learning algorithms "in plain English"? They're popular because many
people want to learn about machine learning, but don't want to be
bombarded with heavy maths the moment they start. I agree, I think
machine learning should be taught &lt;a href="/blog/intuition-first-machine-learning/"&gt;intuition first&lt;/a&gt;.
I also recognise that people are busy and don't want to spend hours
reading up on algorithms to understand them.&lt;/p&gt;
&lt;p&gt;So forget "machine learning in plain English". Instead, I present some
of the most popular algorithms in haiku form. Consider it "machine
learning for the busy".&lt;/p&gt;
&lt;h2&gt;Logistic Regression&lt;/h2&gt;
&lt;p&gt;Inputs weighted, summed.&lt;br&gt;
Passed through logistic function.&lt;br&gt;
Shall we output 1?&lt;/p&gt;
&lt;h2&gt;Random Forest&lt;/h2&gt;
&lt;p&gt;Oh, decision trees!&lt;br&gt;
Alone you're not so useful.&lt;br&gt;
How 'bout a forest?&lt;/p&gt;
&lt;h2&gt;Support Vector Machines&lt;/h2&gt;
&lt;p&gt;Binary outcomes.&lt;br&gt;
Maximum separation,&lt;br&gt;
Is what is needed.&lt;/p&gt;
&lt;h2&gt;Linear Regression&lt;/h2&gt;
&lt;p&gt;Weighted X is summed.&lt;br&gt;
Perhaps regularised, too.&lt;br&gt;
Gives me real numbers.&lt;/p&gt;
&lt;h2&gt;Neural Networks&lt;/h2&gt;
&lt;p&gt;Backpropagation:&lt;br&gt;
Works well, is a mystery.&lt;br&gt;
But Geoff Hinton knows.&lt;/p&gt;
&lt;h2&gt;K Nearest Neighbours&lt;/h2&gt;
&lt;p&gt;Nothing to learn here.&lt;br&gt;
Find the nearest data points,&lt;br&gt;
And use the average.&lt;/p&gt;
&lt;h2&gt;K-means Clustering&lt;/h2&gt;
&lt;p&gt;Find similar things&lt;br&gt;
By grouping based on distance&lt;br&gt;
There's no "right" answer.&lt;/p&gt;
&lt;h2&gt;Markov Chains&lt;/h2&gt;
&lt;p&gt;Mr Markov, your&lt;br&gt;
Use of probabilities&lt;br&gt;
Gave us funny text.&lt;/p&gt;
&lt;h2&gt;Naive Bayes&lt;/h2&gt;
&lt;p&gt;We're gonna pretend&lt;br&gt;
Features are independent,&lt;br&gt;
Naive as that is.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category></entry><entry><title>Analysis: Is Alan Davies Getting Better at QI?</title><link href="/blog/analysis-is-alan-davies-getting-better-at-qi" rel="alternate"></link><published>2017-02-15T12:06:00+00:00</published><updated>2017-02-15T12:06:00+00:00</updated><author><name>david</name></author><id>tag:None,2017-02-15:/blog/analysis-is-alan-davies-getting-better-at-qi</id><summary type="html">&lt;p&gt;I'm a big fan of the quiz show &lt;a href="https://en.wikipedia.org/wiki/QI"&gt;QI&lt;/a&gt;. I
was watching a later series of it recently on Netflix and I couldn't
help but notice that Alan Davies was winning quite a few episodes. This
felt odd, because I was sure that when I first watched the show he …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I'm a big fan of the quiz show &lt;a href="https://en.wikipedia.org/wiki/QI"&gt;QI&lt;/a&gt;. I
was watching a later series of it recently on Netflix and I couldn't
help but notice that Alan Davies was winning quite a few episodes. This
felt odd, because I was sure that when I first watched the show he
routinely finished last, and certainly wasn't winning any shows.&lt;/p&gt;
&lt;p&gt;That prompted me to ask the question: &lt;em&gt;is Alan Davies getting better at
QI?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;To sate my curiosity I tried to answer that question the best way I know
how: with &lt;strong&gt;data&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;The data comes from &lt;a href="https://www.comedy.co.uk/"&gt;The British Comedy Guide&lt;/a&gt;, which has an exhaustive list of
everything related to British comedy, including every episode of QI.
There was a lot of laborious data cleaning involved, which you can see
for yourself in &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/qi-analysis/Scrape%20QI%20Episodes.ipynb"&gt;the associated Jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/qi-analysis/qi_episodes.csv"&gt;final dataset&lt;/a&gt;
includes every episode with its title, broadcast date and each
contestant and their scores.&lt;/p&gt;
&lt;h1&gt;The Analysis&lt;/h1&gt;
&lt;p&gt;As with many data science projects, once the dataset was nice and clean
the question was straightforward to answer. All it needed was a plot to
show Alan's win ratio over time. Here's the accompanying &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/qi-analysis/QI%20Analysis.ipynb"&gt;Jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Alan Davies's QI performance over time" src="/images/analysis-is-alan-davies-getting-better-at-qi/alan_davies_over_time.png"&gt;&lt;/p&gt;
&lt;p&gt;Alan's more or less consistently winning a quarter of shows now&lt;/p&gt;
&lt;p&gt;So to answer the question: yes, Alan does appear to be getting better at
QI, certainly since the first few series. It does however remain to be
seen whether his win ratio will plateau at 25%, I guess we'll see in a
few years' time.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Visualising the Worldwide Win Percentage of the Hungarian National Football Team</title><link href="/blog/visualising-the-worldwide-win-percentage-of-the-hungarian-national-football-team" rel="alternate"></link><published>2017-01-20T11:36:00+00:00</published><updated>2017-01-20T11:36:00+00:00</updated><author><name>david</name></author><id>tag:None,2017-01-20:/blog/visualising-the-worldwide-win-percentage-of-the-hungarian-national-football-team</id><summary type="html">&lt;p&gt;I've often read, and agreed with, the advice that side projects should
be solving problems or answering questions that you yourself are
interested in.&lt;/p&gt;
&lt;p&gt;To that end, I've always wanted to know how well the Hungarian national
team have done against various countries worldwide. Hungary has &lt;a href="https://en.wikipedia.org/wiki/Golden_Team"&gt;a
glorious football past …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've often read, and agreed with, the advice that side projects should
be solving problems or answering questions that you yourself are
interested in.&lt;/p&gt;
&lt;p&gt;To that end, I've always wanted to know how well the Hungarian national
team have done against various countries worldwide. Hungary has &lt;a href="https://en.wikipedia.org/wiki/Golden_Team"&gt;a
glorious football past&lt;/a&gt;,
although the last 30 years have been possibly the worst ever decades in
Hungarian football.&lt;/p&gt;
&lt;p&gt;To explore this question, I scraped a dataset of all matches played by
the Hungarian national team since 1907, aggregated it and made an
interactive world map.&lt;/p&gt;
&lt;p&gt;I'll start with the final map, and you can read about some of the
details beneath.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hungarian National Team worldwide win percentage" src="/images/visualising-the-worldwide-win-percentage-of-the-hungarian-national-football-team/hungarian-nt-win-map.png"&gt;&lt;/p&gt;
&lt;p&gt;The worldwide win percentage of the national team&lt;/p&gt;
&lt;p&gt;There is &lt;a href="https://plot.ly/~dasboth/0.embed"&gt;an interactive version&lt;/a&gt;
where you can zoom and hover over each country to find out more. I
encourage you to look at it if you want to dive into the data and
explore. &lt;/p&gt;
&lt;h1&gt;The Details&lt;/h1&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;I scraped the data from the wonderful
&lt;a href="http://www.magyarfutball.hu/"&gt;http://www.magyarfutball.hu&lt;/a&gt; which is a
great resource for those with the very niche interest in Hungarian
football. The data was unsurprisingly in Hungarian so I also had to
translate it!&lt;/p&gt;
&lt;p&gt;There is &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/hungarian-national-team/Scraping%20NT%20Data.ipynb"&gt;a Jupyter notebook&lt;/a&gt;
for the scraping code and the &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/hungarian-national-team/hungarian_nt_matches.csv"&gt;final dataset&lt;/a&gt;
is also available.&lt;/p&gt;
&lt;h2&gt;The Map&lt;/h2&gt;
&lt;p&gt;My &lt;a href="/blog/the-world-map-of-the-2016-fifa-awards/"&gt;last experiment&lt;/a&gt;
making a &lt;a href="https://en.wikipedia.org/wiki/Choropleth_map"&gt;choropleth map&lt;/a&gt;
was missing some features I'd have liked to add, such as more
interactivity. The library I used, folium, is still under development so
for this project I tried something new.&lt;/p&gt;
&lt;p&gt;Introducing &lt;a href="http://plot.ly/"&gt;plot.ly&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;It's a great visualisation toolkit with a Python wrapper. It is much
easier to make interactive choropleths with it, I specifically used
&lt;a href="https://plot.ly/python/choropleth-maps/"&gt;this set of tutorials&lt;/a&gt;. The
ability to customise the hover label meant I could make a much more
useful visualisation, which you can explore.&lt;/p&gt;
&lt;p&gt;There is &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/hungarian-national-team/Mapping%20Records%20vs%20Countries.ipynb"&gt;a Jupyter notebook&lt;/a&gt;
where I aggregated the data, dealt with countries that don't exist
anymore (the USSR, Yugoslavia, etc.) and made the final map.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="datavis"></category><category term="featured"></category><category term="python"></category></entry><entry><title>The World Map of the 2016 FIFA Awards</title><link href="/blog/the-world-map-of-the-2016-fifa-awards" rel="alternate"></link><published>2017-01-13T15:06:00+00:00</published><updated>2017-01-13T15:06:00+00:00</updated><author><name>david</name></author><id>tag:None,2017-01-13:/blog/the-world-map-of-the-2016-fifa-awards</id><summary type="html">&lt;p&gt;The "&lt;a href="http://www.fifa.com/the-best-fifa-football-awards/best-fifa-mens-player/index.html"&gt;Best FIFA Football Awards&lt;/a&gt;"
took place recently and while the final outcome was not surprising, I've
always wanted to know which countries vote for which players.&lt;/p&gt;
&lt;p&gt;The way the voting works is that the captain and coach of every national
team in FIFA, as well as a member of …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The "&lt;a href="http://www.fifa.com/the-best-fifa-football-awards/best-fifa-mens-player/index.html"&gt;Best FIFA Football Awards&lt;/a&gt;"
took place recently and while the final outcome was not surprising, I've
always wanted to know which countries vote for which players.&lt;/p&gt;
&lt;p&gt;The way the voting works is that the captain and coach of every national
team in FIFA, as well as a member of the media for each country, gets to
vote for their top 3 players, giving them 5, 3, and 1 point(s)
respectively. Like I said, the result was not surprising:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Top players in the FIFA awards" src="/images/the-world-map-of-the-2016-fifa-awards/fifa_topplayers.png"&gt;&lt;/p&gt;
&lt;p&gt;The top 5 highest scoring players in the FIFA awards&lt;/p&gt;
&lt;p&gt;FIFA routinely release all the votes, and I wanted to explore the data
further. Specifically, I thought there might be some interesting
geographic patterns (e.g. all of South America voting for South American
players and so on), so this was a map plot waiting to happen.
 &lt;/p&gt;
&lt;h1&gt;The Data&lt;/h1&gt;
&lt;p&gt;FIFA weren't going to make this easy; the dataset is &lt;a href="http://resources.fifa.com/mm/Document/the-best/PlayeroftheYear-Men/02/86/27/05/faward_MenPlayer2016_Neutral.pdf"&gt;a PDF&lt;/a&gt;
with tables in it. While there appear to be a few ways to extract tables
in Python, none of them worked for extracting these tables, and neither
did highlighting it and trying to paste it into Excel.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://tabula.technology/"&gt;Tabula&lt;/a&gt; to the rescue! Tabula is a great
open source tool for automatically finding tables in PDFs and it worked
perfectly.&lt;/p&gt;
&lt;p&gt;To save anyone else this trouble, here's &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/fifa-awards/player_votes.csv"&gt;the final csv file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Not all countries had all three vote types (captain, coach, media) and I
wanted to use a single vote for each country, so I used the priority
order of coach -&amp;gt; captain -&amp;gt; media. This is why for example the
vote for Argentina isn't for Messi, because the coach vote was missing
and Messi himself is the captain who couldn't vote for himself.&lt;/p&gt;
&lt;h1&gt;The Tools&lt;/h1&gt;
&lt;p&gt;I wanted this to be a chance to explore
&lt;a href="https://github.com/python-visualization/folium"&gt;folium&lt;/a&gt;, a great Python
package for making maps.&lt;/p&gt;
&lt;p&gt;Folium supports choropleths, but you have to provide a JSON file with
topological information. I used data from &lt;a href="https://gist.github.com/markmarkoh/2969317"&gt;this gist&lt;/a&gt; but had to match each
country to the countries in the FIFA tables, because they didn't always
match and there are a few countries that weren't present in both data
sources.&lt;/p&gt;
&lt;p&gt;One modification I had to make is that it's not currently possible to
add an ad hoc legend into a folium map, so I built an HTML legend and
injected it myself. I knew that HTML knowledge would come in handy for
data science!&lt;/p&gt;
&lt;p&gt;The code (hopefully) speaks for itself, you can view &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/fifa-awards/FIFA%20Awards%20data.ipynb"&gt;the Jupyter notebook on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;The Map&lt;/h1&gt;
&lt;p&gt;Here's a little preview of the final map, although &lt;a href="/fifa-awards/"&gt;the HTML version&lt;/a&gt; is better for exploring
different regions.&lt;/p&gt;
&lt;p&gt;&lt;img alt="FIFA Awards World Map" src="/images/the-world-map-of-the-2016-fifa-awards/fifa_awards_finalmap.png"&gt;&lt;/p&gt;
&lt;p&gt;That Griezmann sure is popular in Mongolia &lt;/p&gt;
&lt;h1&gt;Wrap-up&lt;/h1&gt;
&lt;p&gt;Unsurprisingly, a lot of countries vote for their own players where they
have players good enough. I'm looking at Sweden, Germany, or Poland
here. I'm unsure if this is out of national pride or whether they
believe their players are genuinely better than Messi or Ronaldo,
probably the former. Brazil interestingly bit the bullet and voted for
someone else despite having Neymar as a reasonable option.&lt;/p&gt;
&lt;p&gt;Folium doesn't currently support hover actions, but a nice addition
would be to be able to hover over a country to see who they voted for,
especially for the "Other" category. It might be in a future version of
folium, as it seems to be actively under development. Realistically, a
tool like Tableau would be better suited to a visualisation like this,
but the ability to embed maps in Jupyter notebooks makes folium a really
good addition to my data science toolkit.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="datavis"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Lessons Learned from Blogging for 30 Days</title><link href="/blog/30-day-blog-challenge-complete" rel="alternate"></link><published>2016-11-30T19:03:00+00:00</published><updated>2016-11-30T19:03:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-30:/blog/30-day-blog-challenge-complete</id><summary type="html">&lt;p&gt;I did it!&lt;/p&gt;
&lt;p&gt;I wrote 30 blog posts in 30 days (including this one).&lt;/p&gt;
&lt;p&gt;I'm including this one because I wanted to look back at what I got out
of this little experiment in the hopes that it will be useful to others,
so I intend this post to be …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I did it!&lt;/p&gt;
&lt;p&gt;I wrote 30 blog posts in 30 days (including this one).&lt;/p&gt;
&lt;p&gt;I'm including this one because I wanted to look back at what I got out
of this little experiment in the hopes that it will be useful to others,
so I intend this post to be a useful edition to my blog rather than some
sort of self-backpatting (not a real word).&lt;/p&gt;
&lt;h1&gt;The Positives&lt;/h1&gt;
&lt;p&gt;Let's start with the positives.&lt;/p&gt;
&lt;h2&gt;They turned out OK&lt;/h2&gt;
&lt;p&gt;I did this little experiment in the spirit of NaNoWriMo, where people
write a 50,000+ word novel in a month.&lt;/p&gt;
&lt;p&gt;Now, I know that if I dedicated an entire month to nothing but writing
and managed a 50,000+ word novel, it wouldn't be good. It feels like a
goal so unrealistic that I'd lose the motivation very soon and write
rubbish.&lt;/p&gt;
&lt;p&gt;I was worried that this logic would carry over to this project; that
after 30 days I'd be so fed up of writing blog posts that they'd start
turning out terribly.&lt;/p&gt;
&lt;p&gt;I'd like to think that there wasn't a visible drop in quality over the
30 days. I mean, I managed to write them all in full sentences, stayed
on-topic, and didn't resort to rambling about random subjects.&lt;/p&gt;
&lt;h2&gt;I have 30 more blog posts than I started with&lt;/h2&gt;
&lt;p&gt;Obviously this was the entire purpose of the exercise, but because I
feel they turned out OK, I have 30 more &lt;strong&gt;usable&lt;/strong&gt; blog posts than I
started with.&lt;/p&gt;
&lt;p&gt;At my previous pace it would have taken me decades to generate this much
content.&lt;/p&gt;
&lt;h2&gt;No time to overthink&lt;/h2&gt;
&lt;p&gt;Having a daily deadline really got rid of all the distractions and
self-doubt that plagued my blogging efforts before. I had no time to
think about whether a topic was worth discussing - if it was related,
and I felt I could write something useful about it, it was good enough.&lt;/p&gt;
&lt;p&gt;"Good enough" is a concept I'm better friends with now.&lt;/p&gt;
&lt;h2&gt;Putting yourself out there&lt;/h2&gt;
&lt;p&gt;I tweeted about every blog post to make sure I was keeping myself
honest. It created a habit that helped me focus on getting the next post
written.&lt;/p&gt;
&lt;h1&gt;The Negatives&lt;/h1&gt;
&lt;p&gt;Obviously things weren't all rosy.&lt;/p&gt;
&lt;h2&gt;Make it STOP!&lt;/h2&gt;
&lt;p&gt;This was a &lt;strong&gt;long&lt;/strong&gt; month. Having even an hour commitment each evening
was hard work sometimes.&lt;/p&gt;
&lt;p&gt;Being an adult is hard.&lt;/p&gt;
&lt;h2&gt;Do I even have 30 ideas?&lt;/h2&gt;
&lt;p&gt;I'm not going to lie, there were some days I struggled to think of what
to write about.&lt;/p&gt;
&lt;p&gt;I had maybe 7-8 topics I knew I wanted to write about beforehand, and a
few small machine learning tutorials I wanted to write up, but the other
two thirds of the time the posts were conceived on the day or the day
before.&lt;/p&gt;
&lt;p&gt;If I were to do this for another 30 days I might simply run out of
ideas.&lt;/p&gt;
&lt;h2&gt;Big topics, small posts&lt;/h2&gt;
&lt;p&gt;Sometimes I had the opposite problem. I thought of a good topic to
tackle, but due to the time pressure probably didn't do it justice. Or I
responded to a piece of news, such as &lt;a href="/blog/all-your-web-data-are-belong-to-us/"&gt;the UK's IP bill&lt;/a&gt;,
but would have liked to write a more complete, well-researched view of
it. I actually &lt;a href="/blog/more-on-the-snoopers-charter/"&gt;did that with the IP bill&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Conclusion and Advice to Future Bloggers (or past me)&lt;/h1&gt;
&lt;p&gt;All in all, doing something like this forced me out of my comfort zone.&lt;/p&gt;
&lt;p&gt;For a whole month I regularly had to create content and put it out there
without the level of scrutiny and deliberation I normally would have
subjected it to.&lt;/p&gt;
&lt;p&gt;I'm now more comfortable with my posts being good &lt;em&gt;enough&lt;/em&gt; instead of
always aiming for 'perfection'.&lt;/p&gt;
&lt;p&gt;I realised I had lots of code snippets, Jupyter notebooks, and
half-finished machine learning tutorials lying around that I never got
around to writing up. Now they're out there.&lt;/p&gt;
&lt;p&gt;I actually enjoy writing and, apart from a few days where I wished I
didn't have this task looming over me, I'm glad I've been writing so
much recently.&lt;/p&gt;
&lt;p&gt;The momentum is with me, although I will almost certainly stop producing
daily posts (I also have another writing challenge to finish in the near
future, in the form of a Master's thesis). &lt;/p&gt;
&lt;p&gt;So my advice to anyone reading this, including a past version of me,
would be to &lt;strong&gt;&lt;a href="/blog/just-do-stuff/"&gt;do it&lt;/a&gt;&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;If you're struggling to add to your blog/portfolio, want a boost to help
you complete half-finished code projects, or you want to get over your
insistence on perfection (I'm looking at you, past me), start a similar
challenge!&lt;/p&gt;
&lt;p&gt;It doesn't have to be a whole month, start with less if you want. As
I've alluded to before, starting is almost certainly always the hardest
part.&lt;/p&gt;
&lt;p&gt;To get you going, here's some text that you can copy-and-paste into a
tweet to announce your challenge to the world. Fill in the gaps where
appropriate:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I'm starting a blogging challenge: ___ posts in ___ days. My
first topic will be ____, watch this space!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As silly and patronising as this might appear, the "secret" is to reduce
the barrier to entry, to remove obstacles. I've reduced it now to almost
just Ctrl+C, Ctrl+V. No excuses.&lt;/p&gt;
&lt;p&gt;Once that tweet is out there, you'd be surprised at how much focus and
discipline you can muster to make sure you actually complete your
challenge.&lt;/p&gt;
&lt;p&gt;Here's to more blogging!&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category><category term="featured"></category></entry><entry><title>Method Chaining in Pandas</title><link href="/blog/method-chaining-in-pandas" rel="alternate"></link><published>2016-11-30T18:14:00+00:00</published><updated>2016-11-30T18:14:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-30:/blog/method-chaining-in-pandas</id><summary type="html">&lt;p&gt;When you work with pandas, you'll often perform multiple operations on a
DataFrame. Some data cleaning and basic plotting for example, something
like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;column …&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;When you work with pandas, you'll often perform multiple operations on a
DataFrame. Some data cleaning and basic plotting for example, something
like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;column_3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
&lt;span class="n"&gt;by_address&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;by_address&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That works fine, you're incrementally changing the DataFrame until
you're ready for aggregation and plotting.&lt;/p&gt;
&lt;p&gt;There is an alternative, which you might find more readable.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  \  
 &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;column_3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;  \  
 &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;  \  
 &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Why can we keep chaining methods together like this?&lt;/p&gt;
&lt;p&gt;In pandas, each of those functions returns a &lt;em&gt;copy&lt;/em&gt; of the modified
DataFrame, which is why we were setting it back to the df variable each
time. By chaining methods together we're just calling the next method on
the modified DataFrame until we're done.&lt;/p&gt;
&lt;p&gt;Notice we have to break lines with a backslash character to allow the
chain to go over multiple lines. You can avoid that by putting the
entire chain in brackets:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;column_3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;groupby&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
  &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;kind&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bar&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Logically and computationally these examples are equivalent, so this is
mostly just a stylistic consideration.&lt;/p&gt;
&lt;h1&gt;Benefits&lt;/h1&gt;
&lt;h2&gt;Readability&lt;/h2&gt;
&lt;p&gt;I'd argue the second method looks better, it actually reads
right-to-left (or up-to-down I suppose) and you can understand logically
what you're doing each time.&lt;/p&gt;
&lt;h2&gt;No Intermediate Variables&lt;/h2&gt;
&lt;p&gt;In the first example we had to either save back the modified DataFrame
to the original df variable, or create a new one each time. This means
you have to think about whether you want to store the DataFrame in each
of its states &lt;em&gt;and&lt;/em&gt; come up with descriptive names for them, and as we
all know...&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;There are two hard things in computer science: cache invalidation, naming things, and off-by-one errors.&lt;/p&gt;&amp;mdash; Jeff Atwood (@codinghorror) &lt;a href="https://twitter.com/codinghorror/status/506010907021828096?ref_src=twsrc%5Etfw"&gt;August 31, 2014&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;h2&gt;Avoids "inplace" Confusion&lt;/h2&gt;
&lt;p&gt;In my experience the fact that most DataFrame methods return a copy of
the DataFrame is actually confusingly counterintuitive for pandas
beginners. You either have to keep saving your DataFrame back to the
same variable, or use the "inplace" keyword. Using method chaining means
you only have to consider this problem once, i.e. set the final result
to a variable, without ever accidentally throwing away any of your
changes.&lt;/p&gt;
&lt;h1&gt;Downsides&lt;/h1&gt;
&lt;h2&gt;No Intermediate Variables&lt;/h2&gt;
&lt;p&gt;Not having access to the intermediate states of the method can also be a
downside. If you want to reuse any of the intermediate steps in the
process, you need to keep a copy of it so you might not want to use
method chaining all the time.&lt;/p&gt;
&lt;h2&gt;Debugging is Hard&lt;/h2&gt;
&lt;p&gt;Debugging a problem in a long method chain is hard. If your chain
consists of many intermediate steps and the final output is wrong, or
you get an error message, it can be hard to retrace your steps to see
what went wrong. If you had each command line by line, like in the first
example, you could step through the code with a debugger or simply run
the commands one at a time until you find the problem.&lt;/p&gt;
&lt;h2&gt;You Can Get Carried Away&lt;/h2&gt;
&lt;p&gt;You can take method chaining to extremes...&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_time_series.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# take a deep breath...&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;column_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;column_3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;})&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fillna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;price&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_index&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;resample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;M&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
   &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Maybe that's a bit much.&lt;/p&gt;
&lt;h1&gt;Best of Both Worlds&lt;/h1&gt;
&lt;p&gt;There is a time and a place for method chaining.&lt;/p&gt;
&lt;p&gt;If you don't care about intermediate steps, and just want a basic plot
for example, it's a good option.&lt;/p&gt;
&lt;p&gt;If you want to do complex operations that might need serious debugging,
maybe it should be avoided.&lt;/p&gt;
&lt;p&gt;What I tend to do is avoid using it while I'm still writing the code,
and refactor to use method chaining when I'm confident the code works.
The idea is that it helps future readability, so I can better understand
my code if I look back on it later.&lt;/p&gt;
&lt;p&gt;This post was mostly inspired by the great &lt;a href="https://tomaugspurger.github.io/method-chaining.html"&gt;Modern Pandas series.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Footnote: This was the 29&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="programming for data scientists"></category><category term="featured"></category><category term="pandas"></category><category term="python"></category></entry><entry><title>Visualising Decision Trees in Python</title><link href="/blog/visualising-decision-trees-in-python" rel="alternate"></link><published>2016-11-28T22:03:00+00:00</published><updated>2016-11-28T22:03:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-28:/blog/visualising-decision-trees-in-python</id><summary type="html">&lt;p&gt;Interpretability is often an important concern with a machine learning
algorithm (despite spellcheck telling me it's not even a word). Having
an accurate predictive model may be enough in itself, but in some cases
the only way to turn it into a business decision is if you can
understand &lt;strong&gt;why …&lt;/strong&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;Interpretability is often an important concern with a machine learning
algorithm (despite spellcheck telling me it's not even a word). Having
an accurate predictive model may be enough in itself, but in some cases
the only way to turn it into a business decision is if you can
understand &lt;strong&gt;why&lt;/strong&gt; it's getting the results it's getting.&lt;/p&gt;
&lt;p&gt;An obvious candidate for an interpretable classifier is a decision tree.&lt;/p&gt;
&lt;p&gt;I won't go into the specifics of decision trees, Machine Learning
Mastery has &lt;a href="http://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/"&gt;a good tutorial on the subject&lt;/a&gt;,
but I'll just go over the intuition.&lt;/p&gt;
&lt;p&gt;A decision tree is a series of if-then rules that decide what class a
data point should belong to (in the case of a classification tree), or
what value one of its properties should have (in the case of a
regression tree).&lt;/p&gt;
&lt;p&gt;If you've ever seen a flowchart, you can imagine a decision tree the
same way.&lt;/p&gt;
&lt;p&gt;A model might learn a decision tree that can be interpreted as something
like "if the petal length is less than 2, classify the flower as
&lt;em&gt;setosa&lt;/em&gt;, otherwise if the petal width is greater than 1.5 classify it
as &lt;em&gt;virginica,&lt;/em&gt; otherwise classify it as &lt;em&gt;versicolor&lt;/em&gt;".&lt;/p&gt;
&lt;p&gt;You can train decision trees with Python using scikit-learn.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;

&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stratify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;min_samples_leaf&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;max_depth&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I've set the maximum depth to 3, meaning it won't be grown beyond 3
levels, in this case purely for easier visualisation.&lt;/p&gt;
&lt;p&gt;Once you're happy with a model, how do you visualise your tree?&lt;/p&gt;
&lt;p&gt;Scikit-learn has a built-in function called &lt;em&gt;export_graphviz&lt;/em&gt; which
lets you export it to a file, in a specific format.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.tree&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;export_graphviz&lt;/span&gt;

&lt;span class="n"&gt;export_graphviz&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;iris_tree.dot&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;feature_names&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can then open the file in Notepad (or any text editor) and view its
output online by pasting its contents into the textbox at
&lt;a href="http://webgraphviz.com/"&gt;http://webgraphviz.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Our iris decision tree looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Decision tree trained on the iris dataset" src="/images/visualising-decision-trees-in-python/iris_tree.png"&gt;&lt;/p&gt;
&lt;p&gt;A 3-level decision tree trained on the Iris dataset&lt;/p&gt;
&lt;p&gt;By providing the feature names we can label each decision point so it is
obvious what's happening at each step.&lt;/p&gt;
&lt;p&gt;The "value" part of each leaf node shows how the examples that make it
to that node are split between the different classes.&lt;/p&gt;
&lt;p&gt;I wasn't far off with my example - a petal length of 2.45cm separates
the &lt;em&gt;setosa&lt;/em&gt; class nicely, then a further separation using petal width,
length and sepal length is enough to give us over 90% accuracy.&lt;/p&gt;
&lt;p&gt;Once you get to deeper trees, this visualisation becomes ungainly, but
if you want to keep the tree interpretable you probably want to limit
its depth.&lt;/p&gt;
&lt;p&gt;For further visualisation options you can follow the instructions on
&lt;a href="http://scikit-learn.org/dev/modules/tree.html#classification"&gt;the official scikit-learn page&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 28&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category><category term="python"></category></entry><entry><title>London Meetups for Data Scientists</title><link href="/blog/london-meetups-for-data-scientists" rel="alternate"></link><published>2016-11-27T17:54:00+00:00</published><updated>2016-11-27T17:54:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-27:/blog/london-meetups-for-data-scientists</id><summary type="html">&lt;p&gt;London is full of tech-related meetups. As I said before, you could
probably find an interesting meetup to attend &lt;em&gt;every night&lt;/em&gt; if you
wanted to.&lt;/p&gt;
&lt;p&gt;Meetups are great for speaking to like-minded people, hearing
interesting talks and getting an idea of what other people in the field
are up to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;London is full of tech-related meetups. As I said before, you could
probably find an interesting meetup to attend &lt;em&gt;every night&lt;/em&gt; if you
wanted to.&lt;/p&gt;
&lt;p&gt;Meetups are great for speaking to like-minded people, hearing
interesting talks and getting an idea of what other people in the field
are up to.&lt;/p&gt;
&lt;p&gt;But there are so many of them that I thought I'd list a few that I've
personally attended and would recommend for those interested in data
science.&lt;/p&gt;
&lt;p&gt;For most of these meetups, places get filled up pretty fast, so it's
worth signing up to them so you're notified when the (free) tickets
become available.&lt;/p&gt;
&lt;h1&gt;PyData London&lt;/h1&gt;
&lt;p&gt;You can guess what the focus of this meetup is: data and Python. It's a
regular, monthly meetup with a central location, interesting
Python-related talks, and free beer/pizza!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.meetup.com/PyData-London-Meetup/"&gt;https://www.meetup.com/PyData-London-Meetup/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;DataKind UK&lt;/h1&gt;
&lt;p&gt;The UK chapter of DataKind, which is aimed at using data science for
social good. The meetups are less frequent, but always very inspiring.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.meetup.com/DataKind-UK/"&gt;https://www.meetup.com/DataKind-UK/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Databeers&lt;/h1&gt;
&lt;p&gt;A meetup all about data and beer. Talks are limited to 6 minutes, which
is a format that works really well. You get a good flavour of each
project, and the strict time limit means you can hear more talks than at
a usual meetup. Did I mention there's beer?&lt;/p&gt;
&lt;p&gt;&lt;a href="http://databeersldn.tumblr.com/"&gt;http://databeersldn.tumblr.com/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;London Data Science ODSC&lt;/h1&gt;
&lt;p&gt;In recent months the folks at the London Data Science were busy
organising ODSC, a massive data science conference, and there are no new
meetups announced at the moment, but it's one to keep an eye out for.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.meetup.com/London-ODSC/"&gt;https://www.meetup.com/London-ODSC/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;HN London&lt;/h1&gt;
&lt;p&gt;This is a less data-focused meetup, more aimed at startups. However,
there are always good talks and plenty of networking opportunities.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.meetup.com/HNLondon/"&gt;https://www.meetup.com/HNLondon/&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;Data Bites&lt;/h1&gt;
&lt;p&gt;Last, but not least, there is Data Bites. Organised by City, University
of London (where I happen to be doing my MSc) Data Bites is a series of
talks from people in the industry presenting how they use data science
and machine learning to solve their problems. A good complement to one's
data science studies, as it's always good to see what "real life" data
science looks like.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.city.ac.uk/machine-learning/data-bites"&gt;http://www.city.ac.uk/machine-learning/data-bites&lt;/a&gt;&lt;/p&gt;
&lt;h1&gt;MK Geek Night&lt;/h1&gt;
&lt;p&gt;OK one more. Technically not a London meetup because it's in Milton
Keynes, and not a data science meetup because it covers all technology
topics, but MK Geek Night is a blast, so I couldn't not mention it. It's
a quarterly event and if you can make it out to Milton Keynes on a
Thursday evening, it'll be worth your time!&lt;/p&gt;
&lt;p&gt;&lt;a href="http://mkgeeknight.co.uk/"&gt;http://mkgeeknight.co.uk/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This was just a flavour of what's out there. Some meetups are more
specific than others, but there's something for everyone and I'd
encourage everyone to try attending some!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 27&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category></entry><entry><title>SQL For Data Scientists</title><link href="/blog/sql-for-data-scientists" rel="alternate"></link><published>2016-11-26T20:08:00+00:00</published><updated>2016-11-26T20:08:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-26:/blog/sql-for-data-scientists</id><summary type="html">&lt;p&gt;From what I can tell, the biggest difference between data science
curricula and data science job postings is usually knowledge of SQL. I
assume most businesses want a data scientist who knows SQL because a lot
of corporate data is stored in some sort of relational database. For
some reason …&lt;/p&gt;</summary><content type="html">&lt;p&gt;From what I can tell, the biggest difference between data science
curricula and data science job postings is usually knowledge of SQL. I
assume most businesses want a data scientist who knows SQL because a lot
of corporate data is stored in some sort of relational database. For
some reason though, data science courses don't tend to teach it
explicitly.&lt;/p&gt;
&lt;p&gt;I wanted to collect some of the concepts which I think are useful for
aspiring data scientists to learn about databases and SQL. I'll also
link to appropriate parts of the &lt;a href="http://www.w3schools.com/sql/"&gt;w3schools SQL tutorials&lt;/a&gt; along the way.&lt;/p&gt;
&lt;h1&gt;Query Syntax&lt;/h1&gt;
&lt;p&gt;Obviously the first step is to understand how to write a SQL query.&lt;/p&gt;
&lt;p&gt;SQL is a &lt;a href="https://en.wikipedia.org/wiki/Declarative_programming"&gt;declarative language&lt;/a&gt;. All
that means is that when you write a SQL query, you're expressing &lt;strong&gt;what the result should look like, rather than how to achieve it&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's look at a basic SQL query and see how that's the case.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;height&lt;/span&gt; &lt;span class="k"&gt;FROM&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt; &lt;span class="k"&gt;WHERE&lt;/span&gt; &lt;span class="n"&gt;job&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="ss"&gt;&amp;quot;data scientist&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;SQL is not case sensitive, but I capitalised the keywords (which is a
typical thing to do anyway).&lt;/p&gt;
&lt;p&gt;If you know that you have a table of data, called &lt;strong&gt;people&lt;/strong&gt;, you can
pretty much work out what this query will do. The declarative syntax
means you can specify the data source (the people table), what you want
to extract (name, age and height) and any filters you want to apply
(only get the data for people who are data scientists).&lt;/p&gt;
&lt;p&gt;There is a lot going on under the hood in terms of the computer deciding
how best to store and index the data, but when you write queries you
don't want to have to care about that, you just want your results.&lt;/p&gt;
&lt;p&gt;The main keywords you need to know are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_select.asp"&gt;SELECT...FROM&lt;/a&gt; (to select rows from a specific table)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_where.asp"&gt;WHERE&lt;/a&gt; (to filter
    rows - &lt;em&gt;optional&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_insert.asp"&gt;INSERT&lt;/a&gt; (to insert new
    rows)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_update.asp"&gt;UPDATE&lt;/a&gt; (to update
    existing rows)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_delete.asp"&gt;DELETE&lt;/a&gt; (to remove
    rows)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_groupby.asp"&gt;GROUP BY&lt;/a&gt; (to group
    data into... well, groups)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_create_table.asp"&gt;CREATE TABLE&lt;/a&gt;
    (to create new tables)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_alter.asp"&gt;ALTER TABLE&lt;/a&gt; (to make
    changes to tables like adding new columns)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_join.asp"&gt;JOIN&lt;/a&gt; (to join multiple
    tables together)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;The JOIN keyword&lt;/h1&gt;
&lt;p&gt;I left the JOIN keyword until last in that list because it warrants its
own section.&lt;/p&gt;
&lt;p&gt;Merging multiple data sources is a staple data science operation, and
that's no different when working with SQL. If you've used the merge
function in pandas you'll have seen this already, but let's see how they
compare.&lt;/p&gt;
&lt;p&gt;Let's take the example of joining two data sources with pandas. One of
them is a csv of people, with names, ages, heights and jobs. The other
is a csv of phone numbers linked to people's names. The name column is
common between both data sources.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;people.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;phone_numbers&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phone_numbers.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;merged&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;merge&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phone_numbers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;on&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;how&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;inner&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That &lt;em&gt;how&lt;/em&gt; keyword corresponds to the type of join in SQL. The same
operation in SQL looks like this (assuming we have a people and
phone_numbers table in a database, rather than csv files):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;SELECT&lt;/span&gt;
  &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;age&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;height&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phone_numbers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;number&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;
  &lt;span class="n"&gt;people&lt;/span&gt;
  &lt;span class="k"&gt;INNER&lt;/span&gt; &lt;span class="k"&gt;JOIN&lt;/span&gt; &lt;span class="n"&gt;phone_numbers&lt;/span&gt; &lt;span class="k"&gt;ON&lt;/span&gt; &lt;span class="n"&gt;people&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;phone_numbers&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;I've specifically stated in the SELECT clause where the columns come
from, because both tables have a name column and SQL would have gotten
confused otherwise.&lt;/p&gt;
&lt;p&gt;The types of SQL join correspond to the valid values of the "how"
keyword in the pandas merge function. They are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_join_inner.asp"&gt;Inner Join&lt;/a&gt; - only
    rows where both tables have a value are returned&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_join_left.asp"&gt;Left Outer Join&lt;/a&gt; -
    only rows where the table on the left of the statement has a value
    are returned&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_join_right.asp"&gt;Right Outer Join&lt;/a&gt; - only rows
    where the table on the right of the statement has a value are
    returned&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.w3schools.com/sql/sql_join_full.asp"&gt;Full Outer Join&lt;/a&gt; -
    all rows are returned from both tables&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The outer joins let you keep rows from either table if there are no
corresponding rows in the other table.&lt;/p&gt;
&lt;p&gt;So a left join in the previous statements would have shown all people,
regardless of whether they had a phone number in the second data source.
The rows of people who don't have a phone number would have shown a NULL
value for the phone number. Using an inner join wouldn't have returned
them at all.&lt;/p&gt;
&lt;p&gt;It can be helpful to view this visually, and the w3schools pages do that
already, but &lt;a href="https://blog.codinghorror.com/a-visual-explanation-of-sql-joins/"&gt;here's another good example&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;SQL Tools for Data Science&lt;/h1&gt;
&lt;p&gt;If you know the basic query syntax and the various join types, you're
probably equipped enough to start pulling data out of any SQL database.
Programmers working with SQL often use specific tools to access
databases, such as Microsoft's SQL Server Management Studio.&lt;/p&gt;
&lt;p&gt;However, as a data scientist you'll want to do this straight from your
code instead. You have a few options for this.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You can &lt;a href="http://www.datacarpentry.org/python-ecology-lesson/08-working-with-sql"&gt;connect to sqlite&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;You can use SQL queries in pandas &lt;a href="http://blog.yhat.com/posts/pandasql-sql-for-pandas-dataframes.html"&gt;using pandasql&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;pandas also allows &lt;a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql.html"&gt;connecting to SQL sources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, if you're familiar with pandas but not with SQL, the pandas
documentation has a section with &lt;a href="http://pandas.pydata.org/pandas-docs/stable/comparison_with_sql.html"&gt;pandas commands and the associated SQL queries&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I'd argue that's all you need to get up and running.&lt;/p&gt;
&lt;p&gt;W3schools is a great interactive resource to test your sql queries, but
there are plenty of other good learning resources like
&lt;a href="https://www.codecademy.com/learn/learn-sql"&gt;Codecademy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There is more to know of course about relational databases. I haven't
covered the concepts of primary keys, foreign keys, or indexes because
these are more important for database design rather than data retrieval.
Designing a relational database has its own set of skills and required
knowledge, but if your only interaction is retrieving data, you
shouldn't have to worry about it.&lt;/p&gt;
&lt;p&gt;I may write a post about database design in the future, but I'd strongly
argue that it's an optional skill for most data scientists.&lt;/p&gt;
&lt;p&gt;Footnote: This is the 26&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="programming for data scientists"></category><category term="featured"></category></entry><entry><title>More on the Snooper's Charter</title><link href="/blog/more-on-the-snoopers-charter" rel="alternate"></link><published>2016-11-25T19:46:00+00:00</published><updated>2016-11-25T19:46:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-25:/blog/more-on-the-snoopers-charter</id><summary type="html">&lt;p&gt;One problem with this "30 posts in 30 days" format is that it doesn't
lend itself to deep discussion about current events. Trying to react to
a piece of news with a well-reasoned, detailed view &lt;em&gt;in real time&lt;/em&gt; is
hard and I don't feel I'm doing the topics justice.&lt;/p&gt;
&lt;p&gt;I'm …&lt;/p&gt;</summary><content type="html">&lt;p&gt;One problem with this "30 posts in 30 days" format is that it doesn't
lend itself to deep discussion about current events. Trying to react to
a piece of news with a well-reasoned, detailed view &lt;em&gt;in real time&lt;/em&gt; is
hard and I don't feel I'm doing the topics justice.&lt;/p&gt;
&lt;p&gt;I'm still not sure if I can give a fully fledged view on the matter, but
I'd like to revisit the Snooper's Charter.&lt;/p&gt;
&lt;p&gt;&lt;a href="/blog/all-your-web-data-are-belong-to-us/"&gt;About a week ago&lt;/a&gt;
the news broke that the UK government has passed a law (subject to
"Royal Assent") that will increase their ability to access your personal
data by accessing your browsing history &lt;strong&gt;by requiring ISPs to store it
by law&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I hinted at my disbelief and disagreement with this news, but I want to
elaborate a bit, as well as revisit the angle of what we can do about
it.&lt;/p&gt;
&lt;h1&gt;Is it so bad?&lt;/h1&gt;
&lt;p&gt;Philosophers and security experts have no doubt thoroughly chewed over
this topic in the past, but let's briefly examine some arguments on both
sides, pro- or anti-surveillance.&lt;/p&gt;
&lt;h2&gt;Pro-surveillance: Terrorists Shouldn't Have Privacy&lt;/h2&gt;
&lt;p&gt;I guess one way to maximise your chances of identifying illegal activity
is by just monitoring everyone's activity by default. You'll get tons of
false positives, but no one will fall through the net. This obviously
requires an extraordinary apparatus, and we'll get back to that.&lt;/p&gt;
&lt;p&gt;It is therefore possible to argue that we should have the ability to
listen in on people, because we cannot afford &lt;em&gt;not&lt;/em&gt; to miss an instance
of wrongdoing.&lt;/p&gt;
&lt;h2&gt;Anti-surveillance: Listening to Innocent People&lt;/h2&gt;
&lt;p&gt;The counter-argument lends itself: that means we have to monitor
innocent people. What happened to "innocent until proven guilty"? While
it is true that obstructing people's privacy without valid cause sounds
like overkill, how would you know who is innocent and who isn't without
monitoring them in the first place? The answer to this of course could
be that you only start monitoring people who you otherwise have reason
to believe are suspicious, i.e. are already suspects in some way. This
is what's being argued from the side of the people passing this law - in
theory only a court order will allow the government access to anyone's
data.&lt;/p&gt;
&lt;p&gt;And anyway, there's always the following trope that can be wheeled out.&lt;/p&gt;
&lt;h2&gt;Pro-surveillance: "You Shouldn't Be Worried If You Have Nothing To Hide"&lt;/h2&gt;
&lt;p&gt;This argument makes its appearance every time the privacy debate comes
up.&lt;/p&gt;
&lt;p&gt;There are a few simple answers to this. First of all, yes, as a citizen
of the 21&lt;sup&gt;st&lt;/sup&gt; century you have to accept that your data will be
captured, stored and used by third parties, be that your supermarket,
bank or digital advertising companies.&lt;/p&gt;
&lt;p&gt;I don't mind my shopping habits being used by my supermarket to give me
a more tailored and relevant online shopping experience.&lt;/p&gt;
&lt;p&gt;What I do mind is the idea that this might get in the hands of "bad
actors".&lt;/p&gt;
&lt;p&gt;Today I heard a quote apparently attributed to Cardinal Richelieu, which
I thought was apt:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you give me six lines written by the hand of the most honest of
men, I will find something in them which will hang him.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Your otherwise innocuous data can easily be used against you, either by
a future government with malicious intent, or a hacker that gains access
to the data and uses it for &lt;a href="https://en.wikipedia.org/wiki/Social_engineering_(security)"&gt;social engineering&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Anti-surveillance: Right to Privacy&lt;/h2&gt;
&lt;p&gt;Ultimately the argument against this sort of law is that privacy is a
fundamental right. It's &lt;a href="https://en.wikipedia.org/wiki/Right_to_privacy#United_States"&gt;in the American Constitution (sort of)&lt;/a&gt;, it's
in the &lt;a href="https://www.liberty-human-rights.org.uk/human-rights/privacy"&gt;Human Rights Act&lt;/a&gt;. What
you do behind closed doors is your own business, and no one else's.&lt;/p&gt;
&lt;h1&gt;What about the IP Bill specifically?&lt;/h1&gt;
&lt;p&gt;After that bit of philosophical ping-pong, let's consider this specific
bill. What are the real points of contention?&lt;/p&gt;
&lt;h2&gt;Technological Concerns&lt;/h2&gt;
&lt;p&gt;I alluded to returning to this earlier. The bill requires ISPs to store
a year's worth of browsing history for each of their users. Even if it's
just metadata, that's a lot of data to store and in real time. The
storage and latency issues associated with this will pose a &lt;em&gt;huge&lt;/em&gt;
challenge to ISPs who may or may not be equipped to cope with it.&lt;/p&gt;
&lt;p&gt;On top of that, there are huge security concerns. Companies like
&lt;a href="http://www.computerweekly.com/news/450400451/TalkTalk-hit-by-record-400000-fine-over-data-breach"&gt;TalkTalk&lt;/a&gt;
and
&lt;a href="http://www.ispreview.co.uk/index.php/2016/11/serious-data-breach-puts-6-million-three-uk-mobile-customers-risk.html"&gt;Three&lt;/a&gt;
have already had large scale data breaches in the last year. ISPs will
be much more of a target now that hackers will know the kind of data
they could have access to. Now all of a sudden we're not even talking
about malicious governments accessing your data, but concerns about
simple security.&lt;/p&gt;
&lt;h2&gt;Questions about Access&lt;/h2&gt;
&lt;p&gt;I stumbled upon an article on the subject, and I'd just like to quote
from it.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A list of who will have the power to access your internet connection
records is set out in Schedule 4 of the Act. It’s longer than you
might imagine:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Metropolitan police force&lt;/li&gt;
&lt;li&gt;City of London police force&lt;/li&gt;
&lt;li&gt;Police forces maintained under section 2 of the Police Act 1996&lt;/li&gt;
&lt;li&gt;Police Service of Scotland&lt;/li&gt;
&lt;li&gt;Police Service of Northern Ireland&lt;/li&gt;
&lt;li&gt;British Transport Police&lt;/li&gt;
&lt;li&gt;Ministry of Defence Police&lt;/li&gt;
&lt;li&gt;Royal Navy Police&lt;/li&gt;
&lt;li&gt;Royal Military Police&lt;/li&gt;
&lt;li&gt;Royal Air Force Police&lt;/li&gt;
&lt;li&gt;Security Service&lt;/li&gt;
&lt;li&gt;Secret Intelligence Service&lt;/li&gt;
&lt;li&gt;GCHQ&lt;/li&gt;
&lt;li&gt;Ministry of Defence&lt;/li&gt;
&lt;li&gt;Department of Health&lt;/li&gt;
&lt;li&gt;Home Office&lt;/li&gt;
&lt;li&gt;Ministry of Justice&lt;/li&gt;
&lt;li&gt;National Crime Agency&lt;/li&gt;
&lt;li&gt;HM Revenue &amp;amp; Customs&lt;/li&gt;
&lt;li&gt;Department for Transport&lt;/li&gt;
&lt;li&gt;Department for Work and Pensions&lt;/li&gt;
&lt;li&gt;NHS trusts and foundation trusts in England that provide ambulance
    services&lt;/li&gt;
&lt;li&gt;Common Services Agency for the Scottish Health Service&lt;/li&gt;
&lt;li&gt;Competition and Markets Authority&lt;/li&gt;
&lt;li&gt;Criminal Cases Review Commission&lt;/li&gt;
&lt;li&gt;Department for Communities in Northern Ireland&lt;/li&gt;
&lt;li&gt;Department for the Economy in Northern Ireland&lt;/li&gt;
&lt;li&gt;Department of Justice in Northern Ireland&lt;/li&gt;
&lt;li&gt;Financial Conduct Authority&lt;/li&gt;
&lt;li&gt;Fire and rescue authorities under the Fire and Rescue Services Act
    2004&lt;/li&gt;
&lt;li&gt;Food Standards Agency&lt;/li&gt;
&lt;li&gt;Food Standards Scotland&lt;/li&gt;
&lt;li&gt;Gambling Commission&lt;/li&gt;
&lt;li&gt;Gangmasters and Labour Abuse Authority&lt;/li&gt;
&lt;li&gt;Health and Safety Executive&lt;/li&gt;
&lt;li&gt;Independent Police Complaints Commissioner&lt;/li&gt;
&lt;li&gt;Information Commissioner&lt;/li&gt;
&lt;li&gt;NHS Business Services Authority&lt;/li&gt;
&lt;li&gt;Northern Ireland Ambulance Service Health and Social Care Trust&lt;/li&gt;
&lt;li&gt;Northern Ireland Fire and Rescue Service Board&lt;/li&gt;
&lt;li&gt;Northern Ireland Health and Social Care Regional Business Services
    Organisation&lt;/li&gt;
&lt;li&gt;Office of Communications&lt;/li&gt;
&lt;li&gt;Office of the Police Ombudsman for Northern Ireland&lt;/li&gt;
&lt;li&gt;Police Investigations and Review Commissioner&lt;/li&gt;
&lt;li&gt;Scottish Ambulance Service Board&lt;/li&gt;
&lt;li&gt;Scottish Criminal Cases Review Commission&lt;/li&gt;
&lt;li&gt;Serious Fraud Office&lt;/li&gt;
&lt;li&gt;Welsh Ambulance Services National Health Service Trust&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;small&gt;&lt;a href="http://yiu.co.uk/blog/who-can-view-my-internet-history/"&gt;Who can view my internet history? - Chris Yiu&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That list is &lt;strong&gt;bonkers&lt;/strong&gt;. Why would the Welsh Ambulance Services NHS
Trust need my browsing history? Or Food Standards &lt;strong&gt;Scotland&lt;/strong&gt;?&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;I do not support this bill.&lt;/p&gt;
&lt;p&gt;The reason isn't even to do with the general privacy debate.&lt;/p&gt;
&lt;p&gt;Even if, for whatever reason, you agree with governments being able to
access this data in extreme cases (suspected terrorism, whatever) and
&lt;em&gt;even if&lt;/em&gt; we put aside concerns about governments misusing this power,
this bill also relies on ISPs keeping data safe. That is a huge risk in
itself.&lt;/p&gt;
&lt;p&gt;Based on the number of data breaches we've seen over the months and
years, I simply do not trust that my data will be kept safe.&lt;/p&gt;
&lt;p&gt;I'd argue that you should be concerned about this bill in its current
form regardless of how you feel about the politics of the bigger issue
of privacy.&lt;/p&gt;
&lt;p&gt;Although I am also quite concerned about how long that list of
departments that can access my data is.&lt;/p&gt;
&lt;p&gt;It's &lt;strong&gt;long&lt;/strong&gt;.&lt;/p&gt;
&lt;h1&gt;What can we do?&lt;/h1&gt;
&lt;h2&gt;VPNs&lt;/h2&gt;
&lt;p&gt;I touched on this previously. VPNs are not covered by the law, so
they're a good solution. Bear in mind that it matters where your VPN
provider's servers are, and what they do with your data.&lt;/p&gt;
&lt;p&gt;You could &lt;a href="https://torrentfreak.com/vpn-anonymous-review-160220/"&gt;start here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Sign a Petition&lt;/h2&gt;
&lt;p&gt;There is also &lt;a href="https://petition.parliament.uk/petitions/173199"&gt;a petition you can sign&lt;/a&gt;. It's already
past the point where the government must respond to it. It is, at time
of writing this, 68% of the way to being considered for debate in
Parliament.&lt;/p&gt;
&lt;h2&gt;Write to your MP&lt;/h2&gt;
&lt;p&gt;Some have also suggested writing to your local MP. I checked and it
turns out my local MP has one of the lowest response rates out of all of
them, but I might try anyway.&lt;/p&gt;
&lt;p&gt;You can find out who your local MP is (and how they vote on all issues!)
here: &lt;a href="https://www.theyworkforyou.com/"&gt;They Work For You&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Read About It&lt;/h2&gt;
&lt;p&gt;If you want some more detailed discussion on the topic, you could start
with &lt;a href="https://news.ycombinator.com/item?id=13034747"&gt;this Hacker News thread&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 25&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="technology commentary"></category><category term="featured"></category></entry><entry><title>Fun Uses of Deep Learning</title><link href="/blog/fun-uses-of-deep-learning" rel="alternate"></link><published>2016-11-24T20:25:00+00:00</published><updated>2016-11-24T20:25:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-24:/blog/fun-uses-of-deep-learning</id><summary type="html">&lt;p&gt;I was struggling to find inspiration for a topic to write about today,
but then I remembered this guy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When I saw the Google's AlphaGo, I realized something really serious
is happening here [...] That was the trigger for me to start
developing the &lt;strong&gt;cucumber sorter with deep learning technology&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;Makoto …&lt;/small&gt;&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;I was struggling to find inspiration for a topic to write about today,
but then I remembered this guy.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;When I saw the Google's AlphaGo, I realized something really serious
is happening here [...] That was the trigger for me to start
developing the &lt;strong&gt;cucumber sorter with deep learning technology&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;Makoto Koike, &lt;a href="https://cloud.google.com/blog/big-data/2016/08/how-a-japanese-cucumber-farmer-is-using-deep-learning-and-tensorflow"&gt;How a Japanese cucumber farmer is using deep learning and TensorFlow&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Makoto Koike's family own a cucumber farm and he was so impressed with
the idea of deep learning, after seeing &lt;a href="https://deepmind.com/research/alphago/"&gt;AlphaGo's famous victory&lt;/a&gt; over Lee Sedol, that he
decided to create an algorithm that &lt;strong&gt;sorts cucumbers&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;It's an incredibly intricate classification and computer vision problem,
and just a cool use of deep learning. No doubt you've heard the term
'deep learning' and the 'hype' that surrounds it.&lt;/p&gt;
&lt;p&gt;I'm sure we're just at the surface of the amazing things that can be
accomplished with deep learning, and I just wanted to share some of the
(I think) really cool uses of deep learning out there.&lt;/p&gt;
&lt;h1&gt;Image Processing&lt;/h1&gt;
&lt;p&gt;Computer vision is a hard problem in itself, and AI research has been
trying to solve various image recognition problems for decades. However,
there are some offshoots of image-related deep learning projects that I
think deserve a mention.&lt;/p&gt;
&lt;h2&gt;Sketch Simplification&lt;/h2&gt;
&lt;p&gt;Researchers have come up with an algorithm that can &lt;a href="http://hi.cs.waseda.ac.jp/~esimo/en/research/sketch/"&gt;take a sketch drawing and automatically create a "solid" line trace&lt;/a&gt; of it.
Their results are incredible.&lt;/p&gt;
&lt;h2&gt;Image Colorisation&lt;/h2&gt;
&lt;p&gt;Taking black and white images and &lt;a href="http://hi.cs.waseda.ac.jp/~iizuka/projects/colorization/en/"&gt;drawing them in colour&lt;/a&gt;, in
cases where a colour version doesn't even exist.&lt;/p&gt;
&lt;p&gt;Or &lt;a href="https://github.com/cameronfabbri/Colorful-Image-Colorization"&gt;colouring in old black-and-white Pokemon games&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Artistic Style Transfer&lt;/h2&gt;
&lt;p&gt;Taking an image and &lt;a href="https://blogs.nvidia.com/blog/2016/05/25/deep-learning-paints-videos/"&gt;redrawing it in the style of a famous painting&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;Music&lt;/h1&gt;
&lt;p&gt;Deep learning algorithms can also generate realistic-sounding music!&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://deepjazz.io/"&gt;Generating Jazz&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="http://bachbot.com"&gt;Generating Original Bach Pieces&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://swarbrickjones.wordpress.com/2016/11/07/deeprhyme-d-prime-generating-dope-rhymes-with-deep-learning/"&gt;Writing New Rap Lyrics&lt;/a&gt;&lt;/h2&gt;
&lt;h1&gt;Text&lt;/h1&gt;
&lt;p&gt;Some of my favourite AI-powered projects that I've seen relate to text
generation.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://medium.com/deep-writing/harry-potter-written-by-artificial-intelligence-8a9431803da6#.ywwzc78we"&gt;Writing New Chapters of Harry Potter&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://twitter.com/_pandy/status/689209034143084547"&gt;Writing New Episodes of 'Friends'&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://twitter.com/deepdrumpf"&gt;Tweeting Like Trump&lt;/a&gt;&lt;/h2&gt;
&lt;h2&gt;&lt;a href="https://medium.com/&amp;64;samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0#.46e1blsx1"&gt;Generating New TED Talks&lt;/a&gt;&lt;/h2&gt;
&lt;h1&gt;Miscellaneous&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://aiexperiments.withgoogle.com/"&gt;Interactive AI Experiments&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Footnote: This was the 24&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>5 Programming Concepts for Data Scientists</title><link href="/blog/5-programming-concepts-for-data-scientists" rel="alternate"></link><published>2016-11-23T20:17:00+00:00</published><updated>2016-11-23T20:17:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-23:/blog/5-programming-concepts-for-data-scientists</id><summary type="html">&lt;p&gt;In a previous post I talked about maths you should know for data
science, partly with programmers in mind who want to move towards data
science.&lt;/p&gt;
&lt;p&gt;Today I want to do the opposite.&lt;/p&gt;
&lt;p&gt;I want to introduce some software development concepts that I think data
scientists would benefit from, and …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In a previous post I talked about maths you should know for data
science, partly with programmers in mind who want to move towards data
science.&lt;/p&gt;
&lt;p&gt;Today I want to do the opposite.&lt;/p&gt;
&lt;p&gt;I want to introduce some software development concepts that I think data
scientists would benefit from, and which are maybe less talked about
when discussing "what it takes" to be a data scientist.&lt;/p&gt;
&lt;h2&gt;Source Control&lt;/h2&gt;
&lt;p&gt;To be fair, I've seen more and more data science resources cover this
topic, but it's still worth a mention.&lt;/p&gt;
&lt;p&gt;If you start programming without source control and then get introduced
to it I guarantee you can't go back. It feels positively prehistoric
that once upon a time my version of source control was as sophisticated
as "code_20160101.zip", "code_20160104.zip" and so on (that's just an
example, I want to make it clear that I wasn't still doing that in
January 2016...).&lt;/p&gt;
&lt;p&gt;The ability to go back to previous versions with near-zero effort, as
well as keep a log of what you've changed between versions is essential
for something as fundamentally experimental as the data science process.&lt;/p&gt;
&lt;p&gt;Probably the most popular source control method these days is Git,
specifically &lt;a href="https://github.com/"&gt;GitHub&lt;/a&gt; or
&lt;a href="https://bitbucket.org/"&gt;BitBucket&lt;/a&gt;. There are others out there, such as
subversion (using something like
&lt;a href="https://tortoisesvn.net/"&gt;TortoiseSVN&lt;/a&gt;) or Microsoft's proprietary (and
expensive) Team Foundation Server.&lt;/p&gt;
&lt;p&gt;You're probably better off using Git. Having a GitHub portfolio is
pretty handy.&lt;/p&gt;
&lt;p&gt;If you don't want to learn the console commands (&lt;a href="https://xkcd.com/1597/"&gt;obligatory related xkcd&lt;/a&gt;), don't feel obliged to.&lt;/p&gt;
&lt;p&gt;I use &lt;a href="https://www.sourcetreeapp.com/"&gt;SourceTree&lt;/a&gt; and it's pretty great.&lt;/p&gt;
&lt;h2&gt;Automation&lt;/h2&gt;
&lt;p&gt;I've &lt;a href="/blog/turning-jupyter-notebooks-into-reusable-scripts/"&gt;touched on this topic before&lt;/a&gt;;
I think data scientists should adopt a lazy, programmer's mindset.&lt;/p&gt;
&lt;p&gt;Automate everything you can.&lt;/p&gt;
&lt;p&gt;Let the computer do the boring things while you do the hard thinking.&lt;/p&gt;
&lt;p&gt;Have a script that automatically cleans your data, or even &lt;a href="https://github.com/rhiever/tpot"&gt;does some of your machine learning for you&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;IDEs&lt;/h2&gt;
&lt;p&gt;Using something like IPython to test out programming snippets in an
interactive way is a great idea. However, when you need more
reproducible code or just a bit more functionality than typing single
commands, use an IDE (Integrated Development Environment).&lt;/p&gt;
&lt;p&gt;The staple Python IDE is the &lt;a href="http://jupyter.org"&gt;Jupyter notebook&lt;/a&gt;
environment, great for presentation and reproducibility, and therefore
terrific for data science.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://pythonhosted.org/spyder/"&gt;Spyder&lt;/a&gt;,
&lt;a href="https://www.yhat.com/products/rodeo"&gt;Rodeo&lt;/a&gt; and
&lt;a href="https://www.jetbrains.com/pycharm/"&gt;PyCharm&lt;/a&gt; are all good alternatives.&lt;/p&gt;
&lt;p&gt;At the very least use a text editor with syntax highlighting.
&lt;a href="https://notepad-plus-plus.org/"&gt;Notepad++&lt;/a&gt;, &lt;a href="https://www.sublimetext.com/"&gt;Sublime Text&lt;/a&gt;, &lt;a href="https://atom.io/"&gt;Atom&lt;/a&gt; or similar.&lt;/p&gt;
&lt;h2&gt;Relational Databases&lt;/h2&gt;
&lt;p&gt;One skill that is sometimes overlooked on data science courses is the
use of SQL and relational databases. I might even do a separate post
dedicated to this topic because I think it's a useful skill to have.&lt;/p&gt;
&lt;p&gt;Database design is an important aspect of creating a functioning
application, so if you ever want to make your machine learning algorithm
into a software product it's worth knowing how best to design a backend
database.&lt;/p&gt;
&lt;p&gt;Codecademy do &lt;a href="https://www.codecademy.com/learn/learn-sql"&gt;a SQL course&lt;/a&gt;
(in fact it looks like they do multiple). I haven't done them myself,
but other Codecademy courses I've done were all excellent. My first SQL
resource was &lt;a href="http://www.w3schools.com/sql/"&gt;w3schools&lt;/a&gt;, which is also
worth a look.&lt;/p&gt;
&lt;h2&gt;D. R. Y. - Don't Repeat Yourself&lt;/h2&gt;
&lt;p&gt;I'm not usually a fan of teaching people programming best practices when
they're starting out, and many people learning data science don't have a
programming background, but if there's one concept that I'd recommend
internalising it's this.&lt;/p&gt;
&lt;p&gt;Don't repeat yourself.&lt;/p&gt;
&lt;p&gt;If you have a few lines of code that do something that you'll need to do
over and over again, or you find yourself copying and pasting the same
code snippet multiple times, make it into a function.&lt;/p&gt;
&lt;p&gt;If you have a collection of related functions you call over and over
again, make it into a class. At the very least move them into a separate
file.&lt;/p&gt;
&lt;p&gt;These sound like little things but they add up and make your code easier
to read and more maintainable.&lt;/p&gt;
&lt;p&gt;I hope you agree that data scientists can learn a lot of useful concepts
from software development, even if they never end up having to build
production-ready systems!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 23&lt;sup&gt;rd&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="programming for data scientists"></category><category term="featured"></category></entry><entry><title>Maths Concepts You Should Know for Data Science</title><link href="/blog/maths-concepts-you-should-know-for-data-science" rel="alternate"></link><published>2016-11-22T18:31:00+00:00</published><updated>2016-11-22T18:31:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-22:/blog/maths-concepts-you-should-know-for-data-science</id><summary type="html">&lt;p&gt;If you're interested in data science, I have some potentially bad news -
you need to know some maths.&lt;/p&gt;
&lt;p&gt;The good news is that despite what some resources might suggest, you
don't need &lt;em&gt;that much.&lt;/em&gt; You still need more than zero, but chances are
you'll have seen a lot of it …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you're interested in data science, I have some potentially bad news -
you need to know some maths.&lt;/p&gt;
&lt;p&gt;The good news is that despite what some resources might suggest, you
don't need &lt;em&gt;that much.&lt;/em&gt; You still need more than zero, but chances are
you'll have seen a lot of it in high/secondary school.&lt;/p&gt;
&lt;p&gt;Here's an overview of what topics you should brush up on.&lt;/p&gt;
&lt;h1&gt;Linear Algebra&lt;/h1&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;I've always thought the words "linear algebra" sound more intimidating
than they need to.&lt;/p&gt;
&lt;p&gt;Basically, you need to know what a vector and a matrix are, the notation
to represent them, and how to do basic operations with them (addition,
multiplication, transposing, dot products, that sort of thing).&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;Datasets are usually represented as matrices where each rows is a data
point and each column is a feature. When you talk about single data
points or parameters to machine learning algorithms, they're typically
vectors. You can avoid a lot of confusion by having your vector/matrix
knowledge up to date.&lt;/p&gt;
&lt;h1&gt;Statistics&lt;/h1&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;Arguably if you don't brush up on anything else it should be this.&lt;/p&gt;
&lt;p&gt;You should know your means from your medians, your Gaussian distribution
from your multinomial, and you should know about the &lt;a href="http://www.jeannicholashould.com/the-theorem-every-data-scientist-should-know.html"&gt;Central Limit Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Understanding sampling and hypothesis testing is also important.&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;"Data scientists are statisticians because being a statistician is
awesome and anyone who does cool things with data is a statistician."&lt;/p&gt;
&lt;p&gt;&lt;small&gt;Robert Rodriguez, President, American Statistical Association&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;OK so the head of the American Statistical Association might not be the
most reliable source on how useful statistics is.&lt;/p&gt;
&lt;p&gt;Overlooking that, the point is that data science is in many ways
computational statistics. You can't get away from the fact that
understanding fundamental statistical concepts is essential to make any
sense of data.&lt;/p&gt;
&lt;h1&gt;Probability&lt;/h1&gt;
&lt;h2&gt;What?&lt;/h2&gt;
&lt;p&gt;Being comfortable with representations of probability and seeing
probability distributions is enough to cover your bases. You shouldn't
be thrown off by phrases like "conditional probability" or "random
variable".&lt;/p&gt;
&lt;p&gt;Oh, also &lt;a href="https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/"&gt;learn and understand Bayes' Theorem&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Why?&lt;/h2&gt;
&lt;p&gt;Understanding probabilities is a useful life skill anyway. Humans are
typically not wired to intuitively understand probabilities (I recommend
&lt;a href="https://en.wikipedia.org/wiki/The_Drunkard's_Walk"&gt;The Drunkard's Walk&lt;/a&gt;
on this subject). Being able to do it is a good skill for a data
scientist. Also, many machine learning algorithms deal with
probabilities and probability distributions one way or another.&lt;/p&gt;
&lt;h1&gt;Calculus (optional)&lt;/h1&gt;
&lt;p&gt;The word "optional" might be controversial among some data scientists.
I'd argue that you can go a long way in data science without ever
calculating a partial derivative.&lt;/p&gt;
&lt;p&gt;Having said that, knowing &lt;strong&gt;what&lt;/strong&gt; a partial derivative is and what it's
used for can't hurt. Some machine learning algorithms (neural networks,
linear regression) require that understanding if you want to go into the
details. Don't go anywhere near "gradient descent" until you understand
why you'd want to set a partial derivative to zero.&lt;/p&gt;
&lt;p&gt;So high school level calculus (derivatives, integrals, the 'chain rule')
are useful concepts to know, but don't start there. The other things
I've mentioned above are more important.&lt;/p&gt;
&lt;h2&gt;Resources&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://www.khanacademy.org/"&gt;Khan Academy&lt;/a&gt; has been my favourite
resource for brushing up on maths subjects. Something about Sal Khan's
teaching style really resonates with me.&lt;/p&gt;
&lt;p&gt;For more advanced topics, the YouTube channel
&lt;a href="https://www.youtube.com/user/mathematicalmonk"&gt;mathematicalmonk&lt;/a&gt; is
also excellent.&lt;/p&gt;
&lt;p&gt;For linear algebra, getting familiar with the numpy library is also a
good idea if you already know some Python, as numpy encourages you to
deal with vectorised operations.&lt;/p&gt;
&lt;p&gt;If you already have programming experience, check out &lt;a href="https://projecteuler.net/"&gt;Project Euler&lt;/a&gt; - it's a series of mathematical
challenges you solve by writing code. It might not be immediately
related to data science, but it's a great way to get a bit more
motivated about maths.&lt;/p&gt;
&lt;p&gt;Hopefully I've convinced you that you don't need a PhD in maths to
embark on the road to data science!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 22&lt;sup&gt;nd&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category><category term="featured"></category></entry><entry><title>6 Data Visualisations You Might Also Like</title><link href="/blog/6-data-visualisations-you-might-also-like" rel="alternate"></link><published>2016-11-21T18:59:00+00:00</published><updated>2016-11-21T18:59:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-21:/blog/6-data-visualisations-you-might-also-like</id><summary type="html">&lt;p&gt;I love a good data visualisation.&lt;/p&gt;
&lt;p&gt;My talents most certainly don't lie in the visual arts, but I can
appreciate something nice when I see it.&lt;/p&gt;
&lt;p&gt;So today I thought I'd share some data visualisations that I've seen
recently and particularly liked. They all show some creative and/or
powerful …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I love a good data visualisation.&lt;/p&gt;
&lt;p&gt;My talents most certainly don't lie in the visual arts, but I can
appreciate something nice when I see it.&lt;/p&gt;
&lt;p&gt;So today I thought I'd share some data visualisations that I've seen
recently and particularly liked. They all show some creative and/or
powerful ways to display data.&lt;/p&gt;
&lt;p&gt;Most of them are interactive or too big to embed so they're links, but
I'll write a bit about them to convince you to click them and check them
out.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://xkcd.com/1732/"&gt;A Timeline of Earth's Temperature&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This is a long visualisation showing how the Earth's temperature has
changed over time, starting in 20,000 BC. Randall Munroe's typical
stickman drawings add some humour to an otherwise serious message. You
know what to expect to see after around 1850...&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.bloomberg.com/graphics/2015-whats-warming-the-world/"&gt;What's Really Warming the World?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I promise that's the last climate change one, but it tells a very good
story. The write-up at the end is informative too. Honesty is important
when you're presenting data!&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://joeycloud.net/v/pianogram/"&gt;Pianogram&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Something completely different. A piano bar chart showing the frequency
of each key appearing in various classical piano pieces. Chopin's "Black
Keys Etude" isn't called that for nothing.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.nytimes.com/interactive/2016/08/01/us/elections/nine-percent-of-america-selected-trump-and-clinton.html"&gt;Only 9% of America Chose Trump and Clinton as Nominees&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;If you've seen enough data visualisations you'll know that the folks at
the New York Times excel at creating them. I'm quite partial to a
datavis that shows percentages with blocks so you get a &lt;em&gt;feel&lt;/em&gt; for what
they look like.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://www.theguardian.com/society/ng-interactive/2015/sep/02/unaffordable-country-where-can-you-afford-to-buy-a-house"&gt;Where Can You Afford to Buy a House?&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;None of the individual elements are groundbreaking, but this is a well
put-together datavis. It highlights how unaffordable the UK is becoming,
and it doesn't even include more recent data. It takes a stellar salary
for London to be affordable, but that's no surprise.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.r2d3.us/visual-intro-to-machine-learning-part-1/"&gt;A Visual Introduction to Machine Learning&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This final one isn't a datavis per se, but it's one of the best
"introduction to machine learning" resources I've ever seen. It follows
through a concrete example with great visuals and it still manages to be
densely packed with information.&lt;/p&gt;
&lt;p&gt;Let me know in the comments if you have any data visualisations you'd
like to share! &lt;/p&gt;
&lt;p&gt;Footnote: This was the 21&lt;sup&gt;st&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category></entry><entry><title>More on K-means Clustering</title><link href="/blog/more-on-k-means-clustering" rel="alternate"></link><published>2016-11-20T17:01:00+00:00</published><updated>2016-11-20T17:01:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-20:/blog/more-on-k-means-clustering</id><summary type="html">&lt;p&gt;In &lt;a href="/blog/introduction-to-k-means-clustering/"&gt;Part 1&lt;/a&gt;
I described the k-means clustering algorithm and some of its uses along
with a quick Python implementation. Going forward I recommend using the
&lt;a href="http://scikit-learn.org/stable/modules/clustering.html#k-means"&gt;scikit-learn implementation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now let's see k-means in action!
 &lt;/p&gt;
&lt;h1&gt;Image Segmentation&lt;/h1&gt;
&lt;p&gt;One use of clustering is to segment images:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, in computer graphics, color …&lt;/p&gt;&lt;/blockquote&gt;</summary><content type="html">&lt;p&gt;In &lt;a href="/blog/introduction-to-k-means-clustering/"&gt;Part 1&lt;/a&gt;
I described the k-means clustering algorithm and some of its uses along
with a quick Python implementation. Going forward I recommend using the
&lt;a href="http://scikit-learn.org/stable/modules/clustering.html#k-means"&gt;scikit-learn implementation&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Now let's see k-means in action!
 &lt;/p&gt;
&lt;h1&gt;Image Segmentation&lt;/h1&gt;
&lt;p&gt;One use of clustering is to segment images:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For example, in computer graphics, color quantization is the task of
reducing the color palette of an image to a fixed number of colors
&lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;small&gt;From: &lt;a href="https://en.wikipedia.org/wiki/K-means_clustering#Vector_quantization"&gt;k-means clustering (Wikipedia)&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Given an image, we can use k-means clustering to find similar colours in
the image, and re-draw it with fewer colours. This has uses in data
compression for example.&lt;/p&gt;
&lt;p&gt;This is the example I'll run through today.&lt;/p&gt;
&lt;p&gt;We'll take this image of a puppy:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A puppy" src="/images/more-on-k-means-clustering/puppy.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Puppy image from &lt;a href="https://www.flickr.com/photos/gregcullen/250779651/in/photolist-oaj46-cTecd9-sHsHk-5WP7e-9jLY1e-dMrkp4-oak27-8LgQUd-72uozM-9N6oDE-4VoQq-fGnMUG-fkMAUo-hSg7Vm-9xukWa-7K3S2B-fz3KAH-aWe43R-HhRcz-4SZHdM-3d8eAm-Gh4ip-c3LHsG-y6YdK-e4Qn6h-y6U3M-48xfrF-qaZttJ-8MuTV2-aDsi2E-db1Ujw-oxFiuK-y6Ynf-oBGkqj-bVUar-5ft6bn-mwDdV-4BeWnC-itR65i-8d1bVK-5CSiqu-fNwmya-7kTing-7oySVC-boenVS-bBvADe-5fmmvh-4j3Q9U-53pHvi-4qFWve"&gt;Greg on Flickr&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;and redraw it in much fewer colours using k-means clustering.&lt;/p&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;Let's start by defining our data. To represent this problem, we take
each pixel in the image as a data point whose 3 features are the R, G
and B values of the pixel.&lt;/p&gt;
&lt;p&gt;For this particular image this gives us a dataset with 3 columns and
43,680 rows. Some of the pixels are the same colour, but we've still got
over 10,000 unique colours in our image.&lt;/p&gt;
&lt;h2&gt;Running K-means&lt;/h2&gt;
&lt;p&gt;It is conceivable that we can group similar colours together and redraw
the same image with fewer colours in a way that we can still tell what
is in the image. This would reduce the amount of information needed to
represent the image (and therefore the filesize) without visibly losing
much detail.&lt;/p&gt;
&lt;p&gt;Once we have our dataset (the details of extracting the pixel values
will be in the accompanying Jupyter notebook) running the algorithm with
scikit-learn is easy:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.cluster&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;KMeans&lt;/span&gt;

&lt;span class="n"&gt;kmeans&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;KMeans&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_clusters&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# our dataframe (df) is the image data&lt;/span&gt;
&lt;span class="n"&gt;clusters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kmeans&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit_predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# &amp;quot;clusters&amp;quot; is a vector with a cluster assignment for each data point (pixel)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cluster&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;clusters&lt;/span&gt;
&lt;span class="c1"&gt;# use the K cluster centroids as new colours to represent the image&lt;/span&gt;
&lt;span class="n"&gt;colours&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;tuple&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;kmeans&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cluster_centers_&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In this instance the cluster centroids are points in 3D space, where the
3 dimensions are R, G and B, so the centroids can be thought of as
colours themselves.&lt;/p&gt;
&lt;p&gt;That means once we have the cluster assignment for each pixel, plus the
centroid as the associated colour value, we can reconstruct our image
pixel by pixel to get the &lt;span class="math"&gt;\(k\)&lt;/span&gt;-colour representation.&lt;/p&gt;
&lt;p&gt;The same image with only 3 colours looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A 3-colour puppy" src="/images/more-on-k-means-clustering/puppy_3.jpg"&gt;&lt;/p&gt;
&lt;p&gt;The same puppy drawn with only 3 colours&lt;/p&gt;
&lt;p&gt;As you can clearly see, we've reduced the number of colours required,
and incidentally also reduced the filesize threefold, without losing too
much information. The image is still clearly a puppy, despite the fact
that we've only used 3 colours.&lt;/p&gt;
&lt;p&gt;When we use 16 colours the image starts to resemble the original in much
more detail:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A 16-colour puppy" src="/images/more-on-k-means-clustering/puppy_16.jpg"&gt;&lt;/p&gt;
&lt;p&gt;A 16-colour puppy &lt;/p&gt;
&lt;p&gt;You can still see the background isn't smooth but we're getting close.
In fact, we would get to an image that is indistinguishable from the
original by using far less than the original 10,000 colours.&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/k-means/Image%20Clustering%20with%20scikit-learn.ipynb"&gt;Jupyter notebook for drawing puppies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I have a couple of points left to raise, namely some practical tips when
using clustering.&lt;/p&gt;
&lt;h1&gt;Choosing K&lt;/h1&gt;
&lt;p&gt;How would we know which point to stop at? When is &lt;span class="math"&gt;\(k\)&lt;/span&gt; at its optimal
value?&lt;/p&gt;
&lt;p&gt;As I mentioned in part 1, this is usually somewhat subjective, but there
are some general heuristics.&lt;/p&gt;
&lt;p&gt;In this case we could do it by visual inspection. That is, we could say
&lt;span class="math"&gt;\(k\)&lt;/span&gt; is high enough when we are no longer visually able to tell the
difference between the original and the redrawn images.&lt;/p&gt;
&lt;p&gt;Not all datasets will lend themselves to visual inspection like this
though.&lt;/p&gt;
&lt;p&gt;We can use what's called the &lt;em&gt;elbow method&lt;/em&gt; to evaluate when to stop.&lt;/p&gt;
&lt;p&gt;For each value of &lt;span class="math"&gt;\(k\)&lt;/span&gt; you want to evaluate how much of the variance in
your data is explained by the configurations of those &lt;span class="math"&gt;\(k\)&lt;/span&gt; clusters.
This value increases for each value of &lt;span class="math"&gt;\(k\)&lt;/span&gt;, but the idea is that we
stop increasing &lt;span class="math"&gt;\(k\)&lt;/span&gt; when increasing it gives us diminishing returns.&lt;/p&gt;
&lt;p&gt;Let's think of the two extremes. When &lt;span class="math"&gt;\(k\)&lt;/span&gt; = 1, it means every point
will belong to the &lt;em&gt;same&lt;/em&gt; cluster. This configuration explains 0% of the
variance in your data, because it says all your data points are the
same. Then &lt;span class="math"&gt;\(k\)&lt;/span&gt; is equal to the number of data points you have, it
means every point will belong to &lt;em&gt;its own cluster&lt;/em&gt;. This configuration
explains 100% of the variation in your data because it says each of your
data points is different from every other one. A value in between will
explain some % of the variation, because it will say some data points
are equal to some other data points, and different from some others.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://bl.ocks.org/rpgove/0060ff3b656618e9136b"&gt;Here's a good explanation&lt;/a&gt; of the
elbow method, although it uses the "error" for each &lt;span class="math"&gt;\(k\)&lt;/span&gt; value, so the
graph is upside down compared to the "variance explained" metric I
discussed above.&lt;/p&gt;
&lt;p&gt;Either way, there is usually an "elbow" where the increase/decrease is
suddenly less sharp. That's usually a good point to stop and use that
value for &lt;span class="math"&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;h1&gt;Normalisation&lt;/h1&gt;
&lt;p&gt;As I mentioned in the Self-Organising Maps tutorial, in practice you
will want to normalise your data so all features are on the same scale.
This is also true of k-means clustering. If all features are on the same
scale, each feature will "contribute" to the algorithm equally,
otherwise a feature with much larger values will dominate the others.&lt;/p&gt;
&lt;p&gt;In the case of colours, the R, G, and B values are all on the same scale
(0 to 255) so this is not necessary, but in real world examples your
features will often be on different scales.&lt;/p&gt;
&lt;p&gt;See more information on this in Sebastian Raschka's &lt;a href="http://sebastianraschka.com/faq/docs/when-to-standardize.html"&gt;machine learning FAQ&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;K-means and clustering in general have many more uses, and I hope these
puppies have piqued your interest!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 20&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Introduction to K-means Clustering</title><link href="/blog/introduction-to-k-means-clustering" rel="alternate"></link><published>2016-11-20T15:18:00+00:00</published><updated>2016-11-20T15:18:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-20:/blog/introduction-to-k-means-clustering</id><summary type="html">&lt;p&gt;This is the first of a two-part post on K-means clustering.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;h2&gt;Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;I've talked about unsupervised learning before when dealing with
&lt;a href="/blog/self-organising-maps-an-introduction/"&gt;Self-Organising Maps&lt;/a&gt;,
but just to recap. Unsupervised learning is when you have a dataset of
features with no pre-defined outcomes. You give it to an algorithm to …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the first of a two-part post on K-means clustering.&lt;/p&gt;
&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;h2&gt;Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;I've talked about unsupervised learning before when dealing with
&lt;a href="/blog/self-organising-maps-an-introduction/"&gt;Self-Organising Maps&lt;/a&gt;,
but just to recap. Unsupervised learning is when you have a dataset of
features with no pre-defined outcomes. You give it to an algorithm to
learn patterns in the data without knowing in advance what associations
you want it to learn.&lt;/p&gt;
&lt;p&gt;So you're not trying to teach it to tell the difference between images
of cats and dogs; instead, you're trying to make it learn something
about the structure of the images, so it can find similar images without
explicitly knowing about cats and dogs.&lt;/p&gt;
&lt;p&gt;K-means is a type of unsupervised learning method, specifically a type
of clustering.&lt;/p&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;Clustering deals with finding groups of similar data points.&lt;/p&gt;
&lt;p&gt;There are two criteria that make a "good" set of clusters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Intra-cluster similarity&lt;/strong&gt;. That is, all the data points within a
    cluster should be similar to each other (we'll deal with what
    'similar' means a bit later).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inter-cluster dissimilarity.&lt;/strong&gt; That is, data points in one cluster
    should be sufficiently different from data points in another
    cluster.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is also what we're trying to achieve with k-means. It is only one
of the &lt;a href="http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html"&gt;many types&lt;/a&gt;
of clustering algorithm, but I've chosen it as it's popular as well as
being easy to understand and implement.&lt;/p&gt;
&lt;h2&gt;Uses of Clustering&lt;/h2&gt;
&lt;p&gt;What are some uses of clustering? Finding similarities in your data that
you couldn't do by inspection has a lot of uses. The classic example is
"segmenting your customer base", that is identifying customers with
similar buying behaviours for better targeted advertising. Another form
of clustering, hierarchical clustering, is &lt;a href="http://astronomy.swin.edu.au/cosmos/h/hierarchical+clustering"&gt;used in astronomy&lt;/a&gt;.
You can even use it &lt;a href="/blog/analysing-london-house-prices/"&gt;to find similar boroughs in London based on house-buying behaviour&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Potential Problems&lt;/h2&gt;
&lt;h3&gt;Choosing K&lt;/h3&gt;
&lt;p&gt;Before you start training your data to learn the clusters, you need to
choose a value for &lt;span class="math"&gt;\(k\)&lt;/span&gt;. That is, you have to decide beforehand how
many groups there are going to be. This sounds like it defeats the
purpose of being unsupervised, and it is indeed something that you have
to set manually.&lt;/p&gt;
&lt;p&gt;Due to the random nature of the initialisation of the algorithm, and the
uncertainty in the correct value for &lt;span class="math"&gt;\(k\)&lt;/span&gt;, it is advisable to re-run
the algorithm multiple times and decide on which configuration to use.&lt;/p&gt;
&lt;h3&gt;Subjective Evaluation&lt;/h3&gt;
&lt;p&gt;As with unsupervised algorithms in general, evaluating the outcome is
partly subjective. There are objective measures with which we can
compare different runs, but the final evaluation will be based on which
one &lt;em&gt;feels&lt;/em&gt; best.&lt;/p&gt;
&lt;p&gt;Sometimes you can look at the characteristics of the clusters to see
which one makes most sense. For example, if segmenting your customers
into clusters results in two clusters that both contain mostly customers
over 60, you might choose another run that better separates your
customers based on age. Of course, the clusters will be created based on
your data, so if there really are two distinct groups of over-60
customers then no amount of runs will change that!&lt;/p&gt;
&lt;h1&gt;The K-means Algorithm (with words)&lt;/h1&gt;
&lt;p&gt;Clusters have two properties: a &lt;strong&gt;centroid&lt;/strong&gt; and a set of your data
points that are assigned to the cluster.&lt;/p&gt;
&lt;p&gt;The centroid is simply a point which is the &lt;strong&gt;mean&lt;/strong&gt; of the data points
that belong to it (hence, "k-means").&lt;/p&gt;
&lt;p&gt;Mathematically, the centroids are a point in n-dimensional space, where
n is the number of features your data has.&lt;/p&gt;
&lt;p&gt;The basic idea behind the k-means clustering algorithm is simple:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Start with a chosen value of &lt;span class="math"&gt;\(k\)&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Choose &lt;span class="math"&gt;\(k\)&lt;/span&gt; of your data points at random to be your starting
    centroids.&lt;/li&gt;
&lt;li&gt;For each data point, assign it to a cluster based on which of the
    &lt;span class="math"&gt;\(k\)&lt;/span&gt; centroids it is &lt;em&gt;closest&lt;/em&gt; to. Closest can mean any distance
    measure. The Euclidean distance is often used.&lt;/li&gt;
&lt;li&gt;Now you have &lt;span class="math"&gt;\(k\)&lt;/span&gt; groups of data points assigned to a cluster.
    Re-calculate the position of each cluster centroid by taking the
    &lt;em&gt;mean&lt;/em&gt; of the new points that are now associated with that cluster.&lt;/li&gt;
&lt;li&gt;Repeat steps 3 and 4 until convergence. You are typically done when
    no points have changed clusters since the last iteration.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;It's better to see this happen visually - &lt;a href="http://www.onmyphd.com/?p=k-means.clustering#h3_goodexample"&gt;here's a good example&lt;/a&gt;.
 &lt;/p&gt;
&lt;h1&gt;The K-means Algorithm (with code)&lt;/h1&gt;
&lt;p&gt;Let's go through the steps again with code. Let's use &lt;a href="http://archive.ics.uci.edu/ml/datasets/Iris"&gt;the iris dataset&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Steps 1 &amp;amp; 2 - Initialisation&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;iris.csv&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# I&amp;#39;ll supply this alongside the Jupyter notebook&lt;/span&gt;
&lt;span class="c1"&gt;# we don&amp;#39;t need the target variable&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;class&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;inplace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;centroids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="n"&gt;clusters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;

&lt;span class="c1"&gt;# random initialisation of centroids = pick K data points at random as centroids&lt;/span&gt;
&lt;span class="n"&gt;init_centroids&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;replace&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;init_centroids&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# get the data point at index i&lt;/span&gt;
    &lt;span class="n"&gt;pt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,:]&lt;/span&gt;
    &lt;span class="c1"&gt;# append it to the centroids list&lt;/span&gt;
    &lt;span class="n"&gt;centroids&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Steps 3 &amp;amp; 4 - Learning&lt;/h2&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;assign_to_cluster&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# calculate distance (without sqrt) to each centroid&lt;/span&gt;
    &lt;span class="n"&gt;distances&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;centroids&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;distances&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(((&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
    &lt;span class="c1"&gt;# find index of closest cluster&lt;/span&gt;
    &lt;span class="n"&gt;closest&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argmin&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distances&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# assign point to that cluster&lt;/span&gt;
    &lt;span class="n"&gt;clusters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;closest&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;point&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cluster&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;closest&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# first, reset the clusters&lt;/span&gt;
    &lt;span class="n"&gt;clusters&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;clusters&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;([])&lt;/span&gt;
    &lt;span class="c1"&gt;# assign each data point to nearest cluster&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="n"&gt;assign_to_cluster&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="c1"&gt;# now, recalculate the centroids&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;centroids&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;clusters&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;First we've defined a function that calculates the distance between a
data point and each of the cluster centroids. We can use the
implementation trick where we don't need the &lt;em&gt;actual&lt;/em&gt; Euclidean distance
(to avoid the expensive square root operation). We then assign the data
point to that cluster.&lt;/p&gt;
&lt;p&gt;Then we iteratively assign points to the clusters and re-position the
cluster centroids. With the new centroid positions, we assign points
again, then re-calculate the centroids again and so on.&lt;/p&gt;
&lt;p&gt;In this example I've forced it to run just 20 times, so there might
still be room for improvement, but typically you'd run this until no
points have changed cluster since the last iteration.&lt;/p&gt;
&lt;p&gt;Once we've done that we can plot two of the data's dimensions and colour
each point by its assigned cluster (and mark the cluster centroids).&lt;/p&gt;
&lt;p&gt;We've gone from this plot of raw data:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Iris petal plot (no clusters)" src="/images/introduction-to-k-means-clustering/kmeans_iris_1.png"&gt;&lt;/p&gt;
&lt;p&gt;Plot of raw data before clustering&lt;/p&gt;
&lt;p&gt;To this plot where we've clustered our points into 3 groups:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Iris data with 3 clusters" src="/images/introduction-to-k-means-clustering/kmeans_iris_2.png"&gt;&lt;/p&gt;
&lt;p&gt;3 clusters after just 20 iterations&lt;/p&gt;
&lt;p&gt;There is still some overlap between the black and blue clusters, but
just 20 iterations have quite effectively grouped our data into 3
clusters.&lt;/p&gt;
&lt;p&gt;That's all there is to it!&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/k-means/K-Means.ipynb"&gt;associated Jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In Part 2, I'll talk about another practical application of the k-means
algorithm (using scikit-learn this time) as well as some implementation
details such as how to pick the value of &lt;span class="math"&gt;\(k\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 19&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category><category term="python"></category></entry><entry><title>All Your Web Data Are Belong To Us</title><link href="/blog/all-your-web-data-are-belong-to-us" rel="alternate"></link><published>2016-11-18T19:30:00+00:00</published><updated>2016-11-18T19:30:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-18:/blog/all-your-web-data-are-belong-to-us</id><summary type="html">&lt;p&gt;Yesterday I was casually browsing Twitter when I came across this.&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;The UK has just legalized the most extreme surveillance in the history of western democracy. It goes farther than many autocracies. &lt;a href="https://t.co/yvmv8CoHrj"&gt;https://t.co/yvmv8CoHrj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Edward Snowden (@Snowden) &lt;a href="https://twitter.com/Snowden/status/799371508808302596?ref_src=twsrc%5Etfw"&gt;November 17, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;"That doesn't sound good", I thought.&lt;/p&gt;
&lt;p&gt;This is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday I was casually browsing Twitter when I came across this.&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;The UK has just legalized the most extreme surveillance in the history of western democracy. It goes farther than many autocracies. &lt;a href="https://t.co/yvmv8CoHrj"&gt;https://t.co/yvmv8CoHrj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Edward Snowden (@Snowden) &lt;a href="https://twitter.com/Snowden/status/799371508808302596?ref_src=twsrc%5Etfw"&gt;November 17, 2016&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;"That doesn't sound good", I thought.&lt;/p&gt;
&lt;p&gt;This is in reference to something called the Investigative Powers (IP)
Bill, or the "Snooper's Charter" as it's been referred to. The nickname
sounds like it's something out of "&lt;a href="https://en.wikipedia.org/wiki/Yes_Minister"&gt;Yes Minister&lt;/a&gt;", but despite its
silly nickname it's a very serious piece of news.&lt;/p&gt;
&lt;h1&gt;What Is the IP Bill?&lt;/h1&gt;
&lt;p&gt;It effectively gives the UK government much more power over mass
internet surveillance.&lt;/p&gt;
&lt;p&gt;You can read a full rundown of what it entails &lt;a href="http://www.wired.co.uk/article/ip-bill-law-details-passed"&gt;in this Wired article&lt;/a&gt;, but
here are some highlights (emphasis mine):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;For the first time, security services will be able to hack into
 computers, networks, mobile devices, servers and more under the
 proposed plans. The power will be available to police forces and
 intelligence services. [...] Warrants must be issued for the hacking
 to take place.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;Data can be gathered from "a large number of devices in the specified
 location". [...] As a result, &lt;/em&gt;&lt;em&gt;it is likely the data of innocent
 people would be gathered&lt;/em&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;[...] internet history data (Internet Connection Records, in
 official speak) will &lt;/em&gt;&lt;em&gt;have to be stored for 12 months.&lt;/em&gt;**&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;em&gt;[...] intelligence agencies will also be able to obtain and use
 "bulk personal datasets". These mass data sets mostly include a
 "majority of individuals" that aren't suspected in any wrongdoing but
 have been &lt;/em&gt;&lt;em&gt;swept-up in the data collection.&lt;/em&gt;**&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Let that sink in: your ISP will now be &lt;strong&gt;legally required&lt;/strong&gt; to store a
year's worth of your browsing history.&lt;/p&gt;
&lt;h1&gt;Sneak Attack&lt;/h1&gt;
&lt;p&gt;This is all shocking in itself, but the part that really surprised me
was how unaware I'd been that this was in the works.&lt;/p&gt;
&lt;p&gt;Obviously this is also a reflection on my effectiveness at following the
news, but quite a few people I talked to about it since then were as
surprised by it as me.&lt;/p&gt;
&lt;p&gt;The other big surprise was how little attention it got on Hacker News.&lt;/p&gt;
&lt;p&gt;A quick search reveals that there are around 80 mentions of "Snooper's
Charter" in HN stories, and the most comments any story got was &lt;a href="https://news.ycombinator.com/item?id=8938223"&gt;105, two years ago&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;By HN standards that's not a lot. The thread the day after Brexit got
&lt;a href="https://news.ycombinator.com/item?id=11966167"&gt;over 2,500&lt;/a&gt;. This is
perhaps not a big a story as Brexit, but it's also not 25 times less
important.&lt;/p&gt;
&lt;p&gt;My experience, based on people I've spoken to and anecdotes I've read,
is that people are far less interested in data privacy and mass
surveillance than I'd have thought. The fact that even the HN crowd
weren't kicking up a fuss was much less expected.&lt;/p&gt;
&lt;h1&gt;What We Can Do&lt;/h1&gt;
&lt;p&gt;So is there anything we can do about this? I've been thinking about this
a bit today, and there are some concrete steps we can take.&lt;/p&gt;
&lt;h2&gt;Technology&lt;/h2&gt;
&lt;p&gt;If you're interested in making a change based on this outcome and if,
like me, you've become increasingly conscious about losing control over
who has access to your personal information, one obvious idea is to get
a VPN.&lt;/p&gt;
&lt;p&gt;This may change of course, but so far &lt;a href="http://www.ibtimes.co.uk/snoopers-charter-why-arent-vpns-tor-mentioned-investigatory-powers-bill-1527403"&gt;VPNs are not mentioned in the IP Bill&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You could also use &lt;a href="https://www.torproject.org/projects/torbrowser.html.en"&gt;the Tor browser&lt;/a&gt; for
further anonymity.&lt;/p&gt;
&lt;h2&gt;Social "Engineering"&lt;/h2&gt;
&lt;p&gt;As software developers, engineers, sysadmins or other computer-related
professionals we need to help our less tech-savvy friends and family by
educating them about these things. It's not easy persuading someone
about the importance of data privacy for example, but it's a
conversation worth having.&lt;/p&gt;
&lt;p&gt;In this instance maybe even the tech-savvy people around you, who might
be less interested in politics, have had this story go under the radar.
Next time you speak to them bring it up in casual conversation, you
might be helping them out.&lt;/p&gt;
&lt;p&gt;Let's not let this story be swept under the rug.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 18&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="technology commentary"></category></entry><entry><title>Turning Jupyter Notebooks into Reusable Scripts</title><link href="/blog/turning-jupyter-notebooks-into-reusable-scripts" rel="alternate"></link><published>2016-11-17T19:56:00+00:00</published><updated>2016-11-17T19:56:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-17:/blog/turning-jupyter-notebooks-into-reusable-scripts</id><summary type="html">&lt;p&gt;I read an article today called Data Scientists Need More Automation. No
prizes for guessing what it was about.&lt;/p&gt;
&lt;p&gt;A lot of the specifics were focused on sysadmin-type work like using
SSH, but the main idea is one that applies to all data science tasks.&lt;/p&gt;
&lt;p&gt;The thrust of the article …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I read an article today called Data Scientists Need More Automation. No
prizes for guessing what it was about.&lt;/p&gt;
&lt;p&gt;A lot of the specifics were focused on sysadmin-type work like using
SSH, but the main idea is one that applies to all data science tasks.&lt;/p&gt;
&lt;p&gt;The thrust of the article was:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Someone please help data scientists be lazier, do less work, and
reduce the mental overhead of dealing with computers!&lt;/p&gt;
&lt;p&gt;&lt;small&gt;From &lt;a href="http://stiglerdiet.com/blog/2016/Nov/15/data-scientists-need-more-automation/"&gt;Data Scientists Need More Automation&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As part of my commitment to occasionally talk about "programming for
data scientists", I want to share ideas that will facilitate this to
help data scientists focus on important stuff.&lt;/p&gt;
&lt;p&gt;Laziness is a virtue when it comes to programming.&lt;/p&gt;
&lt;p&gt;Always thinking "how can I do the same thing with less effort?" is a
great way to be more productive and focus on the hard parts of data
science.&lt;/p&gt;
&lt;p&gt;For example, it's clear that you want to speed up and automate your data
cleaning. That's not the important stuff you want to focus on. So in
this post I want to share some thoughts on how to make your Jupyter
notebooks easier to "productionise".&lt;/p&gt;
&lt;p&gt;When you do data cleaning, notebooks are a great way to experiment with
your code in an interactive way before you can create a script that runs
on gigabytes of data. These thoughts are mostly concerned with how you
take a notebook that can clean a specific file, and make it into a
Python script you could run in the background to process many similar
files automatically.&lt;/p&gt;
&lt;h2&gt;Start Small&lt;/h2&gt;
&lt;p&gt;If your datasets are big enough that processing them takes longer than a
few seconds, you are going to lose a lot of time if don't you test your
code on a smaller subset first.&lt;/p&gt;
&lt;p&gt;If you are cleaning your data, you shouldn't be using your entire
dataset until you can prove that your script will run on a smaller
version of it. That might be as easy as just restricting your dataframe
to its first N rows:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_huge_file.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# delete this line later, but only when you&amp;#39;re ready!&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This might sound obvious but it's important to get into the habit of
doing it.&lt;/p&gt;
&lt;h3&gt;Brief Digression: Subsets of Data for Machine Learning&lt;/h3&gt;
&lt;p&gt;For machine learning, if you're just testing your code to make sure it
runs, you can do the same thing and take the first few hundred rows.
Obviously if you're training predictive models you want to use your
entire dataset.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;However&lt;/em&gt;, if you want to just get a sense of which models are more
accurate than others, in the case of classification problems you can use
a &lt;strong&gt;stratified&lt;/strong&gt; subset of your data. Instead of taking a random sample
you can sample based on the frequencies of your classes, so that your
smaller sample has the same class proportions. In scikit-learn you can
use
&lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html"&gt;StratifiedShuffleSplit&lt;/a&gt;
for example.&lt;/p&gt;
&lt;h3&gt;Back to Notebooks...&lt;/h3&gt;
&lt;p&gt;Once you've experimented enough with your code so that you know it works
on your small subset, you'll want to ensure your code is general enough
that it would run with any file you give it.&lt;/p&gt;
&lt;p&gt;For example, if you have a dataset that covers one day's of data you
might eventually want to let it loose and process months of data one day
at a time.&lt;/p&gt;
&lt;p&gt;The obvious way to do this is parametrisation.&lt;/p&gt;
&lt;h2&gt;Use Parameters&lt;/h2&gt;
&lt;p&gt;Stop hard-coding things.&lt;/p&gt;
&lt;p&gt;Seriously, whenever you have a value that is likely to change when you
run the script multiple times, make it a variable.&lt;/p&gt;
&lt;p&gt;Turn this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_data_2016-01-01.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Into this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;filepath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;my_data_2016-01-01.csv&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;It's an extra line but you're going to have to do it if you want to
automate your script, so get in the habit of starting out like this.&lt;/p&gt;
&lt;p&gt;Even better, create a separate cell at the top of your notebook &lt;strong&gt;just
for parameters&lt;/strong&gt;. That way you won't forget which things will need to
change for each file.&lt;/p&gt;
&lt;h2&gt;Converting Parameters to Command Line Arguments&lt;/h2&gt;
&lt;p&gt;If you've created the right variables for automation, you can convert
them to command line arguments.&lt;/p&gt;
&lt;p&gt;This can be as simple as exporting your notebook to a Python file (File
-&amp;gt; Download as -&amp;gt; Python) and replacing your parameters with
command line arguments.&lt;/p&gt;
&lt;p&gt;Assuming your notebook looks something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2016-01-01&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;other_parameter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt;

&lt;span class="c1"&gt;# rest of your code here...&lt;/span&gt;
&lt;span class="c1"&gt;# ...&lt;/span&gt;
&lt;span class="c1"&gt;# ...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Export it to Python, then amend the script slightly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;other_parameter&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# the rest of your code from the notebook&lt;/span&gt;
    &lt;span class="c1"&gt;# can be pasted here UNEDITED&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;All we've done is replace the hard-coded parameter values with arguments
from the command line (remember &lt;a href="http://stackoverflow.com/a/2626634/2039162"&gt;sys.argv[0] is the name of the script&lt;/a&gt;), so now you can do
this on the command line:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;python&lt;/span&gt; &lt;span class="n"&gt;my_automated_script&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;py&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2016-01-01&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;15&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This is the "quick and dirty" way of doing it. For more robustness and
better documentation of your arguments, use
&lt;a href="https://docs.python.org/3/library/argparse.html"&gt;argparse&lt;/a&gt; or
&lt;a href="https://pypi.python.org/pypi/begins/0.9"&gt;begins&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;To make this workflow possible, you also need to make sure your notebook
doesn't do too much.&lt;/p&gt;
&lt;h2&gt;Single-Purpose Notebooks&lt;/h2&gt;
&lt;p&gt;There might be a lot of code involved in cleaning your data. You might
need to deal with things like missing values, but then perform
transformations and computations.&lt;/p&gt;
&lt;p&gt;Eventually your notebook might be hundreds of lines of code.&lt;/p&gt;
&lt;p&gt;If you ever get to that point, break the notebook into multiple smaller
ones. You could even make the first notebook output a semi-cleaned
version of your data which your second notebook picks up.&lt;/p&gt;
&lt;p&gt;You can then still combine your notebooks into one Python script by
exporting them, and just removing the intermediate files that you were
creating when experimenting.&lt;/p&gt;
&lt;p&gt;Say your notebook workflow is like this:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Notebook 1 reads raw csv&lt;/li&gt;
&lt;li&gt;Notebook 1 does some data cleaning&lt;/li&gt;
&lt;li&gt;Notebook 1 exports semi-cleaned data (intermediate csv)&lt;/li&gt;
&lt;li&gt;Notebook 2 reads in intermediate csv&lt;/li&gt;
&lt;li&gt;Notebook 2 does data transformations&lt;/li&gt;
&lt;li&gt;Notebook 2 exports final csv&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You can see that if we export both notebooks, combine them into a single
file and &lt;strong&gt;remove steps 3 and 4&lt;/strong&gt;, we get our final automated script. In
fact, if we encounter problems later on we can always go back and debug
them using our notebooks, and re-export them to get an updated version
of the script. As long as you make all your changes in the notebooks and
not the final script, this will be a valuable approach.&lt;/p&gt;
&lt;p&gt;Hopefully I've given you some ideas about how you can design your
notebooks from the start with a view to future automation.&lt;/p&gt;
&lt;p&gt;Now go forth and be lazy!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 17&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="programming for data scientists"></category><category term="python"></category></entry><entry><title>Getting Started</title><link href="/blog/getting-started" rel="alternate"></link><published>2016-11-16T19:58:00+00:00</published><updated>2016-11-16T19:58:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-16:/blog/getting-started</id><summary type="html">&lt;p&gt;Following on from &lt;a href="/blog/just-do-stuff/"&gt;yesterday's post&lt;/a&gt; about getting
stuff done I want to address what I think is the hardest part about it:
getting started.&lt;/p&gt;
&lt;p&gt;That may sound obvious, so let me explain.&lt;/p&gt;
&lt;h2&gt;Starting is Hard&lt;/h2&gt;
&lt;p&gt;Based on my observations, I'm more likely to sit down and work on a side …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Following on from &lt;a href="/blog/just-do-stuff/"&gt;yesterday's post&lt;/a&gt; about getting
stuff done I want to address what I think is the hardest part about it:
getting started.&lt;/p&gt;
&lt;p&gt;That may sound obvious, so let me explain.&lt;/p&gt;
&lt;h2&gt;Starting is Hard&lt;/h2&gt;
&lt;p&gt;Based on my observations, I'm more likely to sit down and work on a side
project or blog post if I've already got something to work on. That's
either something that I started previously, or just an idea that's been
bouncing around my head long enough for me to want to act on it.&lt;/p&gt;
&lt;p&gt;I'm also typically unlikely to cut my work short once I've started it.
Programming is a &lt;a href="https://en.wikipedia.org/wiki/Flow_(psychology)"&gt;flow&lt;/a&gt;
activity and, unlike say &lt;em&gt;doing your taxes&lt;/em&gt;, once you get started it's
rare to feel like stopping after 10 minutes.&lt;/p&gt;
&lt;p&gt;That means if you're like me, the hardest part is the initial hurdle.&lt;/p&gt;
&lt;p&gt;Getting started.
 &lt;/p&gt;
&lt;h2&gt;Why Starting is Hard&lt;/h2&gt;
&lt;p&gt;There are a few reasons why it takes considerable effort to sit down and
start.&lt;/p&gt;
&lt;h3&gt;Fear of Wasting Time&lt;/h3&gt;
&lt;p&gt;This ties in with the point about it being easier to continue working on
something than starting something completely new. I often worry about
how productively I'm spending my free time. If I start working on
Project X, that will mean I'm not progressing with Project Y and working
on Project Z would also be a good use of my time.&lt;/p&gt;
&lt;p&gt;All this does is stop me from working on any of them.&lt;/p&gt;
&lt;h3&gt;"I Don't Have Any Free Time"&lt;/h3&gt;
&lt;p&gt;The biggest lie we tell ourselves.&lt;/p&gt;
&lt;p&gt;There are obviously days or weeks when it's true, but isn't it funny how
I can &lt;strong&gt;always&lt;/strong&gt; find time for another Netflix episode?&lt;/p&gt;
&lt;h3&gt;Too Many Ideas&lt;/h3&gt;
&lt;p&gt;A variant of previous points, but it's worth mentioning that sometimes
you just can't decide which of your ideas to work on, only to give up
working on any of them. Yes it sounds like a laughable excuse, but we're
all great at fooling ourselves.&lt;/p&gt;
&lt;h3&gt;Perfectionism&lt;/h3&gt;
&lt;p&gt;In one of my favourite posts on the topic of procrastination, David Cain
talks about perfectionism.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;For a procrastinator of my kind, perfection (or something negligibly
close to it) thereby becomes the only result that allows one to be
comfortable with him&lt;em&gt;self&lt;/em&gt;. A procrastinator becomes
disproportionately motivated by the pain of failure.
&lt;small&gt;From: &lt;a href="http://www.raptitude.com/2011/05/procrastination-is-not-laziness"&gt;Procrastination Is Not Laziness&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This is especially true of work that you will inevitably share somehow,
be it on a blog or Github, or just by showing it to your friends. If
you're a certain kind of person, this can be a very powerful deterrent
indeed.&lt;/p&gt;
&lt;h3&gt;Impostor Syndrome&lt;/h3&gt;
&lt;p&gt;Related to the above. If you are too concerned with how good your work
is compared to everyone else's (especially that of experts in the field)
you can end up convincing yourself that there's no point even starting.&lt;/p&gt;
&lt;h2&gt;Tips on Getting Started&lt;/h2&gt;
&lt;p&gt;So I realise that all sounds insurmountable. I've found that the key is
to use these mind games to your advantage and effectively try to trick
yourself into starting.&lt;/p&gt;
&lt;p&gt;Based on my own experience I have some suggestions.&lt;/p&gt;
&lt;h3&gt;Artificial Deadlines&lt;/h3&gt;
&lt;p&gt;In some cases, pressure can be a good thing. With side projects, there's
rarely any pressure to get things done, so creating an artificial
deadline for yourself is a good way to focus.&lt;/p&gt;
&lt;p&gt;That begs the question of how you avoid procrastinating over this new
artificial deadline.&lt;/p&gt;
&lt;p&gt;My suggestion for that is to &lt;strong&gt;externalise&lt;/strong&gt; the pressure.&lt;/p&gt;
&lt;p&gt;Tell a friend that you're working on Project X and you'd like to show it
to them next Tuesday. Or tweet about it, saying a blog post is coming on
Friday. It's critical that you set a specific time for this, otherwise
you can keep pushing it back.&lt;/p&gt;
&lt;p&gt;By doing this, you're making yourself accountable to &lt;em&gt;other people.&lt;/em&gt;
Sure there's probably no consequence if you miss the deadline, but it
might still be enough to help you push through the first step.&lt;/p&gt;
&lt;p&gt;You're also putting your future self in a situation where &lt;strong&gt;not&lt;/strong&gt; doing
stuff is the worse outcome, because you'd have to go through the shame
of going back on your word.&lt;/p&gt;
&lt;p&gt;A cheap trick, but it works.&lt;/p&gt;
&lt;h3&gt;Unrealistic Goals&lt;/h3&gt;
&lt;p&gt;It might sound odd, but actually aiming higher than what you think is
reasonable could help get started.&lt;/p&gt;
&lt;p&gt;Let me explain.&lt;/p&gt;
&lt;p&gt;I've been putting off writing on my blog for months. Then one evening I
thought "I know, I'll write 30 blog posts in 30 days". Bear in mind that
previously I'd barely written one every 2-3 months! It was a hugely
ambitious mini-project considering my track record.&lt;/p&gt;
&lt;p&gt;Suddenly it was clear that to make this work I'd have to ensure I have
enough time every day to write a blog post. That's on average an hour a
day I'd have to commit to this.&lt;/p&gt;
&lt;p&gt;Interestingly though, it gave me a focus I never had when I was just
casually contemplating writing my next post. There was no time to second
guess myself because I had no more than an hour a day to get something
done.&lt;/p&gt;
&lt;p&gt;It was &lt;em&gt;almost easy&lt;/em&gt; to sit down and get cracking because there was
literally no time to waste.&lt;/p&gt;
&lt;p&gt;I also made use of my other trick of external pressure by immediately
tweeting about it. Then there was no going back, and I'm already halfway
through a content-creating marathon.&lt;/p&gt;
&lt;p&gt;Thanks for that, Past Me.&lt;/p&gt;
&lt;h3&gt;A Rousing Speech&lt;/h3&gt;
&lt;p&gt;You know how in films there's often a point where a character says a
rousing speech and everyone around them just goes "yeah, let's do
this!"?&lt;/p&gt;
&lt;p&gt;Sometimes you need it.&lt;/p&gt;
&lt;p&gt;In my case, it was this one: &lt;a href="http://www.cracked.com/blog/6-harsh-truths-that-will-make-you-better-person/"&gt;6 Harsh Truths That Will Make You a Better Person&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you ever feel you're struggling with getting stuff done, read that.&lt;/p&gt;
&lt;p&gt;Maybe it will make you go "yeah, let's do stuff!".&lt;/p&gt;
&lt;h3&gt;Be Forgiving&lt;/h3&gt;
&lt;p&gt;Like I said at the end of my previous post, it's important not to take
failure too seriously.&lt;/p&gt;
&lt;p&gt;We don't have to be productive all the time. In fact, we're probably too
obsessed with it. I know I can be.&lt;/p&gt;
&lt;p&gt;If you take it too seriously, procrastination can cause a lot of
distress and productivity becomes a burden.&lt;/p&gt;
&lt;p&gt;So sometimes, choose &lt;strong&gt;not&lt;/strong&gt; to do stuff.&lt;/p&gt;
&lt;p&gt;Just make sure it's a choice you're actively making, not an excuse to
procrastinate.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 16&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category><category term="featured"></category></entry><entry><title>Just Do Stuff</title><link href="/blog/just-do-stuff" rel="alternate"></link><published>2016-11-15T16:44:00+00:00</published><updated>2016-11-15T16:44:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-15:/blog/just-do-stuff</id><summary type="html">&lt;p&gt;Yesterday I read an article called &lt;a href="https://medium.com/&amp;64;bgilham/making-time-for-side-projects-22fbee546b45#.8f9myvkmq"&gt;Making Time for Side Projects&lt;/a&gt;
and its associated &lt;a href="https://news.ycombinator.com/item?id=12949469"&gt;Hacker News discussion&lt;/a&gt;. It got me
thinking about all the advice I've read over the years about
productivity, about how best to use your time, and how to stop
procrastinating over side projects and just get …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday I read an article called &lt;a href="https://medium.com/&amp;64;bgilham/making-time-for-side-projects-22fbee546b45#.8f9myvkmq"&gt;Making Time for Side Projects&lt;/a&gt;
and its associated &lt;a href="https://news.ycombinator.com/item?id=12949469"&gt;Hacker News discussion&lt;/a&gt;. It got me
thinking about all the advice I've read over the years about
productivity, about how best to use your time, and how to stop
procrastinating over side projects and just get stuff done.&lt;/p&gt;
&lt;p&gt;I don't know the answer to how you get stuff done; I'm still just slowly
getting used to spending more time &lt;em&gt;actually doing things&lt;/em&gt;. I've written
&lt;a href="/blog/procrastination-and-monkeys/"&gt;about procrastination&lt;/a&gt;
before, and I'll likely return to this topic every now and again. Partly
so that I can reread it later in case I forget what I've learned.&lt;/p&gt;
&lt;p&gt;Today I'm just going to share the random thoughts I've had after reading
that article. Which brings me onto my first point.&lt;/p&gt;
&lt;h2&gt;Too Much Reading, Not Enough Doing&lt;/h2&gt;
&lt;p&gt;There was one comment in particular from that HN thread that caught my
eye and motivated this post.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;I wonder how many hours that would have otherwise been productive
have been spent reading about productivity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Despite the tongue-in-cheek tone, there's a lot of truth there.&lt;/p&gt;
&lt;p&gt;I've fallen into this trap many times - you read an insightful post,
learn some home truths, get some clarity, and understand your situation
better. You feel like something's clicked and you'll never look at the
problem of procrastination the same way again.&lt;/p&gt;
&lt;p&gt;Then you just go back to &lt;em&gt;not doing stuff&lt;/em&gt;.&lt;/p&gt;
&lt;h2&gt;What "stuff"?&lt;/h2&gt;
&lt;p&gt;At this point I want to clarify something. The problem I'm describing is
related to procrastinating over things &lt;em&gt;no one is making you do&lt;/em&gt;. Maybe
that's why it's hard, because you're not accountable to anyone, but I'm
strictly only talking about working on these side projects.&lt;/p&gt;
&lt;p&gt;You can easily get through life and have a decent career without
spending time outside of work bettering yourself. You'll probably miss
out on opportunities to meet people or learn things, but side projects
are not essential in the way that, say, &lt;em&gt;going to work&lt;/em&gt; is. Again, maybe
that's why it's easy to procrastinate over it.&lt;/p&gt;
&lt;h2&gt;Unsolicited Advice&lt;/h2&gt;
&lt;p&gt;I realise the irony of talking about this problem when I've just said
that reading about it is one of the barriers to getting stuff done. I'm
afraid the only advice that I can offer is, in my view, the only
universally "good" advice on the subject:&lt;/p&gt;
&lt;p&gt;Do stuff.&lt;/p&gt;
&lt;p&gt;It doesn't matter what it is. Write a blog, learn a new framework, mess
around with automating something on your computer, scrape a random
website for data. Anything.&lt;/p&gt;
&lt;p&gt;Overthinking and overcomplicating are the enemies of getting things
done. For bigger projects, obviously make a plan but if you're ever
stuck or feel overwhelmed that your side project is big, or you're not
sure how to continue, just &lt;em&gt;do stuff&lt;/em&gt;. Don't think about how it will fit
into the bigger picture. Don't think about how good a portfolio piece it
will be. Just do stuff.&lt;/p&gt;
&lt;p&gt;I've been following this maxim for about a year now, with what I thought
was almost no success. When I decided to do this &lt;a href="/blog/30-posts-in-30-days/"&gt;30-day blogging challenge&lt;/a&gt; and
had to get a list of ideas together to get started, that's when I
realised how many almost-finished side projects I had. None of them were
very big, but they were all the result of me just sitting down and
&lt;em&gt;doing something&lt;/em&gt;. The structured nature of this challenge and the tight
daily deadline finally brought me enough focus to start finishing them
off by polishing the code and writing up blog posts.&lt;/p&gt;
&lt;p&gt;Suddenly I started producing content.&lt;/p&gt;
&lt;h2&gt;Stop Consuming&lt;/h2&gt;
&lt;p&gt;The bigger picture version of the "too much reading" idea is one of the
harsh truths pointed out in the Making Time for Side Projects article
(and others).&lt;/p&gt;
&lt;p&gt;It is that you should &lt;strong&gt;stop consuming&lt;/strong&gt;, and &lt;strong&gt;start creating&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One of my favourite articles on the subject, which I will talk about in
more depth in a later post, says this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;You hate yourself because you don't do anything.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Harsh, but true. At least in my case it was.&lt;/p&gt;
&lt;h2&gt;Final Musings&lt;/h2&gt;
&lt;p&gt;I realise this all can come across as a bit preachy. Perhaps it is less
so if I point out that I fail at this &lt;em&gt;all the time&lt;/em&gt;. This entire piece
is mostly just aimed at a past version of me.&lt;/p&gt;
&lt;p&gt;What I've come to understand is that yes, these points are all true;
it's obviously better to do things than not do things. But you also have
to understand that you will fail. You will sometimes inevitably spend a
whole Sunday playing video games instead of working on stuff. We're all
human after all.&lt;/p&gt;
&lt;p&gt;If that happens, just forgive yourself and move on. We don't have to be
"productive" all the time (I'll return to that idea).&lt;/p&gt;
&lt;p&gt;If you're halfway through that Sunday and catch yourself
procrastinating, then take some time and &lt;em&gt;do stuff&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Footnote: This is the 15&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category><category term="featured"></category></entry><entry><title>Duck Typing</title><link href="/blog/duck-typing" rel="alternate"></link><published>2016-11-14T16:05:00+00:00</published><updated>2016-11-14T16:05:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-14:/blog/duck-typing</id><summary type="html">&lt;p&gt;In an attempt to bridge the gap between the two disciplines of
programming and data science I will occasionally talk about programming
concepts useful for data scientists, and vice versa.&lt;/p&gt;
&lt;p&gt;Today I want to discuss &lt;strong&gt;duck typing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Duck typing is a concept that originated in the Python community. It is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In an attempt to bridge the gap between the two disciplines of
programming and data science I will occasionally talk about programming
concepts useful for data scientists, and vice versa.&lt;/p&gt;
&lt;p&gt;Today I want to discuss &lt;strong&gt;duck typing&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Duck typing is a concept that originated in the Python community. It is
a way of checking an object's type not by testing its type directly, but
testing its &lt;strong&gt;methods&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The idea is based on something called &lt;a href="https://en.wikipedia.org/wiki/Duck_test"&gt;the duck test&lt;/a&gt;. You've probably heard it
before:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"If it looks like a duck, swims like a duck, and quacks like a duck,
then it probably is a duck."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How does this relate to programming?&lt;/p&gt;
&lt;p&gt;Well, Python is a dynamically-typed language. That means that the types
of objects (whether they're integers, strings etc.) is checked at
&lt;strong&gt;runtime, not compile time&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;A variable is allowed to have different types at different points of a
program's execution. This is perfectly valid:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;my_variable&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;42&lt;/span&gt;
&lt;span class="c1"&gt;# do stuff with my_variable...&lt;/span&gt;
&lt;span class="c1"&gt;# and later...&lt;/span&gt;
&lt;span class="n"&gt;my_variable&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;now it&amp;#39;s a string!&amp;quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;You can't do this in a statically-typed language. Once you declare a
variable as a certain type, it stays that way. Take this example in C#:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;my_variable&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="m"&gt;42&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// explicitly declare an integer&lt;/span&gt;
&lt;span class="c1"&gt;// do stuff&lt;/span&gt;
&lt;span class="n"&gt;my_variable&lt;/span&gt; &lt;span class="p"&gt;=&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;can I be a string?&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// this will produce a compiler error&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Actually there are &lt;a href="https://msdn.microsoft.com/en-us/library/dd264736.aspx"&gt;dynamic types in C#&lt;/a&gt; but we'll just gloss over that.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The point is you can be quite liberal with types in Python.&lt;/p&gt;
&lt;p&gt;You can take this one step further with duck typing.&lt;/p&gt;
&lt;h2&gt;Duck Typing in Python&lt;/h2&gt;
&lt;p&gt;Say you have a function that makes a duck quack, like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;make_it_quack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;something_duck_like&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;something_duck_like&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;quack&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We've taken an object in and called its quack method. We don't care what
type of object this is, only that it is able to quack. So if we had a
"real" duck and an impostor, they'd both work with this method:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Duck&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;quack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Quack quack&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;Ferret&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;object&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# ferrets can&amp;#39;t normally quack, but this one&amp;#39;s cunning&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;quack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Quack quack&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;donald&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Duck&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;fred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Ferret&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;make_it_quack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;donald&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;make_it_quack&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;fred&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Both of these will produce the output "Quack quack" because all we did
was make it quack. If it can do that, then as far as we're concerned
it's a duck.&lt;/p&gt;
&lt;h2&gt;Duck Typing in Data Science&lt;/h2&gt;
&lt;p&gt;This is a concept that can be quite useful in data science.&lt;/p&gt;
&lt;p&gt;For example, imagine that you have your own implementation of a machine
learning algorithm but want to use a lot of the goodness built in to
scikit-learn.&lt;/p&gt;
&lt;p&gt;Well, you know how all scikit-learn implementations have a fit and
predict function?&lt;/p&gt;
&lt;p&gt;You can create your own object and make use of duck typing by "quacking
like a scikit-learn duck".&lt;/p&gt;
&lt;p&gt;First, create a class that has fit and predict methods:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;class&lt;/span&gt; &lt;span class="nc"&gt;MyFakeClassifier&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Working VERY HARD...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    
    &lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# predict 0 no matter what&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;item&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;))]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now you can fit and predict the same way as you could with, say, a
Random Forest.&lt;/p&gt;
&lt;p&gt;Imagine you already wrote a small function that takes in a machine
learning classifier, does a train-test split and gets the accuracy and
confusion matrix of the predictions.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.model_selection&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.metrics&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;

&lt;span class="c1"&gt;# write a function to give us a train-test accuracy score&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_test&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;train_test_split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;test_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stratify&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_train&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_train&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;y_pred&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X_test&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Accuracy: &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;accuracy_score&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                    &lt;span class="n"&gt;confusion_matrix&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;y_test&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y_pred&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We can use this for a built-in classifier, but also our new estimator:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.datasets&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;sklearn.ensemble&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;

&lt;span class="n"&gt;iris&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;load_iris&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;X&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;
&lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;iris&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;target&lt;/span&gt;

&lt;span class="n"&gt;rf&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;RandomForestClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;random_model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;MyFakeClassifier&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="c1"&gt;# we can get the accuracy of our Random Forest&lt;/span&gt;
&lt;span class="n"&gt;get_accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rf&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# and our new model!&lt;/span&gt;
&lt;span class="n"&gt;get_accuracy&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;random_model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;There you have it. All we had to do was create something that can "fit"
and "predict" and Python doesn't need anything else for it to work.&lt;/p&gt;
&lt;p&gt;Note: to use the full range of scikit-learn functions with your own
estimator, you should &lt;a href="http://scikit-learn.org/stable/developers/contributing.html#rolling-your-own-estimator"&gt;do it properly&lt;/a&gt;,
but the point is you can do a lot of it by duck typing.&lt;/p&gt;
&lt;p&gt;Here's &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/duck-typing/Duck%20Typing.ipynb"&gt;the associated notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Happy quacking!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 14&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="programming for data scientists"></category><category term="python"></category></entry><entry><title>How to Connect to Google Sheets in Python</title><link href="/blog/connecting-to-google-sheets-in-python" rel="alternate"></link><published>2016-11-13T13:54:00+00:00</published><updated>2016-11-13T13:54:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-13:/blog/connecting-to-google-sheets-in-python</id><summary type="html">&lt;p&gt;In most data science and machine learning tutorials you typically
encounter csv files. Either you connect to them locally, something like
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_local_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or you access them via a direct url like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd …&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</summary><content type="html">&lt;p&gt;In most data science and machine learning tutorials you typically
encounter csv files. Either you connect to them locally, something like
this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_local_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Or you access them via a direct url like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;pandas&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;http://www.lotsofdata.com/hosted_data.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;What I rarely see though is connecting to slightly more obscure data
sources. You will probably end up doing this once you go out into the
real world of data science.&lt;/p&gt;
&lt;p&gt;One useful data source is Google Sheets. If you have a spreadsheet
hosted on Google Drive, which is made available for public access, and
want to access it, it's not immediately clear how to do that.&lt;/p&gt;
&lt;p&gt;Let's go through an example of how to connect to one. I'll use a
spreadsheet that has the &lt;a href="https://docs.google.com/spreadsheets/d/17Mr201gfDoOTe5ONLS6LYJi1wQbtT26srXeSwUjMK0A/htmlview?usp=sharing&amp;amp;sle=true"&gt;Hacker News salary survey results&lt;/a&gt;
from a couple of years ago.&lt;/p&gt;
&lt;p&gt;You can't use the url directly, because the url isn't just pointing to
the data, it's pointing to the entire Google Sheets interface.&lt;/p&gt;
&lt;p&gt;Instead you need the sheet's export link.&lt;/p&gt;
&lt;p&gt;To do this simply take the url until the /d/ part, and the unique ID
that comes after, so this much:&lt;/p&gt;
&lt;p&gt;https://docs.google.com/spreadsheets/d/17Mr201gfDoOTe5ONLS6LYJi1wQbtT26srXeSwUjMK0A&lt;/p&gt;
&lt;p&gt;and add &lt;strong&gt;/export&lt;/strong&gt; at the end with some parameters.&lt;/p&gt;
&lt;p&gt;You can specify the sheet number (zero-indexed) using &lt;strong&gt;gid&lt;/strong&gt;, and the
format to be csv using &lt;strong&gt;format&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The full url then becomes:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://docs.google.com/spreadsheets/d/17Mr201gfDoOTe5ONLS6LYJi1wQbtT26srXeSwUjMK0A/export?gid=0&amp;amp;format=csv"&gt;https://docs.google.com/spreadsheets/d/17Mr201gfDoOTe5ONLS6LYJi1wQbtT26srXeSwUjMK0A/&lt;strong&gt;export?gid=0&amp;amp;format=csv&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Try that in your browser and it will download the csv file directly.&lt;/p&gt;
&lt;p&gt;You can then read it into pandas and it will be treated as a regular csv
file.&lt;/p&gt;
&lt;p&gt;Here is the &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/connecting-to-google-sheets/Connecting%20to%20a%20Google%20Sheet.ipynb"&gt;associated Jupyter notebook&lt;/a&gt;
to see it all in action.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 13th entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Markov Chains for Text Generation</title><link href="/blog/markov-chains-for-text-generation" rel="alternate"></link><published>2016-11-12T22:09:00+00:00</published><updated>2016-11-12T22:09:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-12:/blog/markov-chains-for-text-generation</id><summary type="html">&lt;p&gt;Markov chains are a popular way to model sequential data. They form the
basis of more complex ideas, such as Hidden Markov Models, which are
used for speech recognition and have applications in bioinformatics.&lt;/p&gt;
&lt;p&gt;Today I want to run through an implementation of Markov chains for
generating text based on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Markov chains are a popular way to model sequential data. They form the
basis of more complex ideas, such as Hidden Markov Models, which are
used for speech recognition and have applications in bioinformatics.&lt;/p&gt;
&lt;p&gt;Today I want to run through an implementation of Markov chains for
generating text based on an existing corpus. First of course we need to
understand what Markov chains are before we can implement one.&lt;/p&gt;
&lt;h1&gt;The Intuition&lt;/h1&gt;
&lt;p&gt;I've talked about &lt;a href="/blog/intuition-first-machine-learning/"&gt;intuition-first machine learning&lt;/a&gt;
before, so I'll start with the intuition. The first thing we need to
know about is what a Markov chain consists of, then we need to define
the Markov assumption.&lt;/p&gt;
&lt;h2&gt;States and Transitions&lt;/h2&gt;
&lt;p&gt;In a Markov chain we are assuming our sequence is made up of &lt;strong&gt;discrete states&lt;/strong&gt;. That is, every item in the sequence is one of a finite set of
possible values. In text, the states could be every letter of the
alphabet and our punctuation marks.&lt;/p&gt;
&lt;p&gt;We assume that for every item in the sequence, there is a probability
associated with what the next value will be or, more formally, what
state we will transition to. This is called the &lt;strong&gt;transition probability&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;This is different between pairs of states, so the probability of going
from state A to state B might be different than going from B to A. There
is also a probability of staying in the same state.&lt;/p&gt;
&lt;p&gt;However, it would be too difficult to attempt to model the transition
probabilities if we always used the entire sequence. This would be like
trying to predict the last word in War and Peace and having to take into
account every single word that came before it.&lt;/p&gt;
&lt;p&gt;To make this easier, we make what's called the Markov assumption.&lt;/p&gt;
&lt;h2&gt;The Markov Assumption&lt;/h2&gt;
&lt;p&gt;The Markov assumption is when we assume the Markov property to be true.&lt;/p&gt;
&lt;p&gt;The Markov property (of a sequence) is typically formulated like so:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The future is independent of the past, given the present.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;That sounds a bit like an old proverb but it's a simple concept.&lt;/p&gt;
&lt;p&gt;If a sequence has the Markov property it means that the value of the
sequence at the next step &lt;strong&gt;depends only on the value at the past &lt;span class="math"&gt;\(n\)&lt;/span&gt; steps&lt;/strong&gt;. The value of &lt;span class="math"&gt;\(n\)&lt;/span&gt; (how far back we assume we need to go) is
called the &lt;strong&gt;order&lt;/strong&gt; of a Markov chain.&lt;/p&gt;
&lt;p&gt;So a second-order Markov chain is one where we use the current and the
previous value to predict the next value. We are assuming that it does
not matter what the rest of the text was before the previous word, we
only need the current word and the one before it.&lt;/p&gt;
&lt;p&gt;In effect, the Markov chain has no "memory" beyond the last &lt;span class="math"&gt;\(n\)&lt;/span&gt; steps.&lt;/p&gt;
&lt;p&gt;The Markov assumption simply states that we believe this Markov property
to hold for a given sequence.&lt;/p&gt;
&lt;p&gt;This is a simplifying assumption that means it is much easier to compute
these Markov chains at the cost of some complexity and accuracy. However
simple this assumption may feel, it performs remarkably well.&lt;/p&gt;
&lt;h2&gt;The Markov Assumption in Text&lt;/h2&gt;
&lt;p&gt;Let's look at an example. What does this Markov assumption look like for
text? We'll work with second-order Markov chains, as defined above.&lt;/p&gt;
&lt;p&gt;Let's imagine that our sequence is this string of animals:&lt;/p&gt;
&lt;p&gt;cat cat cat dog cat cat cat fish cat dog fish cat dog dog cat&lt;/p&gt;
&lt;p&gt;What we say if we assume the Markov property is that our prediction of
the next word in the sequence depends only on the last two words: dog
cat.&lt;/p&gt;
&lt;p&gt;How do we use that for predicting?&lt;/p&gt;
&lt;p&gt;We need to calculate the transition probabilities based on the text that
we have, then assume that going forward those probabilities are what
decide our next words.&lt;/p&gt;
&lt;p&gt;Because we are using second-order Markov chains, we calculate the
probabilities of each word following a two-word pair. To do this, we
simply count what percentage of the time a word appeared after each
possible pair.&lt;/p&gt;
&lt;p&gt;If we take a single example, "cat cat", we'll see that this pair was
followed by "cat" twice, "dog" once and "fish" also once. That means if
we're in the state "cat cat" the next word will be "cat" with a 50%
probability, or "dog" or "fish" with 25% probability each.&lt;/p&gt;
&lt;h1&gt;The Code&lt;/h1&gt;
&lt;p&gt;That's enough intuition, let's get into the code.&lt;/p&gt;
&lt;p&gt;I've scraped the lyrics to all songs written by &lt;a href="http://muse.mu/"&gt;Muse&lt;/a&gt;
and we'll use this as our corpus. We'll use it to generate some more
text, specifically a new Muse song.&lt;/p&gt;
&lt;h2&gt;Preparation&lt;/h2&gt;
&lt;p&gt;The first step is to read the file in and clean it so we have a sequence
of words and new lines. I won't go into the details, you can see how the
cleaning is done in the accompanying Jupyter notebook. I also included
the code I wrote to scrape the lyrics in the first place, but going
through that is optional as I'll also include the final source file.&lt;/p&gt;
&lt;h2&gt;Training&lt;/h2&gt;
&lt;p&gt;When we do the "learning" for a Markov chain, we're just identifying all
unique triplets of words, and creating our transition probabilities
using the first two words as the current state and the third as the next
state.&lt;/p&gt;
&lt;p&gt;As an implementation detail, I've made the transition probabilities a
Python dictionary where each key is a unique word pair, and the value is
a list of all the words that have ever followed that pair. I haven't
even explicitly calculated the probabilities, but instead included each
word as many times as it appears.&lt;/p&gt;
&lt;p&gt;Our "cat cat" example above would look like an entry in a Python
dictionary like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cat cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;cat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;dog&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;fish&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;To generate our transition probabilities we use a helper function:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;generate_triples&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;d&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
    &lt;span class="c1"&gt;# loop through the text and generate triples&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;continue&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
                &lt;span class="n"&gt;d&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;word_list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;d&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So given a list of words (our entire text) we go through it word by
word, creating dictionary entries for every pair we encounter, and
adding the next word into the list associated with that pair.&lt;/p&gt;
&lt;p&gt;To then simulate the probability of choosing a word given a pair of
words, we randomly sample from the list associated with that pair.&lt;/p&gt;
&lt;p&gt;This function gives us the next word each time. This is part of a Markov
class, where self.words is our previously trained dictionary.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_random_word&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phrase&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;phrase&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# find a phrase from the list of words associated with the last two words in the supplied phrase&lt;/span&gt;
        &lt;span class="n"&gt;phrase_words&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lower&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;phrase&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;split&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phrase_words&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;phrase_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;phrase_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;past&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[(&lt;/span&gt;&lt;span class="n"&gt;phrase_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;phrase_words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])]&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;past&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()))]&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;past&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()))]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# no phrase supplied, return a word from our dict at random&lt;/span&gt;
        &lt;span class="n"&gt;past&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;words&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;keys&lt;/span&gt;&lt;span class="p"&gt;()))]&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;choice&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;past&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The function will look at a phrase, check if it is at least two words
long and for the last two words, it finds the appropriate dictionary
entry and samples from the list associated with that pair.&lt;/p&gt;
&lt;p&gt;So if we gave it "banana cat cat" it would look up "cat cat" in the
dictionary and sample from the list.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Remember&lt;/strong&gt;: the words in the list implicitly represent the transition
probabilities. Because we have "cat" twice out of four words in our list
we've defined the transition probability as 50%.&lt;/p&gt;
&lt;p&gt;To generate the text we can then either give it some text to start it
off, or let it start with a random pair.&lt;/p&gt;
&lt;p&gt;I've added some structure so that the code creates a song title, a
chorus and a couple of verses.&lt;/p&gt;
&lt;p&gt;It came up with this:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;hear me moan&lt;/p&gt;
&lt;p&gt;Verse 1&lt;/p&gt;
&lt;p&gt;you are just&lt;br&gt;
too much attention&lt;br&gt;
and its gonna be&lt;br&gt;
show me mercy can someone rescue me&lt;br&gt;
make me agitated&lt;/p&gt;
&lt;p&gt;Chorus&lt;/p&gt;
&lt;p&gt;escaped your world&lt;br&gt;
no one is crying alone&lt;br&gt;
i wont let them hurt&lt;br&gt;
hurting you no&lt;/p&gt;
&lt;p&gt;Verse 2&lt;/p&gt;
&lt;p&gt;you know what youve done&lt;br&gt;
bring me peace and wash away my dirt&lt;br&gt;
spin me round and have me to&lt;/p&gt;
&lt;p&gt;Chorus&lt;/p&gt;
&lt;p&gt;escaped your world&lt;br&gt;
no one is crying alone&lt;br&gt;
i wont let them hurt&lt;br&gt;
hurting you no&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I dare say Matt Bellamy couldn't have written it better himself.&lt;/p&gt;
&lt;p&gt;Here's the &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/markov-chain-muse-lyrics/Markov%20Chain%20Muse%20Lyrics.ipynb"&gt;associated Jupyter notebook&lt;/a&gt;.
I've also previously published the code as more of a plug and play
library, which &lt;a href="https://github.com/davidasboth/markov-chain-for-text"&gt;you can find here&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Next Steps&lt;/h2&gt;
&lt;p&gt;We could improve this code by doing any of the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Experiment with different orders for the Markov chain&lt;/li&gt;
&lt;li&gt;Giving it more data is never a bad idea, you could mash Muse's
    lyrics up with another artist&lt;/li&gt;
&lt;li&gt;You could try a character-level Markov chain to see if it would
    generate meaningful text that way, although for that you might want
    a &lt;a href="https://github.com/karpathy/char-rnn"&gt;Recurrent Neural Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Adding support for punctuation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;Further Reading&lt;/h1&gt;
&lt;p&gt;If you want to learn more about Markov chains, and even see them 'in
action', this is &lt;a href="http://setosa.io/ev/markov-chains/"&gt;a great resource&lt;/a&gt;
to start with.&lt;/p&gt;
&lt;p&gt;I glossed over a lot of the maths behind the algorithm, but if you're
interested you could &lt;a href="https://www.youtube.com/watch?v=WUjt98HcHlk"&gt;start here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Footnote: This is the 12&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Why You Should Reinvent the Machine Learning Wheel</title><link href="/blog/why-you-should-reinvent-the-machine-learning-wheel" rel="alternate"></link><published>2016-11-11T17:17:00+00:00</published><updated>2016-11-11T17:17:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-11:/blog/why-you-should-reinvent-the-machine-learning-wheel</id><summary type="html">&lt;p&gt;When you're a data scientist, unless you're in a job that's very
research-focused (and likely requires a PhD) you'll mostly be using
machine learning algorithms invented anywhere between say 5 and 30 years
ago. Similar to being a programmer, you'll be using many libraries made
by other people based on …&lt;/p&gt;</summary><content type="html">&lt;p&gt;When you're a data scientist, unless you're in a job that's very
research-focused (and likely requires a PhD) you'll mostly be using
machine learning algorithms invented anywhere between say 5 and 30 years
ago. Similar to being a programmer, you'll be using many libraries made
by other people based on other people's ideas.&lt;/p&gt;
&lt;p&gt;There's nothing wrong with that.&lt;/p&gt;
&lt;p&gt;In fact, it's usually more productive to use someone else's
implementation of an idea. Why reinvent the wheel every time?&lt;/p&gt;
&lt;p&gt;With machine learning, I want to make a case for why it's good to
reinvent the wheel &lt;strong&gt;when you're still learning&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;I want to stress that I still think it's a good idea to use scikit-learn
99% of the time. However, when you're learning about &lt;a href="http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html"&gt;one of the most popular algorithms&lt;/a&gt;
(I've linked to this KDNuggets article before because I think it's a
good overview of the minimum you should know) I think it's worthwhile
trying to implement them yourself.&lt;/p&gt;
&lt;p&gt;So why go through the effort of implementing existing algorithms again?&lt;/p&gt;
&lt;h3&gt;Programming Practice&lt;/h3&gt;
&lt;p&gt;This is an obvious benefit of trying to implement anything: you get
programming experience. Many data scientists don't come from a
programming background, so a crucial part of the learning process is
feeling at home when writing code. Coupling this with additional machine
learning practice seems like a good way to do this.&lt;/p&gt;
&lt;p&gt;Also, implementing machine learning algorithms is a harder coding
challenge than the ones you'd face if you were doing an introductory
programming course, so this is a good transition from FizzBuzz-type
problems to more meaty challenges.&lt;/p&gt;
&lt;h3&gt;Deeper Understanding&lt;/h3&gt;
&lt;p&gt;I'd argue this is the most important benefit. It's one thing to
conceptually understand an algorithm and it's another to understand it
in enough depth to tell a computer how to do it. Computers don't deal
with ambiguity so you need to understand every little implementation
detail to get to the end. That can only be helpful in deepening your
understanding. Once you move on and use in-built libraries you can also
more easily debug any strange behaviour because you'll know more about
what is happening under the hood.&lt;/p&gt;
&lt;h3&gt;Portfolio Pieces&lt;/h3&gt;
&lt;p&gt;Perhaps not a reason to do this outright, but a good byproduct of these
coding exercises is getting little pieces to add to your GitHub profile.
That's never a bad thing.&lt;/p&gt;
&lt;p&gt;A few things to remember.&lt;/p&gt;
&lt;h3&gt;We are not trying to "beat" other implementations&lt;/h3&gt;
&lt;p&gt;If you're really into optimisation and want to spend time learning how
to make your implementations faster, obviously I wouldn't advise against
it. At the same time it might not be worthwhile spending too much time
improving the performance/scalability of your code if your aim was to
deepen your understanding of machine learning.&lt;/p&gt;
&lt;h3&gt;Don't overdo it and implement everything you read about&lt;/h3&gt;
&lt;p&gt;Again, I'm not suggesting you should actively &lt;strong&gt;not&lt;/strong&gt; implement every
algorithm you hear about, but some algorithms might be harder to
implement than others. Start small to avoid getting frustrated by
complex problems. For example, don't start by implementing a deep
convolutional neural network.&lt;/p&gt;
&lt;p&gt;Then which algorithms should I choose?&lt;/p&gt;
&lt;p&gt;Here are the ones I've gone for so far, because I thought they were easy
enough to implement but I wanted to dive in to the details.&lt;/p&gt;
&lt;h3&gt;Naive Bayes&lt;/h3&gt;
&lt;p&gt;This was one of the activities of the &lt;a href="http://www.becomingadatascientist.com/learningclub/"&gt;Becoming a Data Scientist Learning Club&lt;/a&gt;.
Actually, the activity was to read about and use the algorithm, but I
took this as an opportunity to go through and &lt;a href="https://github.com/davidasboth/data-science-learning-club/tree/master/activity-5-naive-bayes"&gt;implement it from scratch&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Naive Bayes classifier is conceptually quite straightforward and a
good place to start.&lt;/p&gt;
&lt;h3&gt;K-Means Clustering&lt;/h3&gt;
&lt;p&gt;I will spend some time later this month diving into clustering, but for
now it's enough to say that this is also a good choice to start with.
Conceptually simple, but you need to understand the details to be able
to code the whole thing.&lt;/p&gt;
&lt;p&gt;This is &lt;a href="https://github.com/davidasboth/data-science-learning-club/blob/master/activity-6-kmeans/notebooks/K-Means.ipynb"&gt;another algorithm I implemented&lt;/a&gt;
as part of the Learning Club.&lt;/p&gt;
&lt;h3&gt;Self-Organising Maps&lt;/h3&gt;
&lt;p&gt;I
&lt;a href="/blog/self-organising-maps-an-introduction/"&gt;discussed&lt;/a&gt;
this algorithm recently, and hopefully showed that two blog posts are
enough to go through the details enough to actually &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/self-organising-map/Self-Organising%20Map.ipynb"&gt;write the code&lt;/a&gt;
for it. This is perhaps a less mainstream choice but conceptually lends
itself to a good coding exercise.&lt;/p&gt;
&lt;h3&gt;Markov Chains&lt;/h3&gt;
&lt;p&gt;A popular choice for modelling and predicting sequential data (text,
audio, time series). It only requires simple probability theory and is
another good choice to start with.&lt;/p&gt;
&lt;p&gt;Another topic I'll return to later this month. My
&lt;a href="https://github.com/davidasboth/markov-chain-for-text"&gt;implementation&lt;/a&gt;
generates new text based on text you give it.&lt;/p&gt;
&lt;h3&gt;Other Choices&lt;/h3&gt;
&lt;p&gt;Some other algorithms I suggest might be reasonable choices:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/"&gt;Decision Trees&lt;/a&gt;
    (although you might end up needing the help of
    &lt;a href="https://en.wikipedia.org/wiki/Recursion_(computer_science)"&gt;recursion&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://machinelearningmastery.com/k-nearest-neighbors-for-machine-learning/"&gt;K Nearest Neighbours&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Or if you're feeling a bit more brave, try a &lt;a href="http://deeplearning.net/tutorial/mlp.html"&gt;Multilayer Perceptron&lt;/a&gt;. You'll need to
understand and &lt;a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/"&gt;implement backpropagation&lt;/a&gt;,
but it would be a good advanced programming challenge.&lt;/p&gt;
&lt;p&gt;Hopefully I've convinced you that implementing machine algorithms from
scratch is a worthwhile endeavour. I'm always interested in seeing other
people's implementations so let me know if you've done any!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 11&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category></entry><entry><title>Realistic Machine Learning</title><link href="/blog/realistic-machine-learning" rel="alternate"></link><published>2016-11-10T15:41:00+00:00</published><updated>2016-11-10T15:41:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-10:/blog/realistic-machine-learning</id><summary type="html">&lt;p&gt;As I'm sure most data scientists quickly realise, there's a difference
between the kind of data science you do while learning about it, and the
kind you do at a real job.&lt;/p&gt;
&lt;p&gt;This is especially true of university degrees, and isn't unique to data
science. However much you learn during …&lt;/p&gt;</summary><content type="html">&lt;p&gt;As I'm sure most data scientists quickly realise, there's a difference
between the kind of data science you do while learning about it, and the
kind you do at a real job.&lt;/p&gt;
&lt;p&gt;This is especially true of university degrees, and isn't unique to data
science. However much you learn during your degree, reality will always
be different in a way you weren't prepared for. I don't mean this in a
scary way, just as a matter-of-fact appraisal of how things are.&lt;/p&gt;
&lt;p&gt;This is equally true of data cleaning/wrangling and machine learning.&lt;/p&gt;
&lt;p&gt;The "ML Hipster" &lt;a href="https://twitter.com/ML_Hipster/status/633954383542128640"&gt;summed this up very well.&lt;/a&gt;&lt;/p&gt;
&lt;blockquote class="twitter-tweet" data-lang="en"&gt;&lt;p lang="en" dir="ltr"&gt;… and that concludes Machine Learning 101. Now, go forth and apply what you&amp;#39;ve learned to real data! &lt;a href="http://t.co/D6wSKgdjeM"&gt;pic.twitter.com/D6wSKgdjeM&lt;/a&gt;&lt;/p&gt;&amp;mdash; ML Hipster (@ML_Hipster) &lt;a href="https://twitter.com/ML_Hipster/status/633954383542128640?ref_src=twsrc%5Etfw"&gt;August 19, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;

&lt;p&gt;There's also this quote I've seen (source unknown):&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In data science, 80% of time is spent preparing data, and 20% of time
is spent complaining about the need to prepare data&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;What are the main reasons for this disconnect?&lt;/p&gt;
&lt;h3&gt;Real Data is Messy&lt;/h3&gt;
&lt;p&gt;Yes some data science courses cover this and talk about what sorts of
things you'd need to do to clean a dataset.&lt;/p&gt;
&lt;p&gt;However, this usually amounts to converting strings to dates or dealing
with missing values. These are important skills to have, but only make
up 10% of the kind of data cleaning you might encounter.&lt;/p&gt;
&lt;p&gt;Human-generated data is messy because people are not consistent, and
often data entry systems allow multiple ways of entering data or simply
free text.&lt;/p&gt;
&lt;p&gt;Computer/machine-generated data is messy because things can go wrong,
log files from systems can have weird values in them, etc.&lt;/p&gt;
&lt;p&gt;Not only does this have an impact on your data cleaning efforts (by
requiring 10x more work than anticipated) but also machine learning.&lt;/p&gt;
&lt;p&gt;Scikit-learn random forests are not designed to deal with someone
putting "next Tuesday" in a free-type date field.&lt;/p&gt;
&lt;h3&gt;The Question is Unclear&lt;/h3&gt;
&lt;p&gt;An underappreciated quality of a data scientist is the ability to frame
the question.&lt;/p&gt;
&lt;p&gt;Converting a business requirement, which is often nebulous and
unquantifiable, into a machine learning problem is a difficult task.
There's usually more than one way to approach it, but it boils down to
making a human problem into a concrete, measurable task for an
algorithm.&lt;/p&gt;
&lt;p&gt;This means a shift from your studies where you're given the question. No
longer are you identifying types of iris - you need to start with
identifying what to identify!&lt;/p&gt;
&lt;h3&gt;A Difference of Purpose&lt;/h3&gt;
&lt;p&gt;This is the big one. The moment you're doing data science for a
business, your aims are different.&lt;/p&gt;
&lt;p&gt;Previously you were solving problems for the sake of it. Now, the focus
is on &lt;strong&gt;adding value&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That means that at any point in a project the next step will depend on
what makes sense for the business, not what is academically most
interesting. The kind of work you do will by definition be different to
what you're used to.&lt;/p&gt;
&lt;h1&gt;How can we better prepare?&lt;/h1&gt;
&lt;p&gt;The obvious way to smooth the transition between academia (or "learning"
in general) and industry is to teach more realistic problems.&lt;/p&gt;
&lt;p&gt;That's not to say you should &lt;strong&gt;start&lt;/strong&gt; with solving a business problem,
but you should certainly &lt;strong&gt;end&lt;/strong&gt; on one.&lt;/p&gt;
&lt;p&gt;As &lt;a href="/blog/5-data-science-book-recommendations/"&gt;I've already mentioned&lt;/a&gt;,
the book Data Science from Scratch gives realistic-sounding problems
when introducing various data science concepts.&lt;/p&gt;
&lt;p&gt;I think we need more of that.&lt;/p&gt;
&lt;p&gt;A more rounded way of teaching data science would be to start with toy
problems and gradually build up to more complex ones. Something like
this scale of four difficulties:&lt;/p&gt;
&lt;h4&gt;Difficulty 1&lt;/h4&gt;
&lt;p&gt;You're given an "academic" dataset with perhaps some cleaning to do
(date formats, missing values). There is an explicit target value so
once the cleaning is done you know exactly what to do for machine
learning (e.g. predict the type of iris).&lt;/p&gt;
&lt;h4&gt;Difficulty 2&lt;/h4&gt;
&lt;p&gt;You're given a "real" dataset obtained from some real system or somehow
produced by real humans. There is not just some cleaning to do, but
questions of interpretation, data transformations, maybe even some
feature engineering. The machine learning task is still clear though.&lt;/p&gt;
&lt;h4&gt;Difficulty 3&lt;/h4&gt;
&lt;p&gt;A real dataset but with a less well-defined machine learning task. This
could be an unsupervised problem, where part of the work is in
interpreting the outcomes, or it could just be unclear whether you want
a classification or a regression approach. For example, a dataset with
sales figures where it's unclear if you should predict sales to the
nearest unit or just which group the sales belong to (in terms of size).&lt;/p&gt;
&lt;h4&gt;Difficulty 4&lt;/h4&gt;
&lt;p&gt;The "real world problem". You're given a broad task to solve but no
data, or a dataset that clearly needs enhancing somehow. This task would
involve scraping or just trawling the net for open data, then defining
your features and deciding on how best to frame it as a machine learning
problem.&lt;/p&gt;
&lt;p&gt;For the last type of task to work though, I'd argue the teaching has to
be interactive. It's much easier to evaluate your approach by getting
qualitative feedback, unlike the first 2-3 tasks where you can
objectively measure your success.&lt;/p&gt;
&lt;p&gt;This isn't always a practical way to teach, but it would go a long way
in preparing people for the reality of data science in the wild.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 10&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category></entry><entry><title>Was FiveThirtyEight Just Wrong?</title><link href="/blog/was-fivethirtyeight-just-wrong" rel="alternate"></link><published>2016-11-09T15:44:00+00:00</published><updated>2016-11-09T15:44:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-09:/blog/was-fivethirtyeight-just-wrong</id><summary type="html">&lt;p&gt;I have some brief thoughts about the outcome of the US election from an
entirely data science perspective. It's hard to remain objective and
have a scientific hat on when it comes to political events, but I never
intended this blog to be a place to discuss political opinion. Instead …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I have some brief thoughts about the outcome of the US election from an
entirely data science perspective. It's hard to remain objective and
have a scientific hat on when it comes to political events, but I never
intended this blog to be a place to discuss political opinion. Instead,
I want to look at this election outcome as an opportunity to talk about
probabilistic forecasts.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://fivethirtyeight.com/"&gt;FiveThirtyEight&lt;/a&gt; tracked many polls over
time to forecast the probability of the two candidates. These fluctuated
quite a bit, but in the end their final forecast was an over 70%
probability of a Clinton win.&lt;/p&gt;
&lt;p&gt;So was their model &lt;em&gt;wrong&lt;/em&gt;?&lt;/p&gt;
&lt;p&gt;Funnily enough it was Nate Silver himself who, in his book, talked about
how we evaluate a probabilistic forecast, his example being the weather.
In the case of the weather, this is the question:&lt;/p&gt;
&lt;p&gt;Someone makes a forecast that tomorrow it will rain with a probability
of 30%. It rains tomorrow. Was the forecast correct?&lt;/p&gt;
&lt;p&gt;The key idea here is that in instances like this the standard notion of
"accurate" doesn't hold. You simply can't evaluate the model based on a
single data point, unless its prediction was a 100% probability. That's
because the model isn't designed to make a single prediction. In the
case of the weather, forecasts are made multiple times a day, so very
quickly you have a whole set of "predictions" and true outcomes.&lt;/p&gt;
&lt;p&gt;From that you now &lt;strong&gt;can&lt;/strong&gt; evaluate the model.&lt;/p&gt;
&lt;p&gt;When they say the probability of rain is 30% it doesn't just mean that
it's unlikely to rain. It also means that out of all the times they
predict 30% it will end up raining 30% of the time; to evaluate it
therefore requires multiple similar predictions. For example, after a
hundred 30% predictions, if it only rained on two occasions it's obvious
the model was too pessimistic (assuming you treat rain as a negative
outcome).&lt;/p&gt;
&lt;p&gt;Then what about an event as rare as a general election?&lt;/p&gt;
&lt;p&gt;That's a harder question and one where &lt;a href="https://twitter.com/nntaleb/status/762033443932934144"&gt;there are disagreements&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Clinton lost the electoral vote but appears to have won the popular vote
- an outcome FiveThirtyEight estimated to be around 10% likely. It was
explicitly covered by the forecast, which simply said that it's a rare
event, but still not an implausible one. Again, it is hard to quantify
whether that 10% was correct or not.&lt;/p&gt;
&lt;p&gt;Ultimately in cases like this there are too many variables to be able to
make a definitive prediction. It is more worthwhile to think of it as a
statement of the probability distribution of all possible outcomes
rather than a means to actually predict who will be President. Of course
this probability distribution can be wrong, it's just not apparent how
to evaluate this.&lt;/p&gt;
&lt;p&gt;I must admit I don't know that much about the technical details of such
an evaluation.&lt;/p&gt;
&lt;p&gt;However, I am interested in finding out, so I will go away and do some
research and report back later this month in a future blog article.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 9&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category></entry><entry><title>5 Data Science Book Recommendations</title><link href="/blog/5-data-science-book-recommendations" rel="alternate"></link><published>2016-11-08T14:58:00+00:00</published><updated>2016-11-08T14:58:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-08:/blog/5-data-science-book-recommendations</id><summary type="html">&lt;p&gt;Data science is a hugely growing field.&lt;/p&gt;
&lt;p&gt;That means there is an incredible amount of material out there about it,
from non-technical guides for the casually interested, all the way to
in-depth technical books and articles. There is also lots of "aggregate
content" - collections of books and articles to help …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Data science is a hugely growing field.&lt;/p&gt;
&lt;p&gt;That means there is an incredible amount of material out there about it,
from non-technical guides for the casually interested, all the way to
in-depth technical books and articles. There is also lots of "aggregate
content" - collections of books and articles to help you find the right
handful of resources, rather than be overwhelmed by all that's out
there.&lt;/p&gt;
&lt;p&gt;In today's post I thought I'd add my own recommendations to the ether.
These are strictly books, because I find it's nice to do some learning
away from the screen in book form every now and again. I've tried to go
for a mix of technical and non-technical, so there's something for
everyone.&lt;/p&gt;
&lt;p&gt;These aren't in any particular order, so it's not a Top 5, just a... 5.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.nytimes.com/2013/06/11/books/big-data-by-viktor-mayer-schonberger-and-kenneth-cukier.html"&gt;Big Data - Viktor Mayer-Schönberger &amp;amp; Kenneth Cukier&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I have to say I was skeptical about reading a book simply entitled "Big Data" because of &lt;a href="http://dilbert.com/strip/2012-07-29"&gt;the meaningless hype&lt;/a&gt; that comes with that phrase.
I was pleasantly surprised of course, otherwise the book wouldn't be on
the list. This is the least technical and complex book on the list, and
it is aimed at a much wider audience. The authors highlight the power
that enormous datasets possess, as well as covering difficult issues
such as data privacy.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://en.wikipedia.org/wiki/The_Signal_and_the_Noise"&gt;The Signal and the Noise - Nate Silver&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;It might feel "too easy" to include this one, but it's also too good to
leave out. Nate Silver and his prediction skills are very topical what
with the US election happening as we speak. His website
&lt;a href="http://projects.fivethirtyeight.com/2016-election-forecast/"&gt;FiveThirtyEight&lt;/a&gt;
is arguably the number one source for election forecasts.&lt;/p&gt;
&lt;p&gt;I like the breadth of topics covered in this book. Silver tackles the
difficulties, failures, and successes of forecasting the weather,
baseball, earthquakes and the economy. The book is very dense with
information, so it's definitely not a light read, but it's very much
recommended for anyone with more than a passing interest in forecasting
with data.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.john-foreman.com/data-smart-book.html"&gt;Data Smart - John Foreman&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Not a book I ever expected to see, but this one teaches you data science
fundamentals &lt;strong&gt;in Excel&lt;/strong&gt;. This
actually makes a lot of sense, because that way you can focus on the
concepts rather than the shiny tools that are available. The assumption
is that everyone knows how to use Excel (although it turns out there was
a lot I didn't know!) so the barrier for entry is near zero.&lt;/p&gt;
&lt;p&gt;I like a well-written book, even if it's very technical, and this one
ticks that box too.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://shop.oreilly.com/product/0636920033400.do"&gt;Data Science from Scratch - Joel Grus&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;This was the first hands-on data science book I bought and I got lucky
because it's a great way to start. It assumes no previous knowledge of
data science or programming, although even a small previous background
in Python will be helpful in getting off the ground. It teaches data
science concepts from first principles, with Python implementations that
avoid the use of in-built libraries such as scikit-learn.&lt;/p&gt;
&lt;p&gt;As I mentioned &lt;a href="/blog/intuition-first-machine-learning"&gt;in a previous post&lt;/a&gt;,
I'm a big fan of learning intuitions first and details later. This book
does exactly that; it starts certain chapters with a real world data
science task and then walks you through the solution. Once you've solved
the problem using raw Python, Grus suggests the appropriate libraries to
use in the real world.&lt;/p&gt;
&lt;p&gt;On top of that, a large chunk of the book deals with the things you
should know &lt;strong&gt;before&lt;/strong&gt; diving into data science, such as statistics and
probability.&lt;/p&gt;
&lt;p&gt;An all round valuable resource, and an enjoyable read.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://sebastianraschka.com/books.html"&gt;Python Machine Learning - Sebastian Raschka&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The title speaks for itself. This one is a technical dive into machine
learning using Python. Concepts are all explained very well by an author
whose website is &lt;a href="http://sebastianraschka.com/faq/index.html"&gt;a treasure trove&lt;/a&gt; of information about
machine learning and data science.&lt;/p&gt;
&lt;p&gt;It's always nice to see a book go beyond describing the algorithms -
there's additional material around deploying your machine learning
solutions to the web.&lt;/p&gt;
&lt;p&gt;Python Machine Learning will remain a reference guide for me for a long
time.&lt;/p&gt;
&lt;p&gt;There's my two cents and I'm always on the look out for more
recommendations, so send any my way! &lt;/p&gt;
&lt;p&gt;Footnote: This was the 8&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category><category term="featured"></category></entry><entry><title>"Intuition First" Machine Learning</title><link href="/blog/intuition-first-machine-learning" rel="alternate"></link><published>2016-11-07T09:34:00+00:00</published><updated>2016-11-07T09:34:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-07:/blog/intuition-first-machine-learning</id><summary type="html">&lt;p&gt;I read &lt;a href="http://machinelearningmastery.com/4-steps-to-get-started-in-machine-learning/"&gt;an article on Machine Learning Mastery&lt;/a&gt;
recently about getting started with machine learning.&lt;/p&gt;
&lt;p&gt;The reason it stuck in my mind was that it discussed a way of learning
that I always felt was closer to me and a better way for me to learn,
yet I hadn't encountered …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I read &lt;a href="http://machinelearningmastery.com/4-steps-to-get-started-in-machine-learning/"&gt;an article on Machine Learning Mastery&lt;/a&gt;
recently about getting started with machine learning.&lt;/p&gt;
&lt;p&gt;The reason it stuck in my mind was that it discussed a way of learning
that I always felt was closer to me and a better way for me to learn,
yet I hadn't encountered many resources that teach machine learning that
way.&lt;/p&gt;
&lt;p&gt;The article calls this approach "top down". I refer to the same approach
as "intuition first".&lt;/p&gt;
&lt;p&gt;The problem is that most books and online courses, while technically
aimed at people with no background in machine learning, are usually too
maths-heavy too soon. Andrew Ng's excellent &lt;a href="https://www.coursera.org/learn/machine-learning"&gt;Coursera machine learning course&lt;/a&gt; does a good job
of introducing the intuition behind the algorithms without getting into
too much detail about the underlying mathematics. However, especially in
the chapter about support vector machines, I felt there was still a need
to step away from the technical details a bit more.&lt;/p&gt;
&lt;p&gt;There is a time and a place for deep dives into algorithms and maths,
but it's not at the start of your learning.&lt;/p&gt;
&lt;p&gt;I know from experience that if I want to learn a new concept, and the
first thing I see is equations I'll be discouraged, because there's too
much to understand too soon. By teaching the ideas in an abstract way
first, you get a "feel" for the concept before you get into the
nitty-gritty implementation details.&lt;/p&gt;
&lt;p&gt;Here's the idea of top down machine learning, quoted straight from the
article:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We can summarize this top-down approach as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Learn the high-level process of applied machine learning.&lt;/li&gt;
&lt;li&gt;Learn how to use a tool enough to be able to work through
    problems.&lt;/li&gt;
&lt;li&gt;Practice on datasets, a lot.&lt;/li&gt;
&lt;li&gt;Transition into the details and theory of machine learning
    algorithms.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;I wholeheartedly agree with step 1 being the high-level process. You
want to know exactly what you're trying to achieve before you need to
formalise it mathematically.&lt;/p&gt;
&lt;p&gt;The part I don't agree with is the order of steps 3 and 4. There is a
slight danger in doing things that way around.&lt;/p&gt;
&lt;p&gt;If you learn the intuition and start practising on datasets without
knowing some more details you can get derailed, and it might be
difficult to "debug" any problems you encounter. As with any new
technique, superficial knowledge of it is fine at the start, but you
need to understand it in more depth if you want to actually use it on a
real problem.&lt;/p&gt;
&lt;p&gt;This is what I propose instead, for an amended version of "top down"
machine learning.&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Learn the high-level process of applied machine learning.&lt;/li&gt;
&lt;li&gt;Try some toy examples, get your hands dirty.&lt;/li&gt;
&lt;li&gt;Transition into the details and theory of machine learning
    algorithms.&lt;/li&gt;
&lt;li&gt;Practise on lots of datasets.&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Not a big change, but the key difference is that you should understand
the theory in more detail before you spend lots of time practising.
Trying some toy problems before diving into the theory will help you put
the theoretical concepts into context.&lt;/p&gt;
&lt;p&gt;Getting exposed to more theory is mostly for practical reasons, so you
understand things like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Normalisation&lt;/strong&gt; - does my data need to be normalised? If you
    understand the inner workings of a machine learning algorithm you'll
    intuitively know the answer rather than having to resort to trial
    and error.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Metrics&lt;/strong&gt; - measuring success in machine learning is an important
    topic and you don't want to be using the wrong metrics to judge how
    well your algorithm is performing.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cross-validation&lt;/strong&gt; - this might be covered in the high-level
    process but the idea of training and test sets needs to be drilled
    home before you start getting too deep.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This might not work for everyone, the mathematically-minded could well
prefer an approach where they're given the equations first, but I
suspect they're the minority. If the imbalance is the way I perceive it,
then there does seem to be a disconnect between the way people learn and
the way machine learning is often taught.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 7&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category></entry><entry><title>Self-Organising Maps: In Depth</title><link href="/blog/self-organising-maps-in-depth" rel="alternate"></link><published>2016-11-06T19:44:00+00:00</published><updated>2016-11-06T19:44:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-06:/blog/self-organising-maps-in-depth</id><summary type="html">&lt;p&gt;In &lt;a href="/blog/self-organising-maps-an-introduction/"&gt;Part 1&lt;/a&gt;,
I introduced the concept of Self-Organising Maps (SOMs). Now in Part 2 I
want to step through the process of training and using a SOM - both the
intuition and the Python code. At the end I'll also present a couple of
real life use cases, not just …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In &lt;a href="/blog/self-organising-maps-an-introduction/"&gt;Part 1&lt;/a&gt;,
I introduced the concept of Self-Organising Maps (SOMs). Now in Part 2 I
want to step through the process of training and using a SOM - both the
intuition and the Python code. At the end I'll also present a couple of
real life use cases, not just the toy example we'll use for
implementation.&lt;/p&gt;
&lt;p&gt;The first thing we need is a problem to solve!&lt;/p&gt;
&lt;p&gt;I'll use the colour map as the walkthrough example because it lends
itself very nicely to visualisation.&lt;/p&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;h2&gt;Dataset&lt;/h2&gt;
&lt;p&gt;Our data will be a collection of random colours, so first we'll
artificially create a dataset of 100. Each colour is a 3D vector
representing R, G and B values:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;255&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;That's simply 100 rows of 3D vectors all between the values of 0 and
255.&lt;/p&gt;
&lt;h2&gt;Objective&lt;/h2&gt;
&lt;p&gt;Just to be clear, here's what we're trying to do. We want to take our 3D
colour vectors and map them onto a 2D surface in such a way that similar
colours will end up in the same area of the 2D surface.&lt;/p&gt;
&lt;h2&gt;SOM Parameters&lt;/h2&gt;
&lt;p&gt;Before training a SOM we need to decide on a few parameters.&lt;/p&gt;
&lt;h3&gt;SOM Size&lt;/h3&gt;
&lt;p&gt;First of all, its &lt;strong&gt;dimensionality&lt;/strong&gt;. In theory, a SOM can be any number
of dimensions, but for visualisation purposes it is typically 2D and
that's what I'll be using too.&lt;/p&gt;
&lt;p&gt;We also need to decide the &lt;strong&gt;number of neurons&lt;/strong&gt; in the 2D grid. This is
one of those decisions in machine learning that might as well be black
magic, so we probably need to try a few sizes to get one that feels
right.&lt;/p&gt;
&lt;p&gt;Remember, this is unsupervised learning, meaning whatever answer the
algorithm comes up with will have to be evaluated somewhat subjectively.
It's typical in an unsupervised problem (e.g. k-means clustering) to do
multiple runs and see what works.&lt;/p&gt;
&lt;p&gt;I'll go with a 5 by 5 grid. I guess one rule of thumb should be to use
fewer neurons than you have data points, otherwise they might not
overlap. As we'll see we actually &lt;strong&gt;want&lt;/strong&gt; them to overlap, because
having multiple 3D vectors mapping to the same point in 2D is how we
find similarities between our data points.&lt;/p&gt;
&lt;p&gt;One important aspect of the SOM is that &lt;strong&gt;each of the 2D points on the
grid actually represent a multi-dimensional weight vector&lt;/strong&gt;. Each point
on the SOM has a weight vector associated with it that is the same
number of dimensions as our input data, in this case 3 to match the 3
dimensions of our colours. We'll see why this is important when we go
through the implementation.&lt;/p&gt;
&lt;h3&gt;Learning Parameters&lt;/h3&gt;
&lt;p&gt;Training the SOM is an iterative process - it will get better at its
task with every iteration, so we need a cutoff point. Our problem is
quite small so 2,000 iterations should suffice but in bigger problems
it's quite possible to need over 10,000.&lt;/p&gt;
&lt;p&gt;We also need a &lt;strong&gt;learning rate&lt;/strong&gt;. The learning rate decides by how much
we apply changes to our SOM at each iteration.&lt;/p&gt;
&lt;p&gt;If it's too high, we will keep making drastic changes to the SOM and
might never settle on a solution.&lt;/p&gt;
&lt;p&gt;If it's too low, we'll never get anything done as we will only make very
small changes.&lt;/p&gt;
&lt;p&gt;In practice it is best to start with a larger learning rate and reduce
it slowly over time. This is so that the SOM can start by making big
changes but then settle into a solution after a while.&lt;/p&gt;
&lt;h1&gt;Implementation&lt;/h1&gt;
&lt;p&gt;For the rest of this post I will use 3D to refer to the dimensionality
of the input data (which in reality could be any number of dimensions)
and 2D as the dimensionality of the SOM (which we decide and could also
be any number).&lt;/p&gt;
&lt;h2&gt;Setup&lt;/h2&gt;
&lt;p&gt;To setup the SOM we need to start with the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Decide on and initialise the SOM parameters (as above)&lt;/li&gt;
&lt;li&gt;Setup the grid by creating a 5x5 array of random 3D weight vectors&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the code:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;network_dimensions&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2000&lt;/span&gt;
&lt;span class="n"&gt;init_learning_rate&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;0.01&lt;/span&gt;
&lt;span class="c1"&gt;# establish size variables based on data&lt;/span&gt;
&lt;span class="n"&gt;m&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;n&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# weight matrix (i.e. the SOM) needs to be one m-dimensional vector for each neuron in the SOM&lt;/span&gt;
&lt;span class="n"&gt;net&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;network_dimensions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;network_dimensions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# initial neighbourhood radius&lt;/span&gt;
&lt;span class="n"&gt;init_radius&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;network_dimensions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;network_dimensions&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="c1"&gt;# radius decay parameter&lt;/span&gt;
&lt;span class="n"&gt;time_constant&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_radius&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Those last two parameters relate to the 2D neighbourhood of each neuron
in the SOM during training. We'll return to those in the learning phase.
Like the learning rate, the initial 2D radius will encompass most of the
SOM and will gradually decrease as the number of iterations increases.&lt;/p&gt;
&lt;h3&gt;Normalisation&lt;/h3&gt;
&lt;p&gt;Another detail to discuss at this point is whether or not we normalise
our dataset.&lt;/p&gt;
&lt;p&gt;First of all, SOMs train faster (and "better") if all our values are
between 0 and 1. This is often true with machine learning problems, and
it's to avoid one of our dimensions "dominating" the others in the
learning process. For example, if one of our variable was salary (in the
thousands) and another was height (in metres, so rarely over 2.0) then
salary will get a higher importance simply because it has much higher
values. Normalising to the unit interval will remove this effect.&lt;/p&gt;
&lt;p&gt;In our case all 3 dimensions refer to a value between 0 and 255 so we
can normalise the entire dataset at once. However, if our variables were
on different scales we would have to do this column by column.&lt;/p&gt;
&lt;p&gt;I don't want this code to be entirely tailored to the colour dataset so
I'll leave the normalisation options tied to a few Booleans that are
easy to change.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;normalise_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;

&lt;span class="c1"&gt;# if True, assume all data is on common scale&lt;/span&gt;
&lt;span class="c1"&gt;# if False, normalise to [0 1] range along each column&lt;/span&gt;
&lt;span class="n"&gt;normalise_by_column&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;

&lt;span class="c1"&gt;# we want to keep a copy of the raw data for later&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt;

&lt;span class="c1"&gt;# check if data needs to be normalised&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;normalise_data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;normalise_by_column&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# normalise along each column&lt;/span&gt;
        &lt;span class="n"&gt;col_maxes&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;col_maxes&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;newaxis&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;
    &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# normalise entire dataset&lt;/span&gt;
        &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;raw_data&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now we're ready to start the learning process.&lt;/p&gt;
&lt;h2&gt;Learning&lt;/h2&gt;
&lt;p&gt;In broad terms the learning process will be as follows. We'll fill in
the implementation details as we go along.&lt;/p&gt;
&lt;p&gt;For a single iteration:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Select one of our 3D colour vectors at random from our dataset&lt;/li&gt;
&lt;li&gt;Find the neuron in the SOM whose associated 3D vector is closest to
    our chosen 3D colour vector. At each step, this is called the Best
    Matching Unit (BMU)&lt;/li&gt;
&lt;li&gt;Move the BMU's 3D weight vector closer to the input vector in 3D
    space&lt;/li&gt;
&lt;li&gt;Identify the 2D neighbours of the BMU and also move their 3D weight
    vectors closer to the input vector, although by a smaller amount&lt;/li&gt;
&lt;li&gt;Update the learning rate (reduce it at each iteration)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;And that's it. By doing the penultimate step, moving the BMU's
neighbours, we'll achieve the desired effect that &lt;strong&gt;colours that are close in 3D space will be mapped to similar areas in 2D space&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Let's step through this in more detail, with code.&lt;/p&gt;
&lt;h3&gt;1. Select a Random Input Vector&lt;/h3&gt;
&lt;p&gt;This is straightforward:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# select a training example at random&lt;/span&gt;
&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;randint&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;2. Find the Best Matching Unit&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# find its Best Matching Unit&lt;/span&gt;
&lt;span class="n"&gt;bmu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bmu_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;find_bmu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;For that to work we need a function to find the BMU. It need to iterate
through each neuron in the SOM, measure its Euclidean distance to our
input vector and return the one that's closest. Note the implementation
trick of not actually measuring Euclidean distance, but the &lt;strong&gt;squared&lt;/strong&gt;
Euclidean distance, thereby avoiding an expensive square root
computation.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;find_bmu&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;        Find the best matching unit for a given vector, t, in the SOM&lt;/span&gt;
&lt;span class="sd"&gt;        Returns: a (bmu, bmu_idx) tuple where bmu is the high-dimensional BMU&lt;/span&gt;
&lt;span class="sd"&gt;                 and bmu_idx is the index of this vector in the SOM&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;bmu_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c1"&gt;# set the initial minimum distance to a huge number&lt;/span&gt;
    &lt;span class="n"&gt;min_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iinfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;    
    &lt;span class="c1"&gt;# calculate the high-dimensional distance between each neuron and the input&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
            &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# don&amp;#39;t bother with actual Euclidean distance, to avoid expensive sqrt operation&lt;/span&gt;
            &lt;span class="n"&gt;sq_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;t&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;sq_dist&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;min_dist&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;min_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sq_dist&lt;/span&gt;
                &lt;span class="n"&gt;bmu_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="c1"&gt;# get vector corresponding to bmu_idx&lt;/span&gt;
    &lt;span class="n"&gt;bmu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;bmu_idx&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;bmu_idx&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# return the (bmu, bmu_idx) tuple&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bmu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bmu_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;3. Update the SOM Learning Parameters&lt;/h3&gt;
&lt;p&gt;As described above, we want to decay the learning rate over time to let
the SOM "settle" on a solution.&lt;/p&gt;
&lt;p&gt;What we also decay is the &lt;strong&gt;neighbourhood radius&lt;/strong&gt;, which defines how
far we search for 2D neighbours when updating vectors in the SOM. We
want to gradually reduce this over time, like the learning rate. We'll
see this in a bit more detail in step 4.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# decay the SOM parameters&lt;/span&gt;
&lt;span class="n"&gt;r&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decay_radius&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_radius&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_constant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;decay_learning_rate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;init_learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The functions to decay the radius and learning rate use exponential
decay:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(\sigma_{t} = \sigma_{0} \times \exp(\frac{-t}{\lambda})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class="math"&gt;\(\lambda\)&lt;/span&gt; is the time constant (which controls the decay) and
&lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; is the value at various times &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decay_radius&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_radius&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;time_constant&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;initial_radius&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;time_constant&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;decay_learning_rate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;initial_learning_rate&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;initial_learning_rate&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;n_iterations&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3&gt;4. Move the BMU and its Neighbours in 3D Space&lt;/h3&gt;
&lt;p&gt;Now that we have the BMU and the correct learning parameters, we'll
update the SOM so that this BMU is now closer in 3D space to the colour
that mapped to it. We will also identify the neurons that are close to
the BMU in 2D space and update their 3D vectors to move "inwards"
towards the BMU.&lt;/p&gt;
&lt;p&gt;The formula to update the BMU's 3D vector is:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(w_{t+1} = w_{t} + L_{t}(V_{t} - w_{t})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;That is to say, the new weight vector will be the current vector plus
the difference between the input vector &lt;span class="math"&gt;\(V\)&lt;/span&gt; and the weight vector,
multiplied by a learning rate &lt;span class="math"&gt;\(L\)&lt;/span&gt; at time &lt;span class="math"&gt;\(t\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;We are literally just moving the weight vector closer to the input
vector.&lt;/p&gt;
&lt;p&gt;We also identify all the neurons in the SOM that are closer in 2D space
than our current radius, and also move them closer to the input vector.&lt;/p&gt;
&lt;p&gt;The difference is that the weight update will be &lt;strong&gt;proportional to their 2D distance from the BMU&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;One last thing to note: this proportion of 2D distance isn't uniform,
it's Gaussian. So imagine a bell shape centred around the BMU - that's
how we decide how much to pull the neighbouring neurons in by.&lt;/p&gt;
&lt;p&gt;Concretely, this is the equation we'll use to calculate the influence
&lt;span class="math"&gt;\(i\)&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class="math"&gt;\(i_{t} = \exp(-\frac{d^{2}}{2\sigma_{t}^{2}})\)&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class="math"&gt;\(d\)&lt;/span&gt; is the 2D distance and &lt;span class="math"&gt;\(\sigma\)&lt;/span&gt; is the current radius of
our neighbourhood.&lt;/p&gt;
&lt;p&gt;Putting that all together:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;calculate_influence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;distance&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;radius&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;distance&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;radius&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# now we know the BMU, update its weight vector to move closer to input&lt;/span&gt;
&lt;span class="c1"&gt;# and move its neighbours in 2-D space closer&lt;/span&gt;
&lt;span class="c1"&gt;# by a factor proportional to their 2-D distance from the BMU&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;m&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# get the 2-D distance (again, not the actual Euclidean distance)&lt;/span&gt;
        &lt;span class="n"&gt;w_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;array&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;bmu_idx&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="c1"&gt;# if the distance is within the current neighbourhood radius&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;w_dist&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;=&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# calculate the degree of influence (based on the 2-D distance)&lt;/span&gt;
            &lt;span class="n"&gt;influence&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;calculate_influence&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;w_dist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="c1"&gt;# now update the neuron&amp;#39;s weight using the formula:&lt;/span&gt;
            &lt;span class="c1"&gt;# new w = old w + (learning rate * influence * delta)&lt;/span&gt;
            &lt;span class="c1"&gt;# where delta = input vector (t) - old w&lt;/span&gt;
            &lt;span class="n"&gt;new_w&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;l&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;influence&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;t&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;w&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
            &lt;span class="c1"&gt;# commit the new weight&lt;/span&gt;
            &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_w&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Visualisation&lt;/h2&gt;
&lt;p&gt;Repeating the learning steps 1-4 for 2,000 iterations should be enough.
We can always run it for more iterations afterwards.&lt;/p&gt;
&lt;p&gt;Handily, the 3D weight vectors in the SOM can also be interpreted as
colours, since they are just 3D vectors just like the inputs.&lt;/p&gt;
&lt;p&gt;To that end, we can visualise them and come up with our final colour
map:&lt;/p&gt;
&lt;p&gt;&lt;img alt="som" src="/images/self-organising-maps-in-depth/som.png"&gt;&lt;/p&gt;
&lt;p&gt;A self-organising colour map&lt;/p&gt;
&lt;p&gt;None of those colours necessarily had to be in our dataset. By moving
the 3D weight vectors to more closely match our input vectors, we've
created a 2D colour space which clearly shows the relationship between
colours. More blue colours will map to the left part of the SOM, whereas
reddish colours will map to the bottom, and so on.&lt;/p&gt;
&lt;h1&gt;Other Examples&lt;/h1&gt;
&lt;p&gt;Finding a 2D colour space is a good visual way to get used to the idea
of a SOM. However, there are obviously practical applications of this
algorithm.&lt;/p&gt;
&lt;h2&gt;Iris Dataset&lt;/h2&gt;
&lt;p&gt;A dataset favoured by the machine learning community is Sir Ronald
Fisher's &lt;a href="http://archive.ics.uci.edu/ml/datasets/Iris"&gt;dataset of measurements of irises&lt;/a&gt;. There are four
input dimensions: petal width, petal length, sepal width and sepal
length and we could use a SOM to find similar flowers.&lt;/p&gt;
&lt;p&gt;Applying the iris data to a SOM and then retrospectively colouring each
point with their true class (to see how good the SOM was at separating
the irises into their distinct categories) we get something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="iris clusters" src="/images/self-organising-maps-in-depth/iris_clusters.png"&gt;&lt;/p&gt;
&lt;p&gt;150 irises mapped onto a SOM, coloured by type&lt;/p&gt;
&lt;p&gt;This is a 10 by 10 SOM and each of the small points is one of the irises
from the dataset (with added jitter to see multiple points on a single
SOM neuron). I added the colours after training, and you can quite
clearly see the 3 distinct regions the SOM has divided itself into.&lt;/p&gt;
&lt;p&gt;There are a few SOM neurons where both the green and the blue points get
assigned to, and this represents the overlap between the versicolor and
virginica types.&lt;/p&gt;
&lt;h2&gt;Handwritten Digits&lt;/h2&gt;
&lt;p&gt;Another application I touched on in Part 1 is trying to identify
handwritten characters.&lt;/p&gt;
&lt;p&gt;In this case, the inputs are high-dimensional - each input dimension
represents the grayscale value of one pixel on a 28 by 28 image. That
makes the inputs 784-dimensional (each dimension is a value between 0
and 255).&lt;/p&gt;
&lt;p&gt;Mapping them to a 20 by 20 SOM, and again retrospectively colouring them
based on their true class (a number from 0 to 9) yields this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="A SOM of handwritten characters" src="/images/self-organising-maps-in-depth/mnist_som.png"&gt;&lt;/p&gt;
&lt;p&gt;Various handwritten numbers mapped to a 2D SOM&lt;/p&gt;
&lt;p&gt;In this case the true classes are labelled according to the colours in
the bottom left.&lt;/p&gt;
&lt;p&gt;What you can see is that the SOM has successfully divided the 2D space
into regions. Despite some overlap, in most cases similar digits get
mapped to the same region.&lt;/p&gt;
&lt;p&gt;For example, the yellow region is where the 6s were mapped, and there is
little overlap with other categories. Whereas in the bottom left, where
the green and brown points overlap, is where the SOM was "confused"
between 4s and 9s. A visual inspection of some of these handwritten
characters shows that indeed many of the 4s and 9s are easily confused.&lt;/p&gt;
&lt;h2&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;I hope this was a useful walkthrough on the intuition behind a SOM, and
a simple Python implementation. There is &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/self-organising-map/Self-Organising%20Map.ipynb"&gt;a Jupyter notebook&lt;/a&gt;
for the colour map example.&lt;/p&gt;
&lt;p&gt;Mat Buckland's &lt;a href="http://www.ai-junkie.com/ann/som/som1.html"&gt;excellent explanation and code walkthrough&lt;/a&gt; of SOMs was
instrumental in helping me learn. My code is more or less a Python port
of his C++ implementation. Reading his posts should fill in any gaps I
may have not covered.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 6&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;
&lt;script type="text/javascript"&gt;if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width &lt; 768) ? "left" : align;
        indent = (screen.width &lt; 768) ? "0em" : indent;
        linebreak = (screen.width &lt; 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
&lt;/script&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category><category term="python"></category></entry><entry><title>Self-Organising Maps: An Introduction</title><link href="/blog/self-organising-maps-an-introduction" rel="alternate"></link><published>2016-11-05T18:26:00+00:00</published><updated>2016-11-05T18:26:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-05:/blog/self-organising-maps-an-introduction</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When you learn about machine learning techniques, you usually get a
selection of the usual suspects. Something like: Support Vector
Machines, decision trees/random forests, and logistic regression for
classification, linear regression for regression, k-means for clustering
and perhaps PCA for dimensionality reduction.&lt;/p&gt;
&lt;p&gt;In fact, KDNuggets has a good …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;When you learn about machine learning techniques, you usually get a
selection of the usual suspects. Something like: Support Vector
Machines, decision trees/random forests, and logistic regression for
classification, linear regression for regression, k-means for clustering
and perhaps PCA for dimensionality reduction.&lt;/p&gt;
&lt;p&gt;In fact, KDNuggets has a good post about &lt;a href="http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html"&gt;the 10 machine learning algorithms you should know&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you want to learn about machine learning techniques, you should start
there. The point is, on the subject of these algorithms the internet has
you covered.&lt;/p&gt;
&lt;p&gt;In this post I want to talk about a less prevalent algorithm, but one
that I like and that can be useful for different purposes.&lt;/p&gt;
&lt;p&gt;It's called a Self-Organising Map (SOM).&lt;/p&gt;
&lt;h1&gt;Brief History&lt;/h1&gt;
&lt;p&gt;SOMs are a type of artificial neural network. Some of the concepts date
back further, but SOMs were proposed and became widespread in the 1980s,
by a Finnish professor named Teuvo Kohonen. Unsurprisingly SOMs are also
referred to as Kohonen maps.&lt;/p&gt;
&lt;h2&gt;Artificial Neural Networks&lt;/h2&gt;
&lt;p&gt;Artifical neural networks (ANNs) were designed initially to be a
computational representation of what is believed to happen in the brain.
The way signals are passed along an ANN is based on how signals pass
between neurons in the brain.&lt;/p&gt;
&lt;p&gt;ANNs are constructed as a series of &lt;strong&gt;layers&lt;/strong&gt; of connected nodes. The
first layer consists of your inputs, the last layer consists of your
outputs, and there are any number of so-called &lt;em&gt;hidden&lt;/em&gt; layers in
between.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Simple neural network architecture" src="/images/self-organising-maps-an-introduction/296px-Colored_neural_network.png"&gt;&lt;/p&gt;
&lt;p&gt;&lt;small&gt;By &lt;a href="//commons.wikimedia.org/wiki/User_talk:Glosser.ca" title="User talk:Glosser.ca"&gt;Glosser.ca&lt;/a&gt; - [Own work], Derivative of &lt;a href="//commons.wikimedia.org/wiki/File:Artificial_neural_network.svg" title="File:Artificial neural network.svg"&gt;File:Artificial neural network.svg&lt;/a&gt;, &lt;a href="http://creativecommons.org/licenses/by-sa/3.0" title="Creative Commons Attribution-Share Alike 3.0"&gt;CC BY-SA 3.0&lt;/a&gt;, &lt;a href="https://commons.wikimedia.org/w/index.php?curid=24913461"&gt;Link&lt;/a&gt;&lt;/small&gt;
 &lt;/p&gt;
&lt;p&gt;The broad idea of an ANN is that you give it a dataset and a set of
desired outputs, and it learns to map the inputs to the outputs. A
classic example is teaching an ANN to recognise handwritten characters
by giving it pixel values as inputs and the correct digit (say a number
from 0-9) as the output.&lt;/p&gt;
&lt;p&gt;During the &lt;strong&gt;training phase&lt;/strong&gt; it learns the associations between pixel
values and the digits. Then, you can give it a new set of inputs, digits
it hasn't seen before, and it will be able to recognise them.&lt;/p&gt;
&lt;p&gt;Here is &lt;a href="http://yann.lecun.com/exdb/lenet/"&gt;such a system&lt;/a&gt; recognising
characters in real time. It was built by Yann LeCun in the 1990s.&lt;/p&gt;
&lt;p&gt;The way most ANNs "learn" a particular problem is by error-correcting.
That is, during the training phase they adapt and improve based on the
errors they make, and incrementally get better at solving the problem.&lt;/p&gt;
&lt;p&gt;This is a &lt;strong&gt;supervised&lt;/strong&gt; machine learning problem because you are
telling the algorithm the desired answer for each set of inputs it's
trained on, so it knows if it makes errors.&lt;/p&gt;
&lt;h2&gt;The SOM as an ANN&lt;/h2&gt;
&lt;p&gt;There are three main ways in which a Self-Organising Map is different
from a "standard" ANN:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A SOM is not a series of layers, but typically a 2D grid of neurons&lt;/li&gt;
&lt;li&gt;They don't learn by error-correcting, they implement something
    called &lt;strong&gt;competitive learning&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;They deal with &lt;strong&gt;unsupervised&lt;/strong&gt; machine learning problems&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Competitive learning in the case of a SOM refers to the fact that when
an input is "presented" to the network, only one of the neurons in the
grid will be activated. In a way the neurons on the grid "compete" for
each input.&lt;/p&gt;
&lt;p&gt;The unsupervised aspect of a SOM refers to the idea that you present
your inputs to it without associating them with an output. Instead, a
SOM is used to find structure in your data.
 &lt;/p&gt;
&lt;h1&gt;What is a SOM used for?&lt;/h1&gt;
&lt;p&gt;This last point about unsupervised learning brings me to an important
question, because abstract concepts like neural networks are great to
talk about but I'm a practical kind of guy.&lt;/p&gt;
&lt;p&gt;In that spirit then, what is a SOM used for?&lt;/p&gt;
&lt;h2&gt;Finding Structure&lt;/h2&gt;
&lt;p&gt;A classic example of what clustering algorithms are used for is finding
similar customers in your customer base. SOMs can also do this. In fact,
a SOM is meant to be &lt;strong&gt;a 2D representation of your multi-dimensional
dataset&lt;/strong&gt;. In this 2D representation, each of your original inputs, e.g.
each of your customers, maps to one of the nodes on the 2D grid. Most
importantly, &lt;strong&gt;similar (high-dimensional) inputs will map to the same 2D
node,&lt;/strong&gt; or at least the same region in 2D space. This is how the SOM
finds and groups similar inputs together.&lt;/p&gt;
&lt;h2&gt;Dimensionality Reduction&lt;/h2&gt;
&lt;p&gt;Related to finding structure is the fact that by finding this structure
a SOM finds a lower-dimensional representation of your dataset &lt;strong&gt;while
preserving the similarity between your records&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;That is, data points that are "nearby" in high-dimensional space will
also be nearby in the SOM.&lt;/p&gt;
&lt;h2&gt;Visualisation&lt;/h2&gt;
&lt;p&gt;By creating a (typically) 2D representation of your dataset you can also
more easily visualise it, which you can't do if your data has more than
3 dimensions.&lt;/p&gt;
&lt;h1&gt;Summary&lt;/h1&gt;
&lt;p&gt;To summarise, I'll quote an answer I gave on StackOverflow to a question
about SOMs:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The idea behind a SOM is that you're mapping high-dimensional vectors
onto a smaller dimensional (typically 2D) space. You can think of it
as clustering, like in K-means, with the added difference that vectors
that are close in the high-dimensional space also end up being mapped
to nodes that are close in 2D space.&lt;/p&gt;
&lt;p&gt;SOMs therefore are said to "preserve the topology" of the original
data, because the distances in 2D space reflect those in the
high-dimensional space. K-means also clusters similar data points
together, but its final "representation" is hard to visualise because
it's not in a convenient 2D format.&lt;/p&gt;
&lt;p&gt;A typical example is with colours, where each of the data points are
3D vectors that represent R,G,B colours. When mapped to a 2D SOM you
can see regions of similar colours begin to develop, which is the
topology of the colour space.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2&gt;Colours&lt;/h2&gt;
&lt;p&gt;I hope that sounds interesting, because in Part 2 of this post I'll
discuss some concrete examples and walk through a Python implementation
of Self-Organising Maps.&lt;/p&gt;
&lt;p&gt;The example we'll be working with is using a 3D dataset of colours
(where the 3 dimensions are R, G and B) and producing a 2D SOM where we
visualise the "topology" of the 3D colour space.&lt;/p&gt;
&lt;p&gt;Something like this:&lt;/p&gt;
&lt;p&gt;&lt;img alt="som" src="/images/self-organising-maps-an-introduction/som.png"&gt;&lt;/p&gt;
&lt;p&gt;A Self-Organising Colourmap &lt;/p&gt;
&lt;p&gt;In &lt;a href="/blog/self-organising-maps-in-depth/"&gt;Part 2&lt;/a&gt;, we'll look at an in-depth implementation of SOMs.&lt;/p&gt;
&lt;p&gt;Footnote: This was the 5&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="machine learning"></category><category term="featured"></category></entry><entry><title>Pandas: Thinking in Vectors</title><link href="/blog/pandas-thinking-in-vectors" rel="alternate"></link><published>2016-11-04T20:35:00+00:00</published><updated>2016-11-04T20:35:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-04:/blog/pandas-thinking-in-vectors</id><summary type="html">&lt;p&gt;The more you use pandas to wrangle your data the more likely you'll come
across something complicated that you won't be sure how to do. I found
this quite quickly when trying to calculate metrics with time series
data for example.&lt;/p&gt;
&lt;p&gt;In these cases quite often the first solution that …&lt;/p&gt;</summary><content type="html">&lt;p&gt;The more you use pandas to wrangle your data the more likely you'll come
across something complicated that you won't be sure how to do. I found
this quite quickly when trying to calculate metrics with time series
data for example.&lt;/p&gt;
&lt;p&gt;In these cases quite often the first solution that pops into my head
will be the naive, brute force one. Something like "well we can loop
through all the rows and perform a computation on each row one by one".&lt;/p&gt;
&lt;p&gt;That is almost never the right approach, because it will be &lt;em&gt;slow&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The better solution is almost always to make use of vectorisation.&lt;/p&gt;
&lt;p&gt;Pandas is built on top of numpy, which is built with vectors in mind -
that is, manipulating entire arrays at once rather than the individual
elements.&lt;/p&gt;
&lt;p&gt;That way of thinking can be a hard to adjust to when the temptation is
to use loops.&lt;/p&gt;
&lt;p&gt;Sometimes it's not a bad idea to start with the brute force approach to
be confident of the answer, and then move to a vectorised solution. For
large datasets though, this move can easily be the difference between
the script taking seconds or hours to run.&lt;/p&gt;
&lt;h1&gt;Examples&lt;/h1&gt;
&lt;h2&gt;Date Methods&lt;/h2&gt;
&lt;p&gt;Applying methods to a column of date values is a common data
manipulation task, for example when extracting the day as a new feature.
There are (at least) two functionally identical ways of doing this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Method 1 - row by row&lt;/span&gt;
&lt;span class="n"&gt;days&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;s&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;my_dataframe&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_date_column&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
    &lt;span class="n"&gt;days&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;s&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;my_dataframe&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;day&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;days&lt;/span&gt;

&lt;span class="c1"&gt;# Method 2 - using the inbuilt and vectorised date functionality&lt;/span&gt;
&lt;span class="n"&gt;my_dataframe&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;day&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;my_dataframe&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;my_date_column&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;day&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;They'll do exactly the same thing, but the second will be orders of
magnitude faster.&lt;/p&gt;
&lt;h2&gt;Subtracting Consecutive Values&lt;/h2&gt;
&lt;p&gt;This is one of those problems where, if you don't know the pandas way to
do it, it's easy to start thinking row by row.&lt;/p&gt;
&lt;p&gt;Again, here are two functionally identical ways of doing it:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# Method 1 - a function to loop through the elements one by one&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;naive_diff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;diff_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="c1"&gt;# first value needs to be NaN&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;diff_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;NaN&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;diff_values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;series&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;diff_values&lt;/span&gt;

&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;diff&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;naive_diff&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;measurement&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Method 2 - using the pandas shift() function&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;diff_2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;measurement&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;measurement&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shift&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As well as being shorter, the second method is again much faster.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;This was just a &lt;em&gt;very&lt;/em&gt; brief introduction into thinking in vectors when
using pandas.&lt;/p&gt;
&lt;p&gt;The code is available &lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/pandas-thinking-in-vectors/pandas-vector-examples.ipynb"&gt;as a Jupyter notebook&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The take away message is that whenever you need to do something to each
row, it's worth spending time doing some research to look for an
appropriate, in built function, and thinking a bit harder about how to
solve it in a vectorised way.&lt;/p&gt;
&lt;h2&gt;Footnote: Apply and Itertuples&lt;/h2&gt;
&lt;p&gt;If you absolutely must loop through the dataframe row by row, you should
consider using apply and itertuples. They are two pandas functions that
let you perform elementwise computation, but are faster than manually
looping through the row indices.&lt;/p&gt;
&lt;p&gt;There are further good tips &lt;a href="http://stackoverflow.com/questions/7837722/what-is-the-most-efficient-way-to-loop-through-dataframes-with-pandas"&gt;under this StackOverflow question&lt;/a&gt;.&lt;/p&gt;
&lt;h2&gt;Further Reading&lt;/h2&gt;
&lt;p&gt;When doing some research for this post I came across &lt;a href="https://www.datascience.com/blog/straightening-loops-how-to-vectorize-data-aggregation-with-pandas-and-numpy/"&gt;this blog post&lt;/a&gt;,
which is worth a read on the subject.&lt;/p&gt;
&lt;p&gt;Footnote #2: This was the 4&lt;sup&gt;th&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category><category term="pandas"></category><category term="python"></category></entry><entry><title>The Junk in Fallout 4 - a Web Scraping Tutorial</title><link href="/blog/the-junk-in-fallout-4" rel="alternate"></link><published>2016-11-03T22:36:00+00:00</published><updated>2016-11-03T22:36:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-03:/blog/the-junk-in-fallout-4</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;A few months ago I was playing one of my favourite games of 2015 -
&lt;a href="https://en.wikipedia.org/wiki/Fallout_4"&gt;Fallout 4.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you haven't heard of it, it's an action-RPG set in a post-apocalyptic
world. The story is engaging and you're sort of trying to rebuild
civilisation after the world has been …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;h2&gt;The Problem&lt;/h2&gt;
&lt;p&gt;A few months ago I was playing one of my favourite games of 2015 -
&lt;a href="https://en.wikipedia.org/wiki/Fallout_4"&gt;Fallout 4.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If you haven't heard of it, it's an action-RPG set in a post-apocalyptic
world. The story is engaging and you're sort of trying to rebuild
civilisation after the world has been devastated by nuclear war. I mean
"rebuild" quite literally - a lot of the game is spent scavenging for
raw materials so you can build pretty shoddy housing for strangers. And
that rebuilding requires raw materials, which is contained mostly in
junk.&lt;/p&gt;
&lt;p&gt;Lots and lots of junk.&lt;/p&gt;
&lt;p&gt;You can end up spending a disproportionately long time looking for,
buying and selling things like broken telephones, toy rocketships, teddy
bears, and ashtrays all because they contain precious raw materials for
your resettlement project, or for enhancing your guns and armour.&lt;/p&gt;
&lt;p&gt;If you play it long enough eventually you start remembering which items
contain which raw material (information which then replaces more
important things in your brain...). However, not all items contain the
same amount of raw material. For example aluminium, which is quite
useful but relatively rare, is in oil cans (which contain 1, erm, "unit"
of aluminium) but it's better to get it from surgical trays, which
contain 3.&lt;/p&gt;
&lt;p&gt;That means if you want to maximise your scavenging efforts you have to
start remembering which item contains which material &lt;em&gt;and&lt;/em&gt; how much it
contains.&lt;/p&gt;
&lt;p&gt;So I thought: is there a more efficient way of scavenging?&lt;/p&gt;
&lt;p&gt;The answer is yes.&lt;/p&gt;
&lt;p&gt;With data.&lt;/p&gt;
&lt;h2&gt;The solution&lt;/h2&gt;
&lt;p&gt;What I really wanted to do is look at each material, and figure out
which items it made sense to find to maximise the amount of the material
I get. The plan was:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find a list of all the items in Fallout 4 and the materials they
    contain&lt;/li&gt;
&lt;li&gt;Scrape and store the list&lt;/li&gt;
&lt;li&gt;Analyse it and make some plots&lt;/li&gt;
&lt;/ul&gt;
&lt;h1&gt;The Tutorial&lt;/h1&gt;
&lt;h2&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First of all, we need some data and some Python libraries.&lt;/p&gt;
&lt;p&gt;The data comes from &lt;a href="http://fallout.wikia.com/wiki/Fallout_4_junk_items"&gt;a webpage of all Fallout 4 junk items&lt;/a&gt;, specifically
the table marked "&lt;a href="http://fallout.wikia.com/wiki/Fallout_4_junk_items#Other_junk_items"&gt;Other junk items&lt;/a&gt;".&lt;/p&gt;
&lt;p&gt;The Python libraries we need are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://docs.python-requests.org"&gt;requests&lt;/a&gt; - for fetching the
    webpage content via HTTP&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/"&gt;BeautifulSoup&lt;/a&gt; -
    for reading, parsing and extracting data from the returned HTML&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pandas.pydata.org/pandas-docs/stable/"&gt;pandas&lt;/a&gt; - the usual
    suspect for manipulating data&lt;/li&gt;
&lt;li&gt;&lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt; - for plotting&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Step 1 - Fetch the Data&lt;/h2&gt;
&lt;p&gt;Side note: This isn't a tutorial about HTML, CSS or Javascript, so I'll
assume you know enough about them. However, if you want to learn/brush
up, &lt;a href="http://www.w3schools.com"&gt;w3schools&lt;/a&gt; is a great resource.&lt;/p&gt;
&lt;p&gt;We've identified our data source and it's already in a table format,
which will make things easier. What we want to do is isolate the table
in the HTML so we can extract only that table.&lt;/p&gt;
&lt;p&gt;The easiest way to do this is directly on the website.&lt;/p&gt;
&lt;p&gt;Right click and choose "Inspect Element".&lt;/p&gt;
&lt;p&gt;&lt;img alt="inspect element" src="/images/the-junk-in-fallout-4/inspect_element.png"&gt;&lt;/p&gt;
&lt;p&gt;I'm using Firefox but this should be applicable to any browser.&lt;/p&gt;
&lt;p&gt;This will open the Inspector, or Chrome Dev Tools or the equivalent,
where we can inspect the raw HTML. Then we want to look at the
&amp;lt;table&amp;gt; element to see its ID.&lt;/p&gt;
&lt;p&gt;&lt;img alt="HTML table in the inspector" src="/images/the-junk-in-fallout-4/tablehtml.png"&gt;&lt;/p&gt;
&lt;p&gt;Unlucky, no ID. &lt;/p&gt;
&lt;p&gt;OK, well it doesn't have an ID, so we need to find a way to uniquely
select that element. It has a class "va-table-center" - we can check if
that's unique on the page. To do this, go into the Console tab of the
Inspector (in the above screenshot it's the second tab, but your browser
may differ). In the console you can type and evaluate arbitrary
Javascript/jQuery expressions, so let's try selecting a table with the
class &lt;em&gt;va-table-center&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Selecting the table in the console" src="/images/the-junk-in-fallout-4/junktable_selector.png"&gt;&lt;/p&gt;
&lt;p&gt;Apparently a script on the page finished "shamefully"... &lt;/p&gt;
&lt;p&gt;It looks like that class isn't unique, there are 5 tables that match it
on the page. What we can do is then figure out which one of those 5 is
the one we need. We can expand the output on the console to help us find
it.&lt;/p&gt;
&lt;p&gt;You can highlight each of the 5 items and the corresponding table will
be highlighted on the page. Doing this reveals we're after the second of
those 5 tables.&lt;/p&gt;
&lt;p&gt;&lt;img alt="table highlight" src="/images/the-junk-in-fallout-4/table_highlight.png"&gt;&lt;/p&gt;
&lt;p&gt;Note the Halloween-themed adverts about The Exorcist &lt;/p&gt;
&lt;p&gt;Great, now we've identified the HTML element we want to load in, and we
can do the rest of the wrangling in Python.&lt;/p&gt;
&lt;h2&gt;Step 2 - Clean and Wrangle&lt;/h2&gt;
&lt;p&gt;Using requests and BeautifulSoup we read in the HTML and grab just the
table we need.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;requests&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;bs4&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;
&lt;span class="c1"&gt;# load the entire page&lt;/span&gt;
&lt;span class="n"&gt;req&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;requests&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;http://fallout.wikia.com/wiki/Fallout_4_junk_items&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# read the output as text&lt;/span&gt;
&lt;span class="n"&gt;raw&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;req&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;
&lt;span class="c1"&gt;# load it into BeautifulSoup for parsing&lt;/span&gt;
&lt;span class="n"&gt;soup&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;BeautifulSoup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;raw&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;html.parser&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# select just the second table with the right class&lt;/span&gt;
&lt;span class="n"&gt;junk_table&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;soup&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;table.va-table-center&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next step is to iterate over the table rows and extract what we need.
For now, we'll focus on the item name and its components.&lt;/p&gt;
&lt;p&gt;There are often multiple materials in a single item, so we need to
extract them individually. What you'll notice in the table is that each
new material starts with a capital letter, so we can split on that with
some regex. Then we'll create a dictionary where the key is the item and
the value is a list of its components.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;junk_dict&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

&lt;span class="c1"&gt;# start from the second table row (to skip the header)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;junk_table&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findAll&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;tr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;:]:&lt;/span&gt;
    &lt;span class="c1"&gt;# select all the table cells&lt;/span&gt;
    &lt;span class="n"&gt;cells&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;select&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;td&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="c1"&gt;# get item name and components cells&lt;/span&gt;
    &lt;span class="n"&gt;item_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;components_cell&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;cells&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="c1"&gt;# split components text at uppercase letters&lt;/span&gt;
    &lt;span class="n"&gt;components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;findall&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;[A-Z][^A-Z]*&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;components_cell&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;text&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
    &lt;span class="n"&gt;junk_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;item_name&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;components&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next we'll create a pandas DataFrame from the dictionary. The only quirk
is some components have "x2" style additions if there are more than one
units in the item, and the absence of this signifies 1 unit.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;columns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;item&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;component&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;quantity&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;row_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;k&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;junk_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;junk_dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
        &lt;span class="n"&gt;quantity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="c1"&gt;# default unless otherwise specified&lt;/span&gt;
        &lt;span class="c1"&gt;# extract the multiplier&lt;/span&gt;
        &lt;span class="n"&gt;multiplier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;re&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39; ?x ?([0-9]*)&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;component_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;
        &lt;span class="c1"&gt;# if the multiplier is specified set the quantity to the right value&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;quantity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;multiplier&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;group&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
                &lt;span class="n"&gt;component_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;c&lt;/span&gt;&lt;span class="p"&gt;[:&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="c1"&gt;# add as a row&lt;/span&gt;
        &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;row_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;k&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;component_name&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;quantity&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;row_idx&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2&gt;Step 3 - Plot&lt;/h2&gt;
&lt;p&gt;We now have our very own Fallout 4 junk dataset that we can analyse to
our heart's content. For example we can plot the frequencies of each
component to order them by rarity.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Rarity of junk components" src="/images/the-junk-in-fallout-4/junkplot.png"&gt;&lt;/p&gt;
&lt;p&gt;It's a strange world where concrete is so rare.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;There you have it - data-driven video gaming. Here's the whole thing in
&lt;a href="https://github.com/davidasboth/blog-notebooks/blob/master/fallout-junk/Fallout%20Junk%20Data.ipynb"&gt;Jupyter notebook form&lt;/a&gt;,
and a direct link to &lt;a href="/files/fallout_junk.csv"&gt;the csv file&lt;/a&gt;
if you want to analyse it yourself - if you do, let me know!&lt;/p&gt;
&lt;p&gt;Footnote: This was the 3&lt;sup&gt;rd&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="python"></category></entry><entry><title>Other People</title><link href="/blog/other-people" rel="alternate"></link><published>2016-11-02T18:11:00+00:00</published><updated>2016-11-02T18:11:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-02:/blog/other-people</id><summary type="html">&lt;p&gt;If you were applying for a new job and had a choice between working on a
project alone or with other people, which would you choose?&lt;/p&gt;
&lt;p&gt;Introverts might prefer a project where they minimise human interaction,
and some people really do work better if they're left alone for long
periods …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you were applying for a new job and had a choice between working on a
project alone or with other people, which would you choose?&lt;/p&gt;
&lt;p&gt;Introverts might prefer a project where they minimise human interaction,
and some people really do work better if they're left alone for long
periods of time.&lt;/p&gt;
&lt;p&gt;I'm not one of those people. I do my best work with others.&lt;/p&gt;
&lt;h2&gt;Working with Others&lt;/h2&gt;
&lt;p&gt;When I wanted to start moving from web development to data science, I
could have spent my free time doing various online courses to learn the
material, working on side projects to sharpen my skills and eventually
getting good enough to apply for jobs. However, that would have involved
me doing most of it alone. Without the ability to exchange ideas with
people, get feedback on my work and get that feeling of "we're in this
together" I wouldn't have made enough progress. In the end I bit the
bullet and enrolled on a Master's programme, and it was the right
decision (for me).&lt;/p&gt;
&lt;p&gt;The benefits of collaborating with others is probably higher in data
science than other professions, because it's often so exploratory in
nature. Even in software engineering it's plausible to be the only
developer on a project and still create a perfectly good product. I'd
speculate that the gains in adding a second data scientist to your team,
though, are much higher.&lt;/p&gt;
&lt;p&gt;I might be wrong, and there will almost certainly be amazing data
scientists out there who are working on their own and wouldn't gain much
from a second team member, but my feeling is they're the minority.
Certainly you need to work with other, non-technical people from the
business to make sure you're solving the right problems and in the right
way, so you can't be completely in isolation anyway.&lt;/p&gt;
&lt;h2&gt;Others Outside of Work&lt;/h2&gt;
&lt;p&gt;What about outside of work?&lt;/p&gt;
&lt;p&gt;I'm quite an extroverted person and I'm probably not alone in feeling
energised by the presence of like-minded people who are motivated by the
same things, e.g. data science. Hit Reply, a podcast I listen to,
dedicated &lt;a href="https://hitreply.co/ep/6/surround-yourself/"&gt;an entire episode&lt;/a&gt; to the benefits of
surrounding yourself with others if you're a startup founder, or just
working on a side project with the view to making it into a startup.&lt;/p&gt;
&lt;p&gt;I can feel myself being a lot more motivated to work on a side project
even if I just talk about it for a bit with other people.&lt;/p&gt;
&lt;p&gt;So if you're like me, what can &lt;strong&gt;you&lt;/strong&gt; do to be around the right kind of
"other people"?&lt;/p&gt;
&lt;h2&gt;Meetups&lt;/h2&gt;
&lt;p&gt;The meetup scene in London is particularly big and diverse. You wouldn't
have to go too much outside of your interests to find a meetup to attend
&lt;em&gt;every single day&lt;/em&gt;. Just searching for the word "data" on
&lt;a href="https://www.meetup.com/"&gt;meetup.com&lt;/a&gt; gives me a list where I could
easily pick a daily meetup.&lt;/p&gt;
&lt;p&gt;I often joke that if you want to save money on food, go to a meetup that
has free pizza every night. On top of getting dinner for free you'll
also learn loads and meet new people!&lt;/p&gt;
&lt;p&gt;You shouldn't do that.&lt;/p&gt;
&lt;p&gt;But you could.&lt;/p&gt;
&lt;h2&gt;Online Communities&lt;/h2&gt;
&lt;p&gt;Sometimes you want to be part of a community without being physically
present. Maybe you want to watch from the sidelines as people talk about
topics of interest, or you just don't have the time to attend meetups
all the time. Online communities are a great alternative. Just off the
top of my head, you'll find plenty of content and discussion on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://reddit.com/"&gt;Reddit&lt;/a&gt; (e.g.
    &lt;a href="https://www.reddit.com/r/MachineLearning/"&gt;/r/machinelearning&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://quora.com/"&gt;Quora&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/"&gt;Hacker News&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://stats.stackexchange.com/"&gt;"Cross-Validated" on Stack Overflow&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of these are more geared towards general discussion, some are Q&amp;amp;A
sites, but they're all communities where you can be surrounded
(virtually) by the right kind of people. Being part of a Slack channel
also helps - we (MSc students) use ours regularly.&lt;/p&gt;
&lt;p&gt;What do you do to be surrounded by the right people?&lt;/p&gt;
&lt;p&gt;Footnote: This was the 2&lt;sup&gt;nd&lt;/sup&gt; entry in my &lt;a href="/blog/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category></entry><entry><title>The Power of Podcasts</title><link href="/blog/the-power-of-podcasts" rel="alternate"></link><published>2016-11-01T21:12:00+00:00</published><updated>2016-11-01T21:12:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-01:/blog/the-power-of-podcasts</id><summary type="html">&lt;p&gt;I've been thinking about podcasts recently.&lt;/p&gt;
&lt;p&gt;I spend a significant amount of my week doing necessary things that feel
like lost time, like commuting or washing the dishes. When I commute I
usually have my phone out, probably playing a pointless game just to
pass the time. When I do …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've been thinking about podcasts recently.&lt;/p&gt;
&lt;p&gt;I spend a significant amount of my week doing necessary things that feel
like lost time, like commuting or washing the dishes. When I commute I
usually have my phone out, probably playing a pointless game just to
pass the time. When I do the dishes I feel like "the dishes being done"
is not enough of a return on the investment of time that I put in.&lt;/p&gt;
&lt;p&gt;These pockets of time are perfect for listening to podcasts.&lt;/p&gt;
&lt;p&gt;In general I feel like it's probably worthwhile putting my attention
into what I'm doing rather than letting my mind wander, but in these
cases I make a conscious decision to put my attention elsewhere. I want
to hear about new ideas, keep up with the latest technology news or
machine learning research, and if I can do it while I'm doing something
that needs to be done anyway, I feel like I'm &lt;em&gt;winning&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I also find podcasts a good tool for learning.&lt;/p&gt;
&lt;p&gt;Everyone learns differently, but I find I retain more information that I
&lt;em&gt;hear&lt;/em&gt; than I read. I only have limited time to dedicate to learning new
things, so again podcasts seem like the right kind of resource. It also
helps that there are some great podcasts out there for data science and
technology.&lt;/p&gt;
&lt;p&gt;I want to share a few of the podcasts I listen to, specifically for
those interested in data science.&lt;/p&gt;
&lt;h1&gt;Data Science Podcasts&lt;/h1&gt;
&lt;h2&gt;&lt;a href="http://dataskeptic.com/"&gt;Data Skeptic&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;I like Data Skeptic because of the way they alternate between episodes
dedicated to single concepts in data science, and interesting
interviews. Recommended both from a learning perspective, and for the
inspiration of hearing people talk about their cool projects.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://partiallyderivative.com/"&gt;Partially Derivative&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A lighthearted (but sometimes serious) alternative to the more
learning-heavy podcasts out there. The hosts are entertaining and
alternate between talking about data news to talking to interesting data
people. This is definitely the most "beginner-friendly" podcast on the
list because it's more about the data &lt;em&gt;stories&lt;/em&gt; rather than the
technical details. I like it because it adds variety to a list that's
otherwise quite technical.&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.becomingadatascientist.com/category/podcast/"&gt;Becoming a Data Scientist Podcast&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Host Renee Teate documents her path to becoming a data scientist,
interviewing plenty of inspiring data science people along the way. The
podcast is accompanied by an excellent &lt;a href="http://www.becomingadatascientist.com/learningclub/"&gt;learning club&lt;/a&gt; for aspiring
data scientists. I was a contributor for a while and a guest on &lt;a href="http://www.becomingadatascientist.com/2016/06/15/becoming-a-data-scientist-podcast-episode-12-data-science-learning-club-members/"&gt;one of the episodes&lt;/a&gt;,
but that's not why you should check it out. Also season 2 is coming
soon!&lt;/p&gt;
&lt;h2&gt;&lt;a href="http://www.thetalkingmachines.com/"&gt;Talking Machines&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Arguably the most advanced and technical data science/machine learning
podcast out there. The hosts interview academics, including machine
learning greats Geoff Hinton and Yann LeCun, and discuss advanced topics
in machine learning. This one is harder to listen to while doing the
dishes because it requires my full attention, but it's always useful and
educational.&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://talkpython.fm/"&gt;Talk Python to Me&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Not limited to data science, this one is all about Python. I've only
recently started listening to it, and already I've heard gems like &lt;a href="https://talkpython.fm/episodes/show/77/20-python-libraries-you-aren-t-using-but-should"&gt;20 Python Libraries You Aren't Using But Should.&lt;/a&gt;
Recommended for all Python programmers and anyone interested in the
Python ecosystem.&lt;/p&gt;
&lt;h1&gt;Other Podcasts&lt;/h1&gt;
&lt;h2&gt;&lt;a href="https://hitreply.co/"&gt;Hit Reply&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;A recently-launched podcasts about startups. The hosts have experience
launching software products and each episode is dedicated to advice on a
different matter, like competition or productivity. I'm enjoying it so
far!&lt;/p&gt;
&lt;h2&gt;&lt;a href="https://youarenotsosmart.com/podcast/"&gt;You Are Not So Smart&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;"A celebration of self delusion", this one's all about human behaviour,
hidden biases, logical fallacies, that kind of thing. The podcast also
contains &lt;a href="https://youarenotsosmart.com/cookie-recipes/"&gt;cookies&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;What podcasts do you listen to? I'm always looking for recommendations! &lt;/p&gt;
&lt;p&gt;Footnote: This was the first entry in my &lt;a href="http://davidasboth.com/2016/11/01/30-posts-in-30-days/"&gt;30 day blog challenge&lt;/a&gt;.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category></entry><entry><title>30 Posts in 30 Days</title><link href="/blog/30-posts-in-30-days" rel="alternate"></link><published>2016-11-01T19:24:00+00:00</published><updated>2016-11-01T19:24:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-11-01:/blog/30-posts-in-30-days</id><summary type="html">&lt;p&gt;Today marks the start of November. That's usually not particularly
interesting, but November happens to be &lt;a href="http://nanowrimo.org/"&gt;National Novel Writing Month&lt;/a&gt; (or NaNoWriMo for short - just rolls of
the tongue). The idea behind NaNoWriMo is simple;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"&lt;em&gt;writing a 50,000-word novel by 11:59 PM on November 30&lt;/em&gt;"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As an aside …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Today marks the start of November. That's usually not particularly
interesting, but November happens to be &lt;a href="http://nanowrimo.org/"&gt;National Novel Writing Month&lt;/a&gt; (or NaNoWriMo for short - just rolls of
the tongue). The idea behind NaNoWriMo is simple;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;"&lt;em&gt;writing a 50,000-word novel by 11:59 PM on November 30&lt;/em&gt;"&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As an aside, there's a geekier version: &lt;a href="https://github.com/NaNoGenMo/2016"&gt;National Novel &lt;em&gt;Generation&lt;/em&gt; Month&lt;/a&gt;, where the goal is to write
code that writes a 50,000 word novel. That's the only constraint - a
minimum of 50,000 words. There have been some hilarious entries over the
years, but I digress.&lt;/p&gt;
&lt;p&gt;So why am I bringing up NaNoWriMo? At this point you might be half
expecting me to announce that I'll participate and write a novel.&lt;/p&gt;
&lt;p&gt;Let me quash that idea right now.&lt;/p&gt;
&lt;p&gt;It's not happening.&lt;/p&gt;
&lt;p&gt;However, in the spirit of NaNoWriMo I've decided to set myself a
challenge:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write 30 blog posts in 30 days.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;Why?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;I like writing -  I need a way to get into the habit of writing
    more, although it's entirely possible I'll hate writing by the end
    of the month.&lt;/li&gt;
&lt;li&gt;I've always wanted to maintain a blog - Well, more accurately, I've
    always wanted to &lt;em&gt;be someone who maintains a blog&lt;/em&gt;. I just never got
    around to the part where you actually do it.&lt;/li&gt;
&lt;li&gt;I want to experiment with what it's like to release content which I
    don't have too much time to curate. In the past I've been known to
    spend hours on a single blog post agonising over tiny details in
    pursuit of "perfection" - this needs to stop.&lt;/li&gt;
&lt;li&gt;Also, I currently have a Master's thesis due in 2 months, so
    spending a whole month writing thousands of words towards a
    different thing somehow sounds like a good idea.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Ironically it's also an experiment in &lt;em&gt;beating&lt;/em&gt; procrastination.
External pressure can be helpful in forcing you to do stuff - &lt;a href="http://waitbutwhy.com/2013/11/how-to-beat-procrastination.html"&gt;as Tim Urban puts it&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;If you’re trying to write a consistent blog, put “new post every Tuesday” at the top of the page.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3&gt;The Rules&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;30 posts in 30 days, although not strictly one per day.&lt;/li&gt;
&lt;li&gt;This post doesn't count.&lt;/li&gt;
&lt;li&gt;Err, that's it!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'll try and create a mix of small technical write-ups, targeted lists
of resources and general musings about software, data, machine learning,
AI and technology and life in general. I will try to make each post
meaningful so I don't feel like I'm cheating.&lt;/p&gt;
&lt;p&gt;I'll also &lt;a href="https://twitter.com/davidasboth"&gt;tweet&lt;/a&gt; about each blog post
as I write them. If you want to attempt a similar thing (even just 5
blog posts in 30 days) let me know!&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category></entry><entry><title>Analysing London House Prices</title><link href="/blog/analysing-london-house-prices" rel="alternate"></link><published>2016-10-23T16:33:00+01:00</published><updated>2016-10-23T16:33:00+01:00</updated><author><name>david</name></author><id>tag:None,2016-10-23:/blog/analysing-london-house-prices</id><summary type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;London is expensive. So much so that it's a trope now for those of us
who live here. But what does the data show? Are things getting better or
worse? How did the 2008 recession affect buying behaviour for example?&lt;/p&gt;
&lt;p&gt;I wanted to find out. With data.&lt;/p&gt;
&lt;p&gt;There are …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Introduction&lt;/h1&gt;
&lt;p&gt;London is expensive. So much so that it's a trope now for those of us
who live here. But what does the data show? Are things getting better or
worse? How did the 2008 recession affect buying behaviour for example?&lt;/p&gt;
&lt;p&gt;I wanted to find out. With data.&lt;/p&gt;
&lt;p&gt;There are many directions you could go with a dataset of London house
prices, so I focused on answering specific questions. I wanted to know:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Can the recession, centred around 2008, be seen in the house price
    data?&lt;/li&gt;
&lt;li&gt;Which boroughs are on the rise economically, and which are
    exhibiting a downward turn?&lt;/li&gt;
&lt;li&gt;How have housing prices changed over time overall?&lt;/li&gt;
&lt;li&gt;Does the overall trend match the individual trend in each borough?&lt;/li&gt;
&lt;li&gt;Which boroughs are similar in their economic trends?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;The Data&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://app.enigma.io/table/gov.uk.land-registry.price-paid"&gt;The dataset&lt;/a&gt;
was obtained from an online service called &lt;a href="http://enigma.io/"&gt;Enigma&lt;/a&gt;,
which requires a free registration to access the data. It contained the
address of each house and the postcode, but not latitude-longitude
values. I could have geocoded each address individually to get accurate
lat-long values, but life's too short and postcodes were granular
enough. I added lat-long values by joining a &lt;a href="https://www.freemaptools.com/download-uk-postcode-lat-lng.htm"&gt;geocoded postcode dataset&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The final housing price dataset consisted of 16 columns and around
1,500,000 rows. The main columns of interest were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Date of Sale (to the nearest day)&lt;/li&gt;
&lt;li&gt;Postcode (now with latitudes and longitudes)&lt;/li&gt;
&lt;li&gt;Local Authority (i.e. borough)&lt;/li&gt;
&lt;li&gt;Price (in GBP)&lt;/li&gt;
&lt;li&gt;Property Type (Detached, Semi, Terraced, or Flat/Maisonette)&lt;/li&gt;
&lt;li&gt;Build (Old or New)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The data covers the period between 1996 and 2014, so this post won't
turn into a digression about Brexit, although that will be an
interesting dataset in a few years.&lt;/p&gt;
&lt;p&gt;There were a few boroughs with only a handful of rows in the dataset.
One of them for example was a Welsh address, incorrectly labelled as
'London'. I removed 5 boroughs in total because they were either outside
of London, or contained less than 20 examples.&lt;/p&gt;
&lt;h2&gt;The Tools&lt;/h2&gt;
&lt;p&gt;The data wrangling was done in Python and all the visualisations were
created with &lt;a href="http://www.tableau.com/"&gt;Tableau&lt;/a&gt;.&lt;/p&gt;
&lt;h1&gt;The Analysis&lt;/h1&gt;
&lt;h2&gt;Price Distribution&lt;/h2&gt;
&lt;p&gt;The very first question I wanted to answer is what the distribution of
prices is like.&lt;/p&gt;
&lt;p&gt;The median house price is £320,000 and the upper whisker of the box plot
is £1.26million. After that there are over 39,000 outliers with the
highest house price at over £50 million. This suggests there may be two
tiers of house prices in London: one for the 'average' person and one
for the more wealthy.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Distribution of London house prices" src="/images/analysing-london-house-prices/Price-Box-Plot-and-Histogram.png"&gt;&lt;/p&gt;
&lt;p&gt;Heavy right tails make box plots look ridiculous&lt;/p&gt;
&lt;p&gt;At this point my hunch was that the buying patterns of people who can
afford to pay millions for a house will be different to people buying
houses for around the median price. To this end I created an "outliers"
category and analysed those nearly 40,000 rows separately.&lt;/p&gt;
&lt;h2&gt;Prices over Time&lt;/h2&gt;
&lt;p&gt;So how have prices changed over the years in the "normal" and "outlier" categories?&lt;/p&gt;
&lt;p&gt;&lt;img alt="London house prices over time" src="/images/analysing-london-house-prices/Price-Over-Time.png"&gt;&lt;/p&gt;
&lt;p&gt;At this point it might be cheaper to invest in a time machine and go
back to 1996 to buy a house&lt;/p&gt;
&lt;p&gt;It turns out prices have steadily been climbing for both categories, but
the number of sales dipped during the 2008 recession, but only for the
non-outlier category. That makes sense - you'd expect the most wealthy
to be unaffected by a recession.&lt;/p&gt;
&lt;h2&gt;What about each borough?&lt;/h2&gt;
&lt;p&gt;Does this overall trend look the same in each borough? Let's find out.
For borough-level figures I looked at the non-outlier category to make
the analysis more relatable.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Median prices over time by borough" src="/images/analysing-london-house-prices/Boroughs-Recession-Price-over-Time.png"&gt;&lt;/p&gt;
&lt;p&gt;Some boroughs have a small dip but even a recession couldn't make London more affordable&lt;/p&gt;
&lt;p&gt;There is a more noticeable, but still very small, dip during the
recession years (highlighted in dark red) but prices are climbing in all
London boroughs. The dip in sales during the recession is also present
in each borough, echoing the overall trend.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Sales over time by borough" src="/images/analysing-london-house-prices/Boroughs-Recession-Sales-over-Time.png"&gt;&lt;/p&gt;
&lt;p&gt;This graph would probably be a worrying EKG&lt;/p&gt;
&lt;h2&gt;Buying Behaviour&lt;/h2&gt;
&lt;p&gt;Is buying a house seasonal or do people buy with the same frequency
throughout the year?&lt;/p&gt;
&lt;p&gt;&lt;img alt="Monthly sales histogram" src="/images/analysing-london-house-prices/Month-Histogram.png"&gt;&lt;/p&gt;
&lt;p&gt;Apparently people rarely buy houses as Christmas presents.&lt;/p&gt;
&lt;p&gt;The slight peak in March is interesting.&lt;/p&gt;
&lt;p&gt;After I showed this to a few people, someone suggested it's because the
tax year in the UK ends in April so people might end up buying with some
urgency to keep the spending in the previous tax year. This general
seasonal behaviour is also apparent in each borough.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Monthly sales heatmap by borough" src="/images/analysing-london-house-prices/Monthly-Heatmap-by-Borough.png"&gt;&lt;/p&gt;
&lt;p&gt;Is it still a heatmap if it's blue? &lt;/p&gt;
&lt;p&gt;It looks like some of the boroughs have a bigger peak in March than
others. Can we quantify which boroughs have this behaviour and which
don't?&lt;/p&gt;
&lt;h2&gt;Clustering&lt;/h2&gt;
&lt;p&gt;How do we find similar boroughs? We can make use of clustering.&lt;/p&gt;
&lt;p&gt;First, we aggregate each borough into a 12-dimensional vector, where
each item corresponds to the median percentage of total sales in that
particular month. So in a slightly extreme example, a borough with half
of all its sales in January and the other half in July would look like
this:&lt;/p&gt;
&lt;p&gt;[0.5, 0, 0, 0, 0, 0, 0.5, 0, 0, 0, 0, 0]&lt;/p&gt;
&lt;p&gt;We need to use the percentage rather than actual values to ensure all
boroughs are on a common scale. Otherwise, we would end up clustering
boroughs together simply on sales volume rather than buying behaviour.&lt;/p&gt;
&lt;p&gt;After trying multiple values of &lt;strong&gt;k&lt;/strong&gt; in the k-means clustering
algorithm, it looked like there were two distinct clusters with three
outliers. Those outliers were boroughs that had much fewer rows in the
dataset, so they're arguably not that representative.&lt;/p&gt;
&lt;p&gt;It appears then that there are really just two types of behaviour -
boroughs that have a prominent peak in sales in March and those that
don't. But which boroughs are which? Let's colour a map of London by
cluster:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Clustered boroughs" src="/images/analysing-london-house-prices/Cluster-Dashboard-4.png"&gt;&lt;/p&gt;
&lt;p&gt;London boroughs coloured by their buying behaviour&lt;/p&gt;
&lt;p&gt;Interestingly the different seasonal behaviour almost neatly splits
London into East and West. The dataset unfortunately didn't have enough
details to give any suggestions about whether this is a coincidence.&lt;/p&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;Unsurprisingly, the data confirms what we all complain about - buying in
London is expensive and it is only getting worse.&lt;/p&gt;
&lt;p&gt;The recession impacted the average person; the number of sales dipped
dramatically between 2007 and 2009. However, the rich were unaffected.&lt;/p&gt;
&lt;p&gt;Buying behaviour is seasonal peaking in July, with some boroughs
displaying a small peak in March (possibly for tax purposes). These
boroughs are typically in East London, which may or may not be a
coincidence.&lt;/p&gt;
&lt;p&gt;The next analysis on my list might be finding a new, more affordable
city!&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="featured"></category><category term="python"></category><category term="tableau"></category></entry><entry><title>Procrastination and Monkeys</title><link href="/blog/procrastination-and-monkeys" rel="alternate"></link><published>2016-03-12T13:58:00+00:00</published><updated>2016-03-12T13:58:00+00:00</updated><author><name>david</name></author><id>tag:None,2016-03-12:/blog/procrastination-and-monkeys</id><summary type="html">&lt;p&gt;Procrastination has always affected me as a tech professional. You can
probably tell by the fact that this is my first blog post for a few
months. Ideas have been brewing, but I never got around to writing them
up. Not spending your time on side projects, blogs, learning new …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Procrastination has always affected me as a tech professional. You can
probably tell by the fact that this is my first blog post for a few
months. Ideas have been brewing, but I never got around to writing them
up. Not spending your time on side projects, blogs, learning new things
etc. can be harmful both to your career and your mental health, so it's
a topic worth exploring. I've spent a lot of time over the years reading
about it and wanted to share some of what I've read.&lt;/p&gt;
&lt;p&gt;This post is a short reading list for those interested in the topic of
procrastination, and it follows on from a talk I gave at the excellent
&lt;a href="https://mkgeeknight.co.uk/"&gt;MK Geek Night&lt;/a&gt;. Here is the
&lt;a href="https://soundcloud.com/mkgn/mkgn-16-david-asboth-procrastination-and-monkeys"&gt;audio&lt;/a&gt;
and the &lt;a href="https://www.slideshare.net/secret/A9UhRGl7sWTqfH"&gt;accompanying slides&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're reading this while you should be doing something else, I hope
you appreciate the irony.&lt;/p&gt;
&lt;h1&gt;Articles&lt;/h1&gt;
&lt;h3&gt;&lt;a href="http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html"&gt;Why Procrastinators Procrastinate&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;If you only read one thing from this list, it should be this. It's the
daddy of all procrastination literature, and chiefly where the idea of
procrastination linked to monkeys came from. The whole blog is
fantastic, sadly also a great source of things to read when
procrastinating...&lt;/p&gt;
&lt;h3&gt;&lt;a href="http://www.raptitude.com/2011/05/procrastination-is-not-laziness/"&gt;Procrastination is not Laziness&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;A post from &lt;a href="http://www.raptitude.com/"&gt;Raptitude&lt;/a&gt;, one of my favourite
blogs. Reading this particular post was a real eye-opener, and it's what
started me on my journey of trying to understand procrastination. It
gave me a lot of insight into the procrastinating behaviour and really
hit home. &lt;/p&gt;
&lt;h1&gt;Books&lt;/h1&gt;
&lt;h3&gt;&lt;em&gt;The Now Habit&lt;/em&gt; - Neil Fiore&lt;/h3&gt;
&lt;p&gt;A classic book in the procrastination literature. David Cain mentions it
in his Raptitude post, and I can only echo that a lot of it resonated
with me too. Fiore's idea of an "Unschedule" is a particularly good
thing to try. Disclaimer: no monkeys in this one.&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;The Chimp Paradox&lt;/em&gt; - Prof Steve Peters&lt;/h3&gt;
&lt;p&gt;While not chiefly about procrastination, the whole book is centred on
the "inner chimp" and what to do about it. The insights are definitely
applicable to procrastinators.&lt;/p&gt;
&lt;h3&gt;&lt;em&gt;War of Art&lt;/em&gt; - Steven Pressfield&lt;/h3&gt;
&lt;p&gt;A book with short, sharp chapters dealing with Resistance, Pressfield's
personification of procrastination. Again, no monkeys, just some real
home truths.&lt;/p&gt;
&lt;h1&gt;Hacker News Threads&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=10267564"&gt;"The cure for procrastination? Forgive yourself"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://news.ycombinator.com/item?id=10156240"&gt;"To Stop Procrastinating, Start by Understanding the Emotions Involved"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, I've bookmarked a couple of threads from Hacker News.
Procrastination comes up quite often, as it's a topic that affects a lot
of us in the tech industry, and I found it immensely helpful to see
other people's perspectives. The underlying articles are good too, but
it's the discussion that I think makes these particularly worthwhile
reading. It's great to read a well-written article or book on the
subject, but sometimes it helps to hear it directly from those affected
by it, in a less edited, 'raw' form.&lt;/p&gt;
&lt;p&gt;This is just a small slice of what's out there on the subject of
procrastination, and I'd welcome some of your suggestions on resources
that I haven't included but you feel are worth sharing.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="self improvement"></category><category term="featured"></category></entry><entry><title>Pandas: a Quick Reference Guide</title><link href="/blog/pandas-quick-reference-guide" rel="alternate"></link><published>2015-12-22T18:58:00+00:00</published><updated>2015-12-22T18:58:00+00:00</updated><author><name>david</name></author><id>tag:None,2015-12-22:/blog/pandas-quick-reference-guide</id><summary type="html">&lt;p&gt;Before I start, to placate readers who were expecting a blog post about
panda bears, here's a picture of pandas at play:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pandas at play" src="/images/pandas-quick-reference-guide/pandas-at-play-1174622-638x406.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Panda bears - more mysterious than the Python library? Certainly cuter.&lt;/p&gt;
&lt;p&gt;From now on, 'pandas' will refer to the Python library, not the bears. &lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Pandas is a Python …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Before I start, to placate readers who were expecting a blog post about
panda bears, here's a picture of pandas at play:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Pandas at play" src="/images/pandas-quick-reference-guide/pandas-at-play-1174622-638x406.jpg"&gt;&lt;/p&gt;
&lt;p&gt;Panda bears - more mysterious than the Python library? Certainly cuter.&lt;/p&gt;
&lt;p&gt;From now on, 'pandas' will refer to the Python library, not the bears. &lt;/p&gt;
&lt;h2&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Pandas is a Python library designed to help with data wrangling. I've
been using it for a few months now, and I can't shake the nagging
feeling that I haven't quite got the hang of it yet. For all its power
and obvious usefulness, there's something about it that I just find
unintuitive. I've looked at a few step-by-step tutorials online about
it, such as the &lt;a href="https://www.kaggle.com/c/titanic/details/getting-started-with-python-ii"&gt;one on Kaggle&lt;/a&gt;,
and it still hadn't clicked, so I decided to create an IPython Notebook
as a reference guide. Initially, I was going to make a rough one for
myself, but then I thought I might as well share it considering other
people have complained of similar difficulties.&lt;/p&gt;
&lt;h2&gt;The Notebook&lt;/h2&gt;
&lt;p&gt;As it's meant as a quick reference guide and not a tutorial, the
notebook itself consists mainly of headers and code snippets, often
without much explanation. Where there are caveats, gotchas, or general
things to remember I've made additional notes.&lt;/p&gt;
&lt;p&gt;I was considering pasting the text from the notebook into this post.
However, it will evolve over time as I learn more about pandas, so
instead, you can look at the most up-to-date version on NBViewer (an
online IPython Notebook renderer) or grab it for yourself on GitHub.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://nbviewer.ipython.org/github/davidasboth/pandas-reference/blob/master/Pandas%20tutorial.ipynb"&gt;NBViewer version&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/davidasboth/pandas-reference/blob/master/Pandas%20tutorial.ipynb"&gt;GitHub version&lt;/a&gt;
    (or you can visit the &lt;a href="https://github.com/davidasboth/pandas-reference/"&gt;full repo&lt;/a&gt;, if you want
    to download the notebook for yourself)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, I've seen notebooks with dynamic tables of contents at the top, so
I'll try to figure out how to do that at some point, especially if the
notebook gets unwieldy.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="data science"></category><category term="featured"></category><category term="pandas"></category><category term="python"></category></entry><entry><title>Visualising the Hacker News Salary Survey(Take 1)</title><link href="/blog/visualising-the-hacker-news-salary-survey-take-1" rel="alternate"></link><published>2015-11-06T19:15:00+00:00</published><updated>2015-11-06T19:15:00+00:00</updated><author><name>david</name></author><id>tag:None,2015-11-06:/blog/visualising-the-hacker-news-salary-survey-take-1</id><summary type="html">&lt;p&gt;A few months ago, before starting a Masters in Data Science, I
experimented with a bit of data visualisation. I am an avid reader of
&lt;a href="https://news.ycombinator.com"&gt;Hacker News&lt;/a&gt;, which is a great site for
programmers to frequent. If you don't already know it, I can't recommend
it highly enough if you …&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few months ago, before starting a Masters in Data Science, I
experimented with a bit of data visualisation. I am an avid reader of
&lt;a href="https://news.ycombinator.com"&gt;Hacker News&lt;/a&gt;, which is a great site for
programmers to frequent. If you don't already know it, I can't recommend
it highly enough if you feel like you want to spend your procrastinating
time more wisely (that's my main motivation for visiting, anyway...). It
was an ad-hoc survey on there that led me to make this little
visualisation and I'd like to share the end result, as well as the
process that went into it, for two reasons. The first reason is that it
could give some interesting insight into how an amateur would approach a
data science problem using nothing but programming knowledge and some
common sense. The second reason is that in a few months I will revisit
this exact data set and analyse/visualise it again, to see how I can
apply what I learned on my course to improve upon my first attempt.&lt;/p&gt;
&lt;p&gt;All I can say is: I anticipate the second attempt will be
recognisably... better.&lt;/p&gt;
&lt;p&gt;So, without further ado I present:&lt;/p&gt;
&lt;h2&gt;The (sort of) layman's guide to a basic data visualisation&lt;/h2&gt;
&lt;h3&gt;The Data&lt;/h3&gt;
&lt;p&gt;About the dataset: this was an anonymous survey run on Hacker News of
around 400 developers and designers, with data gathered about their job
title, seniority, location and salary. The dataset is inherently 'dirty'
as it contains some of the usual suspects: it has values in different
currencies, seniority levels and job titles aren't standardised and
there are multiple different entries for the same location.&lt;/p&gt;
&lt;p&gt;The dataset I used is available
&lt;a href="https://docs.google.com/spreadsheets/d/17Mr201gfDoOTe5ONLS6LYJi1wQbtT26srXeSwUjMK0A/htmlview?usp=sharing&amp;amp;sle=true"&gt;here&lt;/a&gt;
(credit to &lt;a href="http://twitter.com/cameronmoll"&gt;@cameronmoll&lt;/a&gt; for making it
happen).&lt;/p&gt;
&lt;p&gt;The original Hacker News discussion for those interested is
&lt;a href="https://news.ycombinator.com/item?id=8573221"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;The Hypothesis&lt;/h3&gt;
&lt;p&gt;As with any data science task, I needed to start with a hypothesis or a
purpose. Essentially I wanted to use this mini-project to find a nice
way to plot data on a map, so the purpose was to clean the data and find
a good library to visualise it geographically. I'll be honest, being a
data science layman I didn't even have a hypothesis about the data.&lt;/p&gt;
&lt;p&gt;I just wanted to make a pretty map.&lt;/p&gt;
&lt;h3&gt;The Process&lt;/h3&gt;
&lt;h4&gt;Data Wrangling&lt;/h4&gt;
&lt;p&gt;Data wrangling is my favourite of the many phrases (closely followed by
'data jiu-jitsu') used to describe the act of 'cleaning' a dataset, i.e.
getting it into a shape that's suitable for analysis. It is apparently
what data scientists spend most of their time doing, so it's an
important step. This includes dealing with missing values, correcting
duplicate values (e.g. making sure we treat "London" and "London,
England" as the same location) and so on.&lt;/p&gt;
&lt;p&gt;Let's have a look at some of the issues with the dataset after an
initial inspection:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The figures are in different currencies, this will need to be
    rectified&lt;/li&gt;
&lt;li&gt;The job titles need to be merged into one column&lt;/li&gt;
&lt;li&gt;The locations contain duplicates (e.g. 'US', 'USA', 'United States'
    all appear) &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before I can do any meaningful analysis or visualisation, these issues
need to be ironed out. Being a data layman at the time, my data
wrangling tool of choice was Excel and I did most of my work manually.
My data wrangling consisted of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Looking up the most up-to-date exchange rates online and converting
    all figures to USD (yes, I did this manually...)&lt;/li&gt;
&lt;li&gt;Merging the job titles into one column&lt;/li&gt;
&lt;li&gt;Manually merging duplicate locations (on the country column only)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Doing this all by hand was a pretty tedious method to use, and one I
don't wish to repeat in future. It's also an implausible method for
larger datasets, so when I revisit this dataset I'll look at what I
could have automated using something like Python or R.&lt;/p&gt;
&lt;p&gt;You can download my final version of the dataset here (hopefully you'll agree it's better!): &lt;a href="http://davidasboth.com/wp-content/uploads/2015/10/SalaryData.csv"&gt;Salary Data&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Summarisation and Aggregation&lt;/h4&gt;
&lt;p&gt;Now was the time to do some basic statistics and aggregation to be able
to see the average salary by country on a map.&lt;/p&gt;
&lt;p&gt;To do this, I used some basic &lt;a href="https://en.wikipedia.org/wiki/R_%28programming_language%29"&gt;R, a popular data science
language&lt;/a&gt;. I
recommend it as a good starting point for aspiring data scientists. My R
script was quite simple, the only real important line was the one that
took the salaries and averaged them by location:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="c1"&gt;# load the data&lt;/span&gt;
&lt;span class="n"&gt;SalaryData&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;read.csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;SalaryData.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# get mean values in a table&lt;/span&gt;
&lt;span class="n"&gt;byCountry&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;-&lt;/span&gt; &lt;span class="nf"&gt;aggregate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SalaryUSD&lt;/span&gt;&lt;span class="o"&gt;~&lt;/span&gt;&lt;span class="n"&gt;Country&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;SalaryData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# write new data to csv&lt;/span&gt;
&lt;span class="nf"&gt;write.csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;byCountry&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;ByCountry.csv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once this was done, it was time to get this onto a map!&lt;/p&gt;
&lt;h4&gt;Visualisation&lt;/h4&gt;
&lt;p&gt;I used &lt;a href="http://jsdatav.is/chap06.html"&gt;this excellent tutorial&lt;/a&gt; to
visualise my data on a map using a map font and some Javascript. I
cheated a bit and manually entered the figures into a Javascript array
(one of the enhancements I'll do is to actually read that in from a
file). I won't go into depth about what I did differently, because I
pretty much followed the tutorial step by step, tweaking things like
colours as I went. I recommend following along, it's a very well-written
tutorial.&lt;/p&gt;
&lt;p&gt;If you want to view it in the browser, here's &lt;a href="/hn-map/"&gt;the final product&lt;/a&gt;, otherwise, you can see it in
all its glory below.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Hacker News Salary Map" src="/images/visualising-the-hacker-news-salary-survey-take-1/hn-salary-map.png"&gt;&lt;/p&gt;
&lt;p&gt;The survey proved Norway is practically radioactive due to high tech salaries&lt;/p&gt;
&lt;p&gt;All the data I wrangled and code I wrote is &lt;a href="https://github.com/davidasboth/HN-Survey-Analysis"&gt;available on GitHub&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Conclusion&lt;/h4&gt;
&lt;p&gt;If you're interested in data analysis but have no idea where to start, I
hope this post will inspire you to try and attempt your own
visualisation without fear of getting it wrong. Data science is all
about trial and error, so I say just go for it! I used nothing but my
own intuition to come up with the process for what I did, there wasn't
any knowledge of best practices behind it, and I still came up with a
colourful visual that communicates something about the data.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="projects"></category><category term="datavis"></category><category term="featured"></category></entry><entry><title>Data Entry Should Be Awesome</title><link href="/blog/data-entry-should-be-awesome" rel="alternate"></link><published>2015-09-18T09:36:00+01:00</published><updated>2015-09-18T09:36:00+01:00</updated><author><name>david</name></author><id>tag:None,2015-09-18:/blog/data-entry-should-be-awesome</id><summary type="html">&lt;h1&gt;Forms are boring&lt;/h1&gt;
&lt;p&gt;This is a universal truth; no one likes filling in forms or surveys.
Online forms are only marginally better to fill in than paper forms
because sometimes you get the little question mark icon telling you how
you're actually supposed to fill it in. But when was …&lt;/p&gt;</summary><content type="html">&lt;h1&gt;Forms are boring&lt;/h1&gt;
&lt;p&gt;This is a universal truth; no one likes filling in forms or surveys.
Online forms are only marginally better to fill in than paper forms
because sometimes you get the little question mark icon telling you how
you're actually supposed to fill it in. But when was the last time you
filled in an online form and afterwards thought to yourself: "that was
fun, I'd like to do that again!"?&lt;/p&gt;
&lt;p&gt;That has never happened &lt;sup&gt;[citation needed]&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;So what is it about forms that we hate so much?&lt;/p&gt;
&lt;h3&gt;They can be too complex&lt;/h3&gt;
&lt;p&gt;If the layout is bad and you are presented with 20 fields at once, it
can be overwhelming.&lt;/p&gt;
&lt;h3&gt;We don't know how much of it to fill in&lt;/h3&gt;
&lt;p&gt;You don't always know which fields are mandatory; often, you only find
out once you've submitted the form. Or, if you fill in the entire form
every time to ensure you don't miss anything, you might waste time or
give away information you don't need to.&lt;/p&gt;
&lt;h3&gt;It's not how we operate as humans&lt;/h3&gt;
&lt;p&gt;Being presented with all the fields at once is just not the way humans
interact. Let's say you are at a job interview. Online forms are the
equivalent of instead of the interviewer asking you his questions one at
a time, you have 20 interviewers asking you 20 questions simultaneously.
We as humans have a limited attention span and don't want to see 20
different questions being asked of us at the same time, so why should we
gather information online in this way?&lt;/p&gt;
&lt;h1&gt;Why is this so bad?&lt;/h1&gt;
&lt;p&gt;Badly designed forms also affect us as developers. You can create the
fanciest, most complex system in the world but if the users' interaction
with it produces a really bad experience for them, you've lost all
buy-in. If your job requires entering data many times a day, the
difficulties in a form accumulate over time and cause unnecessary
resentment.&lt;/p&gt;
&lt;p&gt;Anecdotal evidence: we were working on a new version of an in-house CRM
system and the new sales director was looking to roll this out across
his team, and make the use of it mandatory. The first bit of feedback he
gave when looking at the existing system was: "there's too much here, my
team won't want to fill that form in multiple times a day" and he was
right. There is a greater lesson to be learned there about how, as
developers, we often forget that people will use our products
differently to how we interact with them while testing our own code.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Working with laptop" src="/images/data-entry-should-be-awesome/working-with-laptop-5-1545966-638x386.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This user was literally bored to sleep by the form she was faced with&lt;/p&gt;
&lt;h1&gt;So where do we start with fixing this problem?&lt;/h1&gt;
&lt;p&gt;I have always been a firm believer that any system should interact with
people in a way that is more natural to them. Filling in a form should
be a conversation, not a struggle. The benefits of this approach are
numerous.&lt;/p&gt;
&lt;h3&gt;Less user training&lt;/h3&gt;
&lt;p&gt;If data entry fields are phrased as questions, users are less likely to
be confused about what it is you're after.&lt;/p&gt;
&lt;h3&gt;You're asking for one piece of information at a time&lt;/h3&gt;
&lt;p&gt;You will never overwhelm your users by giving them all the required
fields on a single, cluttered screen.&lt;/p&gt;
&lt;h3&gt;It looks better!&lt;/h3&gt;
&lt;p&gt;This is mostly subjective, but a conversational form will always be more
visually appealing than a collection of fields and labels.&lt;/p&gt;
&lt;h3&gt;Better user experience, more 'human'&lt;/h3&gt;
&lt;p&gt;Because it will better reflect the way people interact with other humans
in real life, conversational forms will result in a pleasant user
experience. Obviously there is a limit to the amount of enjoyment you
can get from data entry, but why not at least try to hit that limit?&lt;/p&gt;
&lt;h3&gt;Forms could even be business-driven!&lt;/h3&gt;
&lt;p&gt;If forms really are to become conversational, there is nothing stopping
business users coming up with the conversation. Never again will you
show your first iteration of your app to a user and confuse them with
your misguided naming conventions! :-)&lt;/p&gt;
&lt;h3&gt;Some examples&lt;/h3&gt;
&lt;p&gt;Here are some noteworthy data entry experiences that have stuck in my
memory as being rather pleasant:&lt;/p&gt;
&lt;h4&gt;Pact Coffee&lt;/h4&gt;
&lt;p&gt;I found the sign up process so seamless that I almost forgot I was
actually filling in a form. On a side note, if you are a coffee lover in
the UK wanting to get your coffee delivered to your door, I recommend
trying Pact. &lt;em&gt;Disclaimer: no one paid me for saying that, not even with
a free coffee :-(&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.pactcoffee.com/funnel/intro"&gt;https://www.pactcoffee.com/funnel/intro&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Airport security&lt;/h4&gt;
&lt;p&gt;I have noticed a recent increase in the happy/sad face panels appearing
just after airport security. I believe they are provided by
&lt;a href="http://www.happy-or-not.com/"&gt;HappyOrNot&lt;/a&gt; and typically have 4 faces
ranging from a red, angry face to a green, happy face where you rate
your experience of airport security effectively on a scale of 1-4. This
is a much better way to gather information from people than a person
with a clipboard. I don't want to stop for someone (or a paper form) to
ask me 50 questions about who I am and where I've travelled, but I am
more likely to press a button with a face on it as I pass. I can imagine
companies implementing this will have seen a rise in survey completion
rates.&lt;/p&gt;
&lt;h4&gt;My contact form!&lt;/h4&gt;
&lt;p&gt;I practice what I preach; my contact form is conversational. Take a look
and &lt;a href="/contact"&gt;contact me&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img alt="Man with laptop" src="/images/data-entry-should-be-awesome/guy-with-laptop-1243596-640x480.jpg"&gt;&lt;/p&gt;
&lt;p&gt;This guy has just been through a great data entry experience&lt;/p&gt;
&lt;h1&gt;Great, now how do I start making awesome, conversational forms?&lt;/h1&gt;
&lt;p&gt;You need look no further than &lt;a href="http://www.typeform.com"&gt;TypeForm&lt;/a&gt;. This
is what they say about forms/surveys:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The key to a good survey is to make questions feel conversational.
Human, even.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hey, that's what I said!&lt;/p&gt;
&lt;p&gt;TypeForm lets you create an amazing, interactive experience for your
users. This can range from a contact form to a full-blown survey and
everything in-between. I won't go into huge detail about using it, I'll
let their &lt;a href="http://www.typeform.com/examples/"&gt;examples&lt;/a&gt; speak for
themselves. What I can say is that along with the forms you'll get lots
of customisation options as well as reporting and analytics on your
responses. &lt;em&gt;Disclaimer: again, no one is paying me to say this. I think
I should really try to change that! :-(&lt;/em&gt;&lt;/p&gt;
&lt;h1&gt;So, what next?&lt;/h1&gt;
&lt;p&gt;I am certain that what I am proposing isn't new. Others have had the
same thoughts as me. TypeForm are making a business out of it. Then why
are we (read: a majority of industry) still so reluctant to innovate and
make our forms more like this? Perhaps it is more effort, but if your
application relies on users entering data correctly they need to buy
into the idea and giving them a form that is straightforward to fill in
is a good start.&lt;/p&gt;
&lt;p&gt;I propose that we make this our default when creating forms in our web
apps. I know I will strive to make all my forms more human-like.&lt;/p&gt;&lt;script src="//platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;</content><category term="technology commentary"></category><category term="featured"></category></entry></feed>